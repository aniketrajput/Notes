
Section 1: Introduction

2. Walkthrough of the AWS Certified Cloud Practitioner Exam Guide

	Format - multiple choice
	
	Time - 90 mins
	
	65 questions
	
	Cost - 100 USD (Practice Exam : 20 USD)
	
	Delivery Method - Testing center or online proctored exam.
	
	Scoring scaled between 100 - 1000
	Minimun passing score - 700
	
	Question format: 
		- Multiple-choice: has one correct response and three incorrect response
		- Multiple-responses: Has two or more correct responses out of five or more options
	
	AWS Whitepapers (removed from exam guide, but recommended): 
		- Overview of Amazon Web Services whitepaper
		- Architecting for the Cloud: AWS Best practices whitepaper
		- How AWS pricing works whitepaper
		- Cost management in the AWS Cloud whitepaper
		
	
	https://aws.amazon.com/certification/certification-prep/	
	
	Exam Syllabus and guide - https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf
	
	Sample questions - https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Sample-Questions.pdf


	We will be downloading few resources, very useful. There are all slides, images and also links to docs which we can read and are useful.
	For links, there are two links one for amazon website and other to digital cloud. We can use digital cloud one, because amazon one will have lots of info which might not be relevant for this certification.
	
	
	Please download the course materials including slides, exam cram PDF and files and commands used in the course using the link below:

	https://digitalcloud.training/aws-ccp-exam-training-course-downloads/

	NOTE: The option to register for a free copy of our AWS Certified Cloud Practitioner Training Notes ebook which is an offline version of the training notes on our website and a really useful cheat sheet, can be found on the page link above.


Section 2. 

	We have all slides available.
	
	
	Cloud Computing Cloud computing is the on-demand delivery of IT services from a
	third-party provider over the Internet - Gmail, FB, Dropbox, etc.

	Hypervisor - Software that runs OS.
	
	
	FB, Gmail are SaaS. We don't have to do anything, just consume the service.
	
	
	Private cloud, its not a multi-tenant enviornment that is shared with other customers.
	
	
19. AWS Global vs Regional Services

	AWS services have either a global or a regional scope. 
	
	That means that they are administered either what in one place globally or they're administered in regions.

	The services highlighted in read here are in ones that are global services. 

	For example AWS IAM service we create them in one place, we don't create them in different regions.
	
	
	The AWS IAM service is a global service which means you create your users, groups, roles and policies in one place
	
	Compute, storage and outbound data transfer are the three fundamentals of AWS pricing
	
	On-demand is the best option when you need the most flexibility. There are no long-term commitments or upfront payments.
	
	The AWS Acceptable Use Policy describes the prohibited uses of AWS
	

Section 4: Identity and Access Management (IAM)

24. IAM Overview


	We can also create a user account that's used by a service, means rather than an individual using it to log on, an application is using those credentials. We can do it using IAM roles. 
	
	Policy is the document that defines what the permissions are for that group.
	

27. IAM Roles

	Example - You have an EC2 instance and you assign that EC2 instance a role and that role provides full access to the Amazon S3 free storage service so that EC2 instance is able to read from S3 or write data to S3. 
	So what this means is we are actually giving the EC2 instance those permissions but because it's using an IAM role what happens is you don't have to configure any username or password on that EC2 instance. So it's much more secure way of providing that EC2 instance the access it needs.

	So again if you give your EC2 instance access to S3 using a role you don't need to place a username and password somewhere on the EC2 instance or somewhere in its code. 
	
	IAM users or AWS services can assume the role and they actually get temporary security credentials that they then use to make those API calls. 
	Now the security credentials are temporary, means that when your EC2 instance requires access to S3 it goes to IAM and it gets some temporary security credentials which will expire. It will get renewed. Means it will expire after a period of time but it will just get some new ones and it will continually be able to access S3 unless you make a change to the configuration and you don't want it to be able to access S3 anymore. 
	
	
	
	STS (AWS Security Token Service) is the way that you request temporary limited privilege credentials.
	
	
	An access key ID and secret access key is associated with a user and is used for granting programmatic access using the CLI or API
	
	
	This is the correct answer, an Amazon Resource Name (ARN) is associated with entities such as users and groups
	
	
	When assigning permissions always grant the least privileges required. This is a security best practice
	
	
	You can delegate permissions using roles. It's a great way to provide permissions to resources for users and services without using permanent credentials
	
	
	So even if you create an IAM user and give them the full administrative privileges to your account there's still a few privileges that they don't get but the root account has everything.
	
	
Section 6: AWS Compute

42. Amazon EC2 vs Traditional Servers

	Amazon EC2 - Amazon Elastic Compute Cloud.
	
	
	Traditional servers - 
		
		CPU - Processors
		RAM 
		Disk - Storage
		NIC - Network Interface Card for sending data over a network
		
		So if we want to upgrade, we either buy the complete server or by any of the hardware component to scale.
	
	
	Now as we know AWS is a web service. So as a web service we go into the Amazon EC2 management console and launch an EC2 instance. 
	
	So an instance is an individual server, it's a virtal server.
	
	https://www.youtube.com/watch?v=JGtr-O3KJlE
	
	Physical server - I can actually see it, go and open it up.
	
	Virtual server - Server which host virtual machines. We may not see it, it may be on cloud.
	

Section 6: AWS Compute

44. Amazon Machine Images (AMI)
	
	EBS snapshot is copy of EBS volume. EBS (Elastic Block Store) volume is a persistent storage device and its like a virtual hard drive in the cloud. So we can think of it as a place where we store data and that data is stored persistently it doesn't get lost. 

	So our AMI can be backed by an EBS snapshot and whatever data is on that snapshot like our windows or linux OS and your configuration and software packages you might have installed will be part of your instance. 

	Now alternatively our AMI can be backed by what's known as an instance store and an instance store is a non-persistent device. That means the data is lost when you shut it down. And these are very high performance. 
	
	
46. Launch EC2 Instance Hands-on (optional)
	
	EC2 is a regional service, means you must select a region in the AWS management console and you launch your instance into that region.
	
	VPC (Virtaul Private Cloud), we can think of this as a virtual data center so everything within this VPC is just for you. 
	Now it's logical it doesn't mean you have any physical segmentation within the AWS cloud but it does mean that its kind of like a security boundary and a networking boundary and within that all the resources are yours and they're very private.
	
	We have public and private subnet and if we want to be able to connect directly to our instance from the Internet we put it into a public subnet and that means it gets a public address. So it gets a public IP address so we can find it on the Internet. 
	

46. Launch EC2 Instance Hands-on (optional)

	Step Configure Security Group - it is a instance level firewalls. Firewall is a networking service which determines or allows certain traffic to reach your instance or it denies traffic from reaching your instance.
	And it can do the same for the traffic leaving your instance. So you can configure which port or protocols what type of traffic should be allowed to your instance and allowed from your instance out to internet.  
	


Youtube video - Introduction to AWS Services - https://www.youtube.com/watch?v=Z3SYDTMP3ME

	Edge Locations - We can consider it as a caching devices. All the media like fb, youtube videos, images they get cached at the nearest Edge locations and when user request for these media instead of going back to seerver and fetching it from storage, they are directly served from these Edge locations, hence latency is low. 	

	
	Global services - IAM, Billing, Route53, etc.
	
	Regional services - S3, DynamoDB

	Availability services - EC2, RDS, EBS. 
							We will have one instance of EC2 on one availability zone. That means one EC2 instance can't be in multiple AZs, it will be in only one AZ.
							
	Now, let's try to build an Social Media application. 
	First we will see how we can build without AWS services and then we will see how we can build it using AWS services.
	
	
	Check Diagram - fb-architecture-without-aws-on-premises.JPG
	
	Now if we want to deploy this fb.com application on our on premise data center then first thing we will need is Private Network. 
	
	Next we will require Web Server. So if we want to extend out application and have some business logic and UI then we will also add an Application Server. So frontend will be taken care by Web Server and backend will be taken care by Application Server. 
	
	We might also need a Relational Database. 
	
	And users will be using this application by using an IP address. 
	
	So till now we have this three tier application.
	
	Now as our application is becoming popular, and number of users is increasing then at some point our Web server and Application server becomes bottle neck, they are not able to handle the increase load on our application. 
	So the solution to this would be scaling, typically it would be vertical scaling means we increase the capacity of these machines (by add some hardwares) or we can do haorizontal scaling, that means we bring more web servers and more application servers. 
	
	Now as we have multiple web servers now that means there will be multiple ip address, so now we need an intelligent entity who can distribute loads between these web servers. That's were we bring in Load Balancer service.
	So user will hit request to Load Balancer and load balancer will evenly distribute the load to backend servers. 
	
	So now we don't want people to access our application using ip address, we want them to use some domain name. So we need some DNS service where we will map domain name with the ip address.
	
	Now everything is fine but now we are having lot of data and our relational database is not able to handle those. So for these we need scalable databases. So we bring in the No Sql database. So now some part of data is stored in relational database and other in No Sql database. 
	But still our relational database could be a performance bottle neck. May be there some read heavy operation happening on this db, so for that we bring in DB Cache engine. So frequently accessed data will be cached here and our application won't always have to hit the db. 
	
	So our fb.com might be getting millons of picture and video upload daily, and the disk which are attached to the VMs are not capable of extending on the fly, they have size restrictions and thus all this media is never stored on Web servers or an application servers. 
	For this we need some uplimited kind of storage and that's where we use External Storage. And this should not be necessarily block storage like our disk, it can be a file storage. 
	
	Now when we upload some photos, videos we need some kind of Content Filter, so that we can filter the content which is not permited. 
	
	Now we know that fb also continuously throws ads and it also continuously watching what activity we are doing while we are on fb page, like what kind of products we are liking, what posts we are liking, And based on that it gives use suggestions, ads, etc. So this is called Click Stream Analysis. Every click is being captured some where and it is getting analysed in real time. So we need some kind of Click Stream Analysis engine there.
	
	Now whatever data this Click Stream Analysis capture it has to be further stored into some kind of External Storage. And future we can use this data to run some operations, like aggregations, sort, find some meaning out of it. So for that we need some kind of Hadoop/Spark platform.
	
	And over the time we will also need a Data Warehouse, because fb does lot of data analytic, like may be at the end of the year they want to know which kind of user are accessing fb more, there age, region they come from, what is trending, etc. All this info is taking out from some kind of data warehousing engine and then doing some kind of Business Intelligence on top of it. So we need some kind of Business Intelligence tool which will query this data and analyse this data and generate some kind of reports from it. 
	
	Now all the photos and vedios stored in the external storage can be directly served on the internet. So user may come from web browser and watch a video. 
	But our user can also come from mobile devices. And in that case we will need same video but in some kind of different format because mobile divices might play a different format of the video. And this will need some kind of video converter. 

	Now whenever some video get viral, millons of people watch that video, now everytime if that video is fetch from the External Storage, this might become a bottle neck. So to solve this problem we have something called CDN(Content Delivery Network). 
	CDN will cache these videos in the nearest caching devices from where the user is accessing your videos. So that all the user's from that geography when they want to watch that same video it can be served from CDN and not from external storage. So user will experience a low latency.
	So application like fb, youtube have a large CDN networks from where they serve videos.
	
	Now we know that fb also sends us mobile notifications, for this we need some kind of notifications service like SMS Mobile Push Notifications.
	
	Also it can send Emails, so we need an Email service.
	
	We can also chat in fb, for this we need a Messaging Queue service.
	
	And we will also need some kind og Monitoring Service to monitor how our VMs is doing, how our db, storage is doing, etc.
	
	
	Now we will see if we want to do same thing on AWS, then how we will do this.
	
	Check Diagram - fb-architechture-with-AWS.JPG
	
	
	So the Private Network we had in AWS it is called VPC(Virtual Private Cloud). 
	
	All the VMs we had are nothing but EC2(Elastic Cloud Compute) machines. 
	We can have auto scaling enabled for EC2.
	
	And the Disk we had are EBS(Elastic Block Storage). They have limitation of maximum size. 
	
	For Relational Database there is a service called RDS and for No Sql DB there is service called DynamoDB.
	
	For DB-Cache we have service called ElastiCache. 
	
	For Load Balancer we have ELB (Elastic Load Balancer) Service.
	
	For DNS we have Route53 service.
	
	For External Storage we have S3 service. It is unlimited storage and accessible over the internet directly. No size limitation on how much data we can store in S3 buckets.
	
	Also we need some Content Filter, so for that we have Rekognition service which can filter out an objectible media and it can filter it out before uploading to S3 bucket. 
	
	As we need some service to convert videos from one format to some mobile friendly format, for this one option is, we can have some EC2 instances which continuously watch our S3 bucket for new videos and as new video comes they download it there and converts it to some format and put it to some another S3 bucket. 
	But there is a better option for this, that is Lambda service. Lambda is a serverless service of Amazon, where we just write a code to convert a video and we can execute this whenever a new upload is happening in S3. So when new video come Lambda function is trigger, conversion is done and it may upload the converted video to some new S3 bucket.
	So here there are no servers to manage. Everything is taken care by Lambda and they scale automatically.
	
	For Click Stream Analysis we have a service called Kinesis.
	
	For Spark/Hadoop there is a service called EMR. 
	
	And we also need to do ETL transactions from our DynamoDB tables. So we need Glue service to do extract, tranfrom and load operations. 
	
	For Data Warehouse we have Redshift service. 
	
	For Business Intelligence we can use Amazon Quicksight and we can also use Athena. 
	Athena is an SQL query interface. So we can pull data from S3 perform some sql operation on that and all those results can be seen in Quicksight.
	
	For CDN there is CloudFront service. 
	CloudFront service stores or caches the data in Edge Locations. 
	
	For SMS, Mobile Push notifications there is SNS(Simple Notification Service) service.
	
	For Emails there is SES(Simple Email Service) service.
	
	For Messaging Queue we have SQS (Simple Queue Service)
	
	For Monitoring we have CloudWatch service. 
	We can also set alarams here.
	
	
	Now we will see some Application Services. Check Diagram - AWS-application-services
	
	
	We know that fb, twitter, amazon it exposes all its service via an API calls so that different 3rd party application can integrate with this applications. And for that they need an REST API service where they can expose all there APIs. So in Amazon we can have managed API Gateways. It takes care of scaling, throtling, everything.

	Web and Mobile User Management Service - Cognito
	
	
	
	Security Service. Check diagram - AWS-securities-services.JPG
	
	IAM(Identity and Access Management) service.

	Encrypt our data at various storage locations like EBS, S3, RDS, DynamoDB, SQS, EMR, Redshift, etc. using KMS(Key Management Service) service. KMS manages all encryption keys for you. We don't have to store keys.
	
	As we know these application will be probabily accessed over HTTPS, So we can secure this communication using digital certificates. We can deploy it on either Load Balancer or we can deploy it on CloudFront so that our communication is secured. For this AWS has service called ACM (Amazon Certificate Manager)
	
	We can also have the application Firewalls. They are called WAF(Web Application Firewall). It can prevents attacks like SQL injection, etc. We can deploy it on ELB, CloudFront or API Gateways.
	
	AWS Inspector service makes sure our machines are free from vulnerabilities. It puts an agent inside our machine and scan our machine for any known vulnerabilities. And then it gives us reports.
	
	
	We will now see AWS Development and DevOps Service.
	Check diagram - AWS-development-and-DevOps-services.JPG
	
	In our architechture we have lot of services and they are all connected. So if we want to deploy everything by hand may be manually then it will take lot of time. Couple of days.
	With AWS it gives us ability to code our infrastructure. That's called Intrastructure as a code. So for this we can have a service called CloudFormation. It takes kind of template from us which is in JSON or yml format and it will just create this infrastructure from scratch for us. It takes around max 30 mins.
	The template can be written by DevOps people.
	
	At the same time we have DevOps, Developers, QAs and everyone need some kind of code repository. For that AWS has CodeCommit service, where they can checkin the code. 
	So DevOps guys write that template, put it in CodeCommit and CloudFormation can pick it up from there. 
	
	Once we have our infrastructure up, we actually need our application to be build. For that we have CodeBuild service. So the Amazon CodeBuild will take the source code from CodeCommit and build it using some kind of build tool like Maven, Ant, etc. While building it will also do some unit tests and it produces some artifacts. 
	
	Now we can deploy artifacts using CodeDeploy service. 
	
	This is Continuous Dilivery.
	If we want this pipeline to be automated, like developer checkin code, it gets build and then deployed automatically. Then we can have a CodePipeline service. 
	We can completely build our CI platform here. 
	
	And if we want to integrate all this with Project management tools like Jira, etc. then we can use CodeStar service. CodeStar is project management, issue tracking, continuous Delivery service. 
	It integrate well with Atlassian Jira and other tools. 
	
	
Section 6: Amazon EC2 instance in a Public Subnet.

	Within region we have VPC. We can think of it as a Virtual data center, so everything within this VPC is just for us. Its logical, it doesn't mean we have any Physical segmentation within the AWS cloud. 
	
	
	Select an existing key pair or create a new key pair - we now need to selct or create a key pair.
	So a key pair is something which consists of a public key and a private key. 
	So this is a way of securely connecting to our instances using public key cryptography.
	
	You have to download the private key file (*.pem file) before you can continue. Store it in a secure and accessible location. You will not be able to download the file again after it's created.
	

48. Instance user data and Instance metadata

	User data is a data that we enter in our EC2 instance at the launch time. 
	Its a series of commands. And those commands are run as the instance is launched. 
	
	Metadata is the way that you can learn information about your instance and you do so by running a command which is a URL and that URL is a local address. 
	(The address in URL shown in slide is local to machine, it doesn't make any sense anywhere else.)
	
	So that's what metadata is it's information about your instance and its located at this location(URL).
	
	So in exam if question comes like how would you find address or availability zone or region then metadata would be the answer and not user data. 
	
	There is also the instance Metadata Query tool that allows you to query the instance metadata, so we don't have to use command line.
	
	
49. Instance User Data and Metadata Hands-on
(optional)

	We will see how we can lauch a web server using the user data.
	
	In step 3: Configure instance details, if we scroll down we have a option to put user data. 
	
	This time in Step 6: Configure Security Group we will select option - Select an existing security group and then select the Security that has HTTP configured. If HTTP is not configure we can add it by going into security menu and then come back here.
	We want HTTP because we will connect throught Browser and if it was SSH, we would have need SSH client or Putty.
	
	So what's going to happen now is the instance is going to launch and it's going to run the user data we provided and that user data remember told it to go and install some software. So it's going to install the Web server software. So it's going to go out via the Internet gateway to the internet and it's going to pull down that software and it's going to install the web server on this instance and we are then going to connect using HTTP so we are going to use our Web browser to connect. 
	
	If we now copy the Public DNS and paste it in browser we would get reply.
	
	Using this we have actually installed a web server on our EC2 instance. 
	
	To check metadata we can connect to our instance using Putty and then do curl http://169.254.169.254/latest/meta-data
	We can then see that it has recorded some meta data.
	We can then query this like - http://ip-address/latest/meta-data/instance-id. 
	
	
	[ec2-user@ip-172-31-10-90 ~]$ curl http://169.254.169.254/latest/meta-data
		ami-id
		ami-launch-index
		ami-manifest-path
		block-device-mapping/
		events/
		hibernation/
		hostname
		identity-credentials/
		instance-action
		instance-id
		instance-type
		local-hostname
		local-ipv4
		mac
		metrics/
		network/
		placement/
		profile
		public-hostname
		public-ipv4
		public-keys/
		reservation-id
		security-groups
		[ec2-user@ip-172-31-10-90 ~]$

		
		[ec2-user@ip-172-31-10-90 ~]$ curl http://169.254.169.254/latest/meta-data/hostname
			ip-172-31-10-90.ap-south-1.compute.internal


50. Amazon Elastic Container Service (ECS)

	Slides are self explainatory. Its a container running on top of EC2 instance (Node). 
	
	Next we will see how to create a container running nginx on top of Amazon Fargate. So it's a web service that's going to run as a container on Amazon Fargate. 


51. Launch Container using Fargate Hands-on
(optional)

	Here we will see how to create a container running on ECS.
	
	Along with Amazon ECS, we have couple of other options like Amazon EKS (Elastic Kubernetes Service). It is an implementation of Kubernetes, which is a open source container orchestration platform.
	
	We also have a Elastic Container Registry (ECR) which is where you can store images of containers.
	
	We also have Marketplace where we can purchase images of containers as well. 
	
	
52. AWS Lambda

	You don't have to manage any of the underlying servers. So with Lambda all you do is worry about code. 
	So you literally take your code and you put it onto Lambda and something called a trigger is executed.
	So for instance a trigger might be that somebody visits a certain web page or another service might trigger Lambda.
	And when that trigger occurs Lambda starts executing your code.
	
	So when we say serverless, there's obviously a server somewhere running which executes your code. The key point is you don't know anything about that server.
	
	CloudWatch is a monitoring service and it has a logging function.
	

53. Create Simple AWS Lambda Function Hands-on (optional)

	Execution role are permissions that enable the Lambda function to work.
	
	You may not see CloudWatch Logs connected to Lambda in your console as Lambda is now always connected by default.
	

54. Amazon LightSail

55. Create Amazon LightSail WordPress Instance Hands-on (optional)

56. Choosing the right compute service


Quiz 3: AWS Compute


	User data can be run at instance launch time. You can use it to run commands

	Amazon Elastic Container Service (ECS) is used to run Docker containers on AWS
	
	AWS Lambda is a serverless services that runs code as "functions"
	
	Amazon LightSail is great for users who do not have deep AWS technical expertise as it make it very easy to provision compute services
	
	An Amazon Machine Image (AMI) provides the information required to launch an instance. 
	AMIs used for Launching an Amazon EC2 instance.
	
	As AWS Lambda is a serverless service, there are no instance types to choose from
	
	Amazon Elastic Block Store (EBS) is used for the root volume on EBS-backed instances
	
	This is a key benefit of the AWS Cloud. You can elastically increase or decrease capacity by changing instance types whenever you need to
	

	
	
	EBS - It is a storage system that we can connect to a OS. It's very similar to a hard disk drive or SSD drive in fact that's what the storage system is made up of. .
	And then your instance can mount that volume and it does so in the case of EBS over a network.
	In many cases it would be a hard drive that's actually inside your computer but with a block based storage. 
	You are able to connect to the volume and create partitions and format file systems, install OS, etc.
	
	In EFS you actually connect using a protocol such as NFS and with NFS you then mount the file system to a mount point. That's just means that if you change directory to mount point directory you would see whatever's on this file system.
	

60. Amazon Simple Storage Service (S3)

	Amazon S3 is a public AWS service and that means you connect to it over the public internet so you migt have a browser on an internet based client and you connect to it. 
	
	Or we might connect from EC2 instance and that instance could be in a private or public subnet. But you still have to hit an Internet gateway which takes you out to public internet and then to S3.
	Now there is one exception called a gateway endpoint and it's a way of connecting from an EC2 instance to one of the AWS public services without going over the public internet. 
	
	Objects are nothing but files, of any type. 

	S3 is a global service but you have to choose where you store your data(in which region) and that's within a region. 


61. Amazon S3 Storage Classes

	There are six s3 storage classes: 
	- S3 standard (durable, immediately available, frequently accessed)
	
	- S3 Intelligent-Tiering(automatically moves data to the most cost-effective tier)
		This means that there are different storage classes and some more cost effective than others depending on how often you access your data and S3 will intelligently tier, in order word it's automatically going to move data to the most cost effective tier for your usage.
		
	- S3 Standard-IA (durable, immediately available, infrequently accessed)
	
	- S3 One Zone-IA (lower cost for infrequently accessed data with less resilience)
	
	- S3 Glacier (archived data, retrieval times in minutes or hours)

	- S3 Glacier Deep Archive (lowest cost storage class for long term retention)



	Suppose we have created S3. And when we are uploading the object at that time it asks for storage class. So storage class is someting like object level.
	
	To connect to S3 from EC2 instance, there are two ways, 
		- We can use Access key and Secret Access key to connect
		- We can create a role with permissions for EC2 to access S3
		
	First one is not ideal because when we connect using Access Key and Secret Access key a credentials files is created with these key in it. And this is not good.
	Thus we should always use second approach of role.
	

66. Amazon Elastic Block Store (EBS)

	An EBS volume is essentially like a virtual hard disk that's in the AWS cloud and you can attach these volumes to your EC2 instances. 
	And these are used as Boot volumes for your EC2 instance.
	And that means that it's like the disk device that your instance uses to find the data to load the OS when it starts up.
	
	So firstly we can take an EC2 instance and you can attach multiple EBS volumes to that instance. So one could be the boot volume with the OS and then another one might just be a data volume.
	
	But we cannot attache same volume to multiple instances.
	So that means you can't share that data between two EC2 instances.
	
	You also can't connect an EC2 instance to a volume in a different availability zone.
	
	EBS volumes are stored within an availability zone. 
	
	EBS provides persistent block storage, we can even terminate the instance and detach and EBS volume from it and keep that data in the volume.
	
	
67. Amazon EBS Volume Types

	The EBS General Purpose SSD(gp2) is default volume type when you launch an EC2 instance.
	
	Neither of the HDD options can actually be used as a boot volume so you can't choose this to boot your EC2 instance at launch time.
	
	
68. Amazon EBS Snapshots
	
	EBS snapshot are a way that you can take a back up of your volume.
	
	
	
	EC2 instance is not a whole virtual PC(server), One PC(server) can have multiple EC2 virtual instances. 

	Now the instance store volumes are actually physically located within the service so they're within this physical server. 
	
	So EC2 instance can connect to EBS over network 
	And instance store are in servers that host EC2 instances.
	
	So the instance store volume is local to the host server and is attached directly to an EC2 instance whereas an EBS volume is attached over a network connection.
	
	We cannot dettach/reattach the instance store volume so unlike an EBS volume where you can detach it and it can exists in your account independently you cannot do so with instance store volume.
	

71. Take snapshot, create AMI, launch new instance (optional)

	We have a EC2 instance attached to a root EBS. We will create a snapshot of this in S3 and then create a AMI and using this AMI we would create a new volume in different availability zone and attach it to a new EC2 instance in different availability zone.
	
	So this is a great way of creating a copy of your existing EC2 instance in different availability zone.
	

72. Amazon Elastic File System (EFS)

	EFS is a file system rather than a block storage system or an object storage system.
	And this means that it's a network based file system that you connect to so you connect to it over a network and you mount it as a remote file system.
	
	This is different EBS. EBS you connect to over a network but you connect to it at a block level. So that means it's a volume it's almost like having a hard disk in your computer. You can create partitions on it, you can format those partitions.

	With a file system you just mount it. It's like a network Drive or T drive we have in our company. 
	
	NFS(Network File System)
	
	We can connect multiple EC2 instances to a single File System.
	
	So it is a great solution when we want to share data. 
	
	That means that you can mount the file system to EC2 instances in multiple AZs within a region.
	
	You can also connect your on premises clients so you can connect over a VPN and then mount the EFS file system from a computer in your corporate data center.
	

73. Create and mount EFS File System(optional)
	
	Security groups are the firewalls to the instances.

	To connect to EC2 using cmd prompt => ssh ec2-user@ip-address -i mynewkeypair.pem
	
	
74. AWS Storage Gateway


	AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.

	
Quiz 4: AWS Storage

	
	Which Amazon Machine Image can be used to mount an Amazon Elastic File System (EFS) file system?
		- Amazon Linux 2 AMI and not Microsofts...
		
	Only Linux AMIs can be used with Amazon EFS


Section 8: Amazon Virtual Private Cloud(VPC)


76. Introduction

	A VPC is a logical construct into which you can create networking elements such as subnets.
	So just remember this is a virtual data center essentially so think of it as your virtual data center on AWS cloud.

	Many services are deployed into VPC.


	A VPC is a logically isolated portion of AWS cloud within a region. So its private and secured and controlled by you. 
	
	And we can create a multiple VPCs within a region.
	
	Within a VPC we create subnets and the subnets sit inside AZs.
	So each subnet is within one AZ. 
	
	A VPC has an address block, a block of address that can be assigned to all the instances you deploy. And a subnet just has a block within that, within the larger block.
	So it's a kind of smaller block of addresses within the overall block of addresses that are assigned to that network.
	
	Public subnet means that the EC2 instances in this subnet will get a public IP address so the address that they get is one that you can find on the Internet rather than a private address which is what you have for EC2 instances in private subnets.

	So how does conenctivity works?
	There's somthing called a VPC router so you don't actually see it but it's there.
	And the way that you interact with it or control it is through what we call a route table.
	
	Internet gateway is attached to a VPC but it kind of sits at the edge of your region because it is the gateway to the Internet itself.
	So all traffic going in and out of the from the Internet to your VPC and out again has to go via the Internet gateway. 
	
	So we can have multiple VPCs and each VPC has a different CIDR block.
	
	Subnet cannot go across availability zones, so it's always within an individual availability zone. 
	
	VPC Endpoints - Noramally if you are connecting from an EC2 instance to Amazon S3 it would be out in the region somewhere. Which means we actually have to go to the Internet gateway to reach it because it's got a public IP address. Even if you've got a public subnet with an EC2 instance with a public IP address it still has to go out the Internet Gateway and actually reach S3.
	If you have a VPC Endpoint, amazon S3 would be able to talk directly into your VPC.  
	
	A Virtual Private Gateway and a Customer Gateway are both parts of Virtual Private Networks(VPN). So a way to connect into Amazon over the public Internet and create a private or at least a virtual private network, so something that's encrypted and secure.
	
	AWS Direct Connect is another way of connecting from in this case a corporate data center or a corporate network into the AWS cloud and you create a private connection that doesn't go over the Internet.
	

80. Security Groups and Network ACLs

	
	A Network ACL actually controls access to the subnet. 
	So it actually is a firewall which controls access to traffic going in and coming out of your subnet.
	
	So suppose an EC2 in one subnet wants to talk to EC2 in another subnet, so it will have to first go throught it's ACL then it will go to Router and get routed to ACL of the target subnet and finally to the target EC2 instance.
	
	ACL are at subnet level.
	
	Security groups are at instance level.
	
	The key thing about security groups is you create them and then you attach instances to them and they then control the traffic going into and out of that instance. 
	
	So in some case if we have two instances in same security group in same subnet and if they send traffic to each other the security group will have to allow that traffic to go. 
	And suppose these instances could be in different security groups like we have in diagram, so in that case we need to have the rules that allow the traffic to go out from security group A and into security group B if these two want to communicate with each other.
	

81. Public Private and Elastic IP Addresses

	Public IP address - This is an address that is routable on the public Internet.
	That means that you can connect to your instance on its address from the Internet.
	When we shut down the instance we loose the address, we get new one next time it boots up.
	Associated with private IP address on the instance.
	
	So we will always have an private IP address on the instance.
	
	If we don't to loose public IP we can assign an Elastic IP, which is a static IP. They are also associated with Private IP of instance
	
	Public IP used in public subnet.
	
	Private IP used in Public and Private Subnet.
	
	We don't loose Private IP.
	

82. NAT Gateways and NAT Instances

	NAT - Network Address Transalation.
	
	So we have an EC2 instance in a private subnet. That means it only has a private IP address that means it can't communicate directly on the Internet.
	
	So how do we get this instance to talk to a device out on the Internet?
	
	What we do is we create a NAT gateway in a public subnet. And that NAT gateway has both a private IP address so it can communicate with the EC2 instance and it also has an elastic IP which is a type of public IP.
	
	So that means that EC2 instance can send traffic to the NAT gateway and it will forward it out to the Internet.
	So it does what's called Network Address Transalation.
	
	So we need to create NAT gateway in management console then we need to update our route tables so that we have a route in private subnet table that goes to the NAT gateway ID.
	
	Other way is to use NAT instance, which is actually an EC2 instance

	So an EC2 instance that we launch from a specific AMI and you set it up so that it does the same thing as a NAT gateway.
	
	A bastion host is an EC2 instance in a public subnet that you can connect to on the from the Internet and then use that to jump in to an instance on a private subnet for management purposes.
	
	Means we create an EC2 instance in the public subnet which we will use to connect in and connnect to our EC2 instance in the private subnet. That's what we call a bastion host.
	
	
Quiz 5: Amazon VPC

	An Amazon VPC is created within a region. You can created multiple VPCs within a region and there is a default VPC created in every AWS region by default

	How can an organization create a private hybrid cloud connection between their on-premises data center and the AWS Cloud?
	AWS Direct Connect is a private network connection to the AWS Cloud. It provides high bandwidth and low latency with reliable performance
	Using AWS direct connect we are connecting from Corporate's private cloud to AWS(public cloud). Thus hybrid.
	
	With Elastic IP addresses, the address is retained when the instance is stopped. Remember that you do pay for unused Elastic IP addresses
	
	Which AWS-managed network service can be used to enable Internet connectivity for EC2 instances in private subnets?
	A NAT Gateway is an AWS managed service that can be used for enabling instance in private subnets to access the Internet
	
	A company needs a network connection to the AWS cloud with predictable performance. What should they use?
	AWS Direct Connect is a private network connection and offers predictable performance
	AWS managed VPN is incorrect because a VPN uses the public Internet, it doesn't offer predictable performance.
	
	
Section 9: AWS Databases

90. Amazon DynamoDB

	DynamoDB is serverless. So that means it's a fully managed fault tolerant service and you don't have to provision the underlying service so it's not like your provisioning in EC2 instance or having to change an instance type when you need to scale its service.
	So like all the serverless service you don't need to worry about the compute capacity and storage capacity on which it is running.
	
	There's also something called Global Tables so you can actually have a fully managed solution where it's a multi region multi master database. 
	That just means that the daabase is in multiple regions around the world the same database and it's replicated, but multi-master means that you can actually write to all of those different database and all the data is synchronizes.
	
	In hand on lab - we don't actually create a db, because it exists, we are more specifically creating a table in dynamo db. becasue this is a managed service.
	
	
	
93. Amazon ElastiCache

	Memcached and Redis are examples of open source in-memory databases. 
	So it's about in-memory caching and that helps to improve latency and throughput for read heavy application workloads or compute intensive workloads. 
	
	So ElastiCache is a in-memory database and it does in-memory caching.
	
	Use cases - 
		- Web session store - that means you have an application can the seesion state for that application information about a user seesion that's currently running is stored in ElastiCache and that measn that if a server is lost then the session information is still available.
		
		- Database caching.
		
		- Leaderboards
		
		- Streaming data dashboards.
		
		
Quiz 6: AWS Databases

	Amazon RDS is an example of a relational database used for online transaction processing (OLTP) workloads. This means its typically used for production databases that process transactions
	
	Which AWS database service offers seamless horizontal scaling?
	Amazon DynamoDB offers seamless "push-button" horizontal scaling

	How can fault tolerance be added to an Amazon RDS database?
	Multi-AZ creates a standby copy of the master DB in a separate availability zone
	
	How can an organization enable microsecond latency for a DynamoDB database?
	DynamoDB Accelerator (DAX) is an in-memory cache that increases performance of DynamoDB databases

	Which AWS database service is a relational, data warehouse?
	RedShift is a relational, SQL database that is well suited for data warehouse use
	
	Why might an organization decide to move an on-premises database to Amazon RDS?
	You can reduce operational overhead by moving to AWS managed services. With RDS this means you no longer need to manage the operating system.
	
	How do you increase the capacity of an Amazon RDS database?
	You can scale Amazon RDS by changing to a larger instance type. This is an example of vertical scaling
	
	DynamoDB is a No-SQL database which has a flexible schema and is good for unstructured data


Section 10: Elastic Load Balancing and Auto Scaling
	

	With ELB sometimes an instance becomes unhealthy and the load balancer will take it out of service (means will no more route there). It doesn't actually replace the instance. 
	
	This is where Auto-scaling comes in. With auto-scaling when an instance becomes unhealthy and that means it's failing a status check, the Auto Scaling group will replace the instance, it launches a new instance. 
	So that means every time you have a failure a new instance will be launched and will replace that instance.
	

Quiz : 

	How can a company emable elasticity for an application running on Amazon EC2?
	By using Amazon EC2 Auto Scaling.
	Amazon EC2 Auto Scaling enables elasticity for EC2 by launching and terminating instances as demand changes.
	
	Which type of Elastic Load Balancer can direct traffic based on the domain name?
	The application load balancer can do host-based routing which means it can direct traffic based on information in the host header such as a domain name.
	
	Amazon EC2 Auto Scaling launches and terminates instances as demand for your application changes, this ensures you are only paying for instances that you need to service demand.
	
	How does Elastic Load Balancing (ELB) assist with fault tolerance?
	By distributing connections to multiple back-end instances.
	ELB distributes connections to multiple backend instances and this means your application is fault tolerant. You should couple this with Auto Scaling to ensure the right number of back-end instances are available.
	
	Which of the following statements is INCORRECT about Elastic Load Balancing?
	ELB cannot distribute connections across regions, only availability zones. To direct traffic across regions use Amazon Route 53
	
	Health checks are used by ELB to check that an instance is available and healthy
	
	The NLB operates at layer 4 of the OSI model only, routing connections based on IP protocol data
	
	Scaling policies determine when, if, and how the ASG scales and shrinks (on-demand/dynamic scaling, cyclic/scheduled scaling)
	
	Auto Scaling Group (ASG) are collections of EC2 instances
	
	What type of template is used by Amazon EC2 Auto Scaling to define instance family, AMI key pair, and security groups?
	Launch configuration
	A launch configuration is the template used to create new EC2 instances and includes parameters such as instance family, instance type, AMI, key pair and security groups


Section 11: Content Delivery and DNS Services

	CloudFront is Content Delivery Network (CDN), Route53 is Domain Name System (DNS)
	
	
	DDos attach is a malicious attack where someone tries to send lots of data to your service to bring it down.
	
	
Quiz 8: Content Delivery and DNS Services

	
	How can an organization improve performance for users around the world accessing online videos?
	Amazon CloudFront can be used to get the content cached around the world, closer to users, which will improve performance
	
	A regional edge cache sits between a CloudFront Distribution and an edge location
	
	Which types of Origin does Amazon CloudFront support?
	You can use S3 buckets and EC2 instances as origins for you CloudFront distribution. You can also use S3 static websites, other HTTP servers using Route 53, instances behind an ELB, and MediaStore Containers
	Can not user - RDS database, EFS filesystem, Auto Scaling Group

	Which services have a Global scope?
	AWS IAM, Amazon CloudFront, Amazon Route 53
	
	Regional - Amazon VPC, Amazon CloudWatch, AWS Lambda
	
	Which services does Amazon Route 53 provide?
	Health checking, DNS, domain registration
	
	Amazon CloudFront has built-in Distributed Denial of Service (DDoS) attack protection
	
	Which of the following is used to cache data to bring it closer to end users?
	Edge Locations are part of the AWS Global Infrastructure and are located around the world. They are used to cache content to bring it closer to end users for improved performance.

	In Amazon Route, what is the name for the configuration item that holds a collection of records belonging to a domain?
	A hosted zone represents a set of records belonging to a domain
	

Section 12: Monitoring and Logging Services

	Amazon CloudTrail is a service that keeps a record of events that happen in our AWS account. 
	Everytime yo create or modify a resource in AWS a record of that event is sent to a CloudTrail and that's the API calls. 
	So AWS is a web service and that means every time you perform any action in AWS an API call is actually being generated. 
	So you might go through the console and create and EC2 instance or you migt use the command line to modify VPC. Whatever you do an API call is actually generated and CloudTrail records all of those API calls.
	
	
	In CloudTrail by default we can see events for last 90 days, if we want any longer then we need to create a trail and log to an S3 bucket.


Quiz 9: Monitoring and Logging Services

	Which service can be used for alerting if the CPU is heavily loaded on an EC2 instance?
	CloudWatch is used for performance monitoring

	Which statement about Amazon CloudWatch is INCORRECT?
	CloudWatch only integrates with Amazon EC2 - This is not true. CloudWatch integrates with most AWS services
	
	Correct statements - 
	- CloudWatch Logs collects and centralizes logs from AWS resources
	- CloudWatch Alarms can be set to react to changes in your resources
	- CloudWatch monitoring can include application performance.
	
	
	Which service can be used to record information about API activity in your AWS account?
	CloudTrail can keep a record of all API activity in your account

	Does Amazon CloudTrail permanently record all API activity in your account by default?
	No.
	By default Amazon CloudTrail only keeps 90 days of records. To keep records permanently you need to create a Trail and record events to an Amazon S3 bucket
	
	
Section 13: Automation and Platform Services

	CloudFormation and Elastic Beanstalk, these technologies allow us to automate the build of our infrastructure and our applications in the cloud.

	CloudFormation is all about "Template-driven provisioning" that means you deploy infrastructure using code.
	
	Elastic Beanstalk is more focussed on deploying applications on EC2(PaaS)
	
	CloudFormation is actually used by Elastic Beanstalk. 
	When you create a beanstalk enviornment it actually goes off and talks to CloudFormation and CloudFormation builds out the stack.
	
	Elastic Beanstalk builds what we call Elastic Beanstalk container.
	
	So when we create using Elastic Beanstalk, it launches CloudFormation, CloudFormation creates the enviornment, it will create Auto-scaling group, EC2 instance and install enviornment for our application and deploy. 
	Developer will just upload artifact, select enviornment and all infrastructure is creation is taken care by Elastic Beanstalk. 
	
	Same way if we do delete from Elastic Beanstalk it will use CloudFormation to delete the enviornment it created.
	

Quiz 10: Automation and Platform Services

	An organization is looking for a way to deploy infrastructure on AWS in different regions whilst ensuring consistency configuration. Which service should the organization use?
	Using the "template-driven" provisioning of CloudFormation organizations can build infrastructure on AWS in multiple regions using the same template file, ensuring consistency in their builds
	
	Which service can assist a developer with quickly deploying and managing a web application on AWS?
	AWS Elastic Beanstalk can be used to quickly deploy and manage applications in the AWS Cloud
	
	Which service uses JSON or YAML template files to deploy infrastructure as code?
	AWS CloudFormation deploys infrastructure using code and uses JSON or YAML template files
	
	Elastic Beanstalk actually uses CloudFormation to deploy services used in the environment
	
	Elastic Beanstalk is considered to be a PaaS service. This means the underlying infrastructure and the runtime engine are managed for you and you only need to upload the code
	
	Which service can be used to automatically create an Amazon VPC and then launch and EC2 instance, Auto Scaling Group and Elastic Load balancer?
	AWS CloudFormation can automate the entire process of creating a VPC and launching many different types of resources into it
	

Section 14: Migration and Transfer Services
	
Quiz 11: Migration and Transfer Services

	Which service can be used to migrate exabytes of data into the AWS Cloud?
	Snowmobile is “exabyte scale” with up to 100PB per Snowmobile
	
	Which service can be used to quickly and affordably migrate 50TB of data to Amazon S3 for a company with a slow Internet connection?
	This is the best solution for this amount of data as the company has a slow connection
	
	How can a company migrate a database from Amazon EC2 to RDS without downtime?
	Context makes this one a bit easier! Just remember that DMS can migrate databases online without downtime and it supports many different source / destination options

	You get charged for outbound data transfer. Data going out.
	You get charged for data that goes in between regions.

	
	Computing pricing - 
	- Detailed monitoring(CloudWatch) every one minute your EC2 instance is sending data to CloudWatch. By default basic monitoring is enabled which is every five minutes and that's free.
	

Section 15: AWS Billing and Pricing
	
Quiz 12: AWS Billing and Pricing

	Which pricing model is best suited for a batch computing workload that requires significant compute power and can be stopped at any time?
	Spot instances are great for this type of workload. You can achieve significant discounts which will mean a big cost saving for such a compute intensive workload. You can be stopped at any time if AWS need the capacity back but that's OK for some batch workloads.
	On-demand would be very expensive for this type of workload.

	Which AWS services are free?
	Amazon EC2 Auto Scaling, CloudFormation, IAM
	However you do pay for resources created by Auto Scaling and CloudFormation

	With Amazon S3, which of the following are NOT chargeable items?
	You do not pay for inbound data transfer, only outbound data transfer

	What is the most cost-effective storage tier for data that is not often accessed, will be retained for 7 years, and needs to be retrievable within 24 hours?
	Amazon S3 Glacier Deep Archive - This is the most affordable option for long term data storage where retrieval times of 24 hours are acceptable.
	
	What is a key cost advantage of moving to the AWS Cloud?
	You can provision what you need and scale on demand - This is a great reason to move to the cloud and a key cost benefit. This means you are only ever paying for resources that you are actually using, with little or no idle capacity	

	Which storage classes are available for the Amazon Elastic File System?
	Standard and Infrequently Access Storage - these are the two storage classes available for EFS

	With Amazon Virtual Private Cloud (VPC) what must you pay for?
	You do need to pay for VPN connections
	
	What can you use to assign metadata to AWS resources for cost reporting?
	Tags and resource groups are great tools for assigning metadata to AWS resources and then being able to group resources that share one or more tags
	
	Which AWS pricing feature can be used to take advantage of volume pricing discounts across multiple accounts?
	With Consolidated billing you can combine the usage across all accounts in the organization to share the volume pricing discounts and Reserved Instance discounts. This can result in a lower charge for your project, department, or company than with individual standalone accounts
	
	What is the best tool for an organization to compare the cost of running on-premises to using the AWS Cloud?
	The TCO calculator is a free tool provided by AWS that allows you to estimate the cost savings of using the AWS Cloud vs. using an on-premise data center
	
	Which AWS support plan comes with a Technical Account Manager (TAM)
	Only the enterprise support plan comes with a TAM
	
	Which of the following needs to be considered in a Total Cost of Ownership (TCO) analysis?
	Data center operations costs
	You need to include all costs that are being incurred to run on-premises so you can compare costs to the AWS cloud

	Which of the following is NOT a payment option for Amazon EC2 reserved instances?
	All at the end
	This is not a payment option for Amazon EC2 reserved instances
	
	Which tool can an IT manager use to forecast costs over the next 3 months?
	The AWS Cost Explorer is a free tool that allows you to view charts of your costs. You can view cost data for the past 13 months and forecast how much you are likely to spend over the next three months











































	