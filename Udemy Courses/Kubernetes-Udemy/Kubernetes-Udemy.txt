Master Kubernetes With Docker On Google Cloud, AWS and Azure

Section 1: Introduction

Kubernetes Crash Course for Java Developers - Course Overview

	Github Repository for the course => https://github.com/in28minutes/kubernetes-crash-course

	Project 01 => Pods, Cluster, Deployments, Replica Sets and more

	Project 02 and 03 => Config Maps, Secrets and Persistent Volumes
	
	Project 04 and 05 => Microservices with kubernetes DNS, Service Discovery and Load Balancing
	
	Project 06 => Explore Spring Cloud kubernetes - Ribbon and Centralized Configuration
	
	Project 07 and 08 => Centralized Logging and Distributed tracing with StackDriver (Google Cloud)

	Project 09, 10 and 11 => Service Mesh with Istio - Release Strategy, Tracing and more...
	
	Project 12 => Automate Deployments with Helm.
	
	
Section 2: Getting Started with Kubernetes and Google Kubernete Engine (GKE)

Step 01 - Getting Started with Docker, Kubernetes and Google Kubernetes Engine

	Check diagram C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\WhyDoWeNeedDocker.jpg
	
	From the diagram =>
	
		Standardized Application Packaging:
			Same packaging for all types of applications
			
		Features: 
			- Language Neutral
			- Cloud Neutral
			- Enables Standardization
			
		Challeges:
			1000 microservices
			1000 Instances

	Why Do we need Docker:
		
		Lets consider a scenario, we have friend in Operations team, and we want to quickly deploy an application. We go to him and say I want to deploy an application. He says what's you docker image? We gave him docker image and he says he will run a simple command and as he runs the command, we can see our application is being deployed within seconds and when we hit the url the application is accessible. 
		
		Now think about this our friend didn't ask us what language we build our application in, what frameworks did we use, what is the OS the application needs to be deployed on, what is the configuration that our application needs. Thing is he doesn't really need all that. This is what docker enables. 
		
		Developers build the image and we can run the same image where the container runtime is installed. We can run it in our local machine, in the enterprise data center or in the cloud. 
		
		Docker enables standardization of how we package and deploy our application, irrespective of the language or frameworks which is used to build the application or the platform where we would want to deploy the application to. 
		
		Now we are really happy with docker and we keep the application running with docker. Now couples of days later, we go to our operation's team friend and ask him, do you know if my application is still running? I want it to be always running. And by the way, I am expecting a lot of load this weekend so I would want to be able to increase the number of instances whenever there is lot of load. And also there are number of other applications that are coming up very soon. I won't be able to deploy and manage all that. 
		Now our operational team friend gives up, he says no thats not what an docker does. 
		So we do research and find out the brand new tool in the market - kubernetes. 
		
		Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\kubernetes-Architechture.jpg
		
		From the diagram => 
		
			Container Orchestration: Manage 1000's of instances 1000's of microservices declaratively
			
			Features:
				- Auto Scaling
				- Service Discovery
				- Load Balancing
				- Self Healing
				- Zero Downtime Deployments
				
			Cloud Neutral:	Standardized platform on any infrastructure
			
	So we are now the expert on kubernetes and we have created an amazing kubernetes cluster, which is nothing but group of servers that are managed together. And we are now connected to the cluster. And its our friend's turn to specify the requirements right now. 
	He says, I would want to quickly deploy an application  with this image. You say, thats easy, we just need to run one or two commands and its deployed and available to him.
	He says, I could have done this using docker using just one command. kubernetes is making it complex, you are executing two commands to expose the deployment. 
	We tell him, hey there is a lot of power to what we are doing, wait and see. 
	You show him that application is live and running in browser and ask him what more you want?
	He says I want 3 instances, you say I will just run one more command for that. And within second we can see that 3 instances are coming up(I can see that they were on same port). We can also see that load balancing is happening automatically, load is being distributed amongst different instances that are present in there. 
	We tell him hey see the magic with kubernetes, with just three command we wer able to create multiple instances of the application and also do load balancing. 
	Our friend is still no satisfied, he says, I would want these instances to be running all the time. Even if one of the container fail, even if one of the instance goes down, I would want another instance to come up immediately. 
	We tell him ok, I will execute one more command and delete one of the instance. And lets keep refreshing, we can see that the application doesn't goes down as one of the instance goes down. And we would see that within few seconds another instance of the application would come up automatically. 
	We tell him kubernetes does lot of magic, we don't need to monitor our application,, kubernetes does that for us. 
	We tell kubernetes that we want three instances, kubernetes will always keep those many instances, even if one of instance goes down, kubernetes will do all it can to bring another instance up. 
	
	Our friend has another request, he says I would want to increase number of instances when there is more load. During the weekend I am expecting a lot of load, but there after I may immediately go down. Also during the day I will have lot of load and during evening there will be no load at all. And I don't want to manually increase the number of instance and decrease the number of instances. 
	We execute another instruction to do auto-scale and say okay this is taken care of. 
	
	He asks for one more requirement, he says he would want to make a new release without anything going down. I would want to go from 0.0.1 version of application to 0.0.2 version of application without anything going down. We tell him that its little complex and we have a solution that works 90% of times and we do it and after refreshing the browser we can see that everything is up and within seconds new version of application comes up as well. And within few seconds we can see that new release application is the only on live. 
	
	With just few command we were able to enable powerful features on our application. We made it auto-scale based on the load. We also so that kubernetes were automatically able to do load balancing between the present instances. We also saw that kubernetes was monitoring the whole thing and whenever any instance goes down, he was able to bring another instance up. And we were able to release the new version of application with zero downtime. 
	
	The amazing thing with kubernetes is the fact that, we can do all these things with 1000s of microservices having 1000s of instances in a declarative way. We tell kubernetes, I need 100 instances of this microservice running and we don need to do anything, kubernetes will take care of it.
	
	On top of this kubernetes is cloud neutral. There are solutions provided by different cloud providers - AWS, Azure, Google Cloud, etc. each one having there own kubernetes solution and we can install kubernetes on any of these cloud provider. So if we are deploying any of the application to kubernetes we can run in any of the clouds. 
	
	Now where do these microservices run, we would need a kubernetes cluster or group of servers. 
	
	One of the interesting thing about kubernetes is, setting up kubernetes is not easy, creating and managing kubernetes clusters can be really tough. And that's where we use Google kubernetes Engine(GKE). GKE is a managed service provided by Google Cloud Platform(GCP). So we don't really want to worry about creating kubernetes cluster or managing it, GKE takes care of it for us. 
	
	And amazing thing is we would be using same infrastructure which google uses to run youtube, google maps, gmail and google search. 
	
	We can also do deployments using just Docker, by above additional feature will not be provided. 
	
	
Step 02 - Creating Google Cloud Account

	Lot of concepts to understand. We will work with google cloud, so few google cloud concepts as well.
	Complex things will be broken down into small things. 
	
	We will start with deploying a simple REST API with kubernetes. 
	
	To do thing this we would need a 
		- Google Cloud free trail account. 
		- We will create kubernetes cluster 
		
		and then we will all set to deploy our REST API. 
		
	Login to your google account and go to - https://cloud.google.com/

	Access to all Cloud Platform products
	Get everything that you need to build and run your apps, websites and services, including Firebase and the Google Maps API.

	$300 credit for free
	Sign up and get $300 to spend on Google Cloud Platform over the next 12 months.

	No auto-charge after free trial ends
	We ask you for your credit card details to make sure that you are not a robot. You won't be charged unless you manually upgrade to a paid account.

	Once account is created, our google cloud comes with $300 credit and with 12 months validity.
	
	
Step 03 - Creating Kubernetes Cluster with Google Kubernete Engine (GKE)

	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\kubernetes-Cluster-Architechture.jpg

	From diagram =>
	
	Cluster - 
		Master Node(s) => Manages Cluster

		Worker Node(s) => Run your application

	kubernetes - Best resource manager ever. Manages resources. So important resources that kubernetes manages is your servers. And these servers are in the cloud. So they are virtual servers, they are not real servers but they are actually servers in the cloud which are typically called virtual servers. 
	
	The fact is different cloud providers have different names for this virtual servers. 
	Amazon calls them EC2 (Elastic Compute Cloud)
	Azure calls them Virtual Machines
	Google Cloud calls them Compute Engines
	And kubernetes uses very generic terminology and calls them Nodes. 
	kubernetes can actually manage thousands of such nodes. 
	
	Now when we have thousands of things to manage we introduce a manager. To manage thousands of kubernetes nodes we have few Master Nodes. 
	Typically we will have one master node, but when we need high availability we go for multiple master nodes. 
	
	So Cluster is nothing but a combination of nodes and master nodes. 
	
	The nodes that do the work are called worker nodes or simply nodes.
	The nodes that do the management work are called master nodes.
	
	Master nodes ensure that nodes are available and doing some useful work.

	Now in google cloud we will first enable kubernetes engine. 
	
	Search kubernetes engine, it will open kubernetes dashboard.
	
	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\kubernetes-Dashboard.jpg
		
	From diagram:
		
		Clusters: Here we can create cluster and manage them.
		
		Workloads: Here we can manage application or containers that we want to deploy into the cluster. 
		
		Services & Ingress: Some of our workload may actually be REST API or web applications, we would like to provide access to external world to these workloads and thats where Services come into picture. Services gives external world access to applications which are deployed into kubernetes clusters. 
		
		Applications: 	
		
		Configuration: To storege our application configuration. 
		
		Storage: To provide persistent data storage. 
		
		
	While creating a Cluster we will have to choose how powerful our cluster is and where it should be located. You will need to choose the type of nodes you want, the number of nodes you want and there location.
	
	
Step 04 - Review Kubernetes Cluster and Learn Few Fun Facts about Kubernetes

	kubernetes abbrevation is K8S. 
	
	kubernetes pronountiation - KOO-BER_NET-EEZ
	
	Kubernetes in Greek means "Helmsman of a Ship"
	
	kubernetes on Cloud =>
		Azures calls it AKS (Azure kubernetes Service)
		Amazon calls it EKS (Elastic kubernetes Service)
		Google calls it GKE (Google kubernetes Engine)
		
		
	When we created a node we created them with 1 VCPU and about 3.75 GB of memory. However if we see it is only 2.77 GB of memory available to each of the node. The rest of the memory is being used by kubernetes. 
	kubernetes needs to manage the nodes and it needs this memory and cpu for the same. kubernetes have something installed on each of these nodes to manage them.
	
	
Step 05 - Deploy Your First Spring Boot Application to Kubernetes Cluster	 
	
	To deploy an application to a kubernetes cluster, first we need to connect to the kubernetes cluster. 
	
	How do we connect to cluster? We will use command line. Google cloud provides us something called Google cloud Shell.
	First go insde the cluster and then click on Activate Cloud Shell icon on top right corner.

	Manage your projects and resources from your web browser with Google Cloud Shell, an interactive shell environment.
	With Cloud Shell, the Cloud SDK gcloud command-line tool and other utilities are pre-installed, fully authenticated and up to date.
	
	To connect we can click on Connect button and then copy the command in there - 
	
		gcloud container clusters get-credentials in28minutes-cluster --zone us-central1-a --project boreal-gravity-271305		
			
			Its a google cloud command to connect to a cluster. 
			boreal-gravity-271305 is the name given to our cluster. 
			
	Now we are connected to the cluster, and we would want to run commands against this cluster. How do we do that? Thats where kubectl(Kube Controller)... comes in picture. 
	kubectl is an awesome kubernetes command to interact with the cluster. 
	kubectl will work with any kubernetes cluster. Irrespective of whether the cluster is in your local machine, or in data center or its in the cloud. 
	Once you are connected to the cluster you can execute command to any cluster using kubectl.
	kubectl can do a lot of powerful things with kubernetes like, you want to deploy a new application, you want to increase the number of instances of the application, you want to deploy a new version of application, etc.
	kubectl is already installed for us in Cloud Shell.
	
	aniketrajput90@cloudshell:~$ gcloud container clusters get-credentials in28minutes-cluster --zone us-central1-a --project boreal-gravity-271305
	Fetching cluster endpoint and auth data.
	kubeconfig entry generated for in28minutes-cluster.
	
	aniketrajput90@cloudshell:~$ kubectl version
	Client Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.10-dispatcher", GitCommit:"f5757a1dee5a89cc5e29cd7159076648bf21a02b", GitTreeState:"clean", BuildDate:"2020-02-06T03:29:33Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platfor
	m:"linux/amd64"}
	Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.10-gke.17", GitCommit:"bdceba0734835c6cb1acbd1c447caf17d8613b44", GitTreeState:"clean", BuildDate:"2020-01-17T23:10:13Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platform:"l
	inux/amd64"}
	
	Server version is the cluster that we are connecting kubectl to. 

	To deploy an application => kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE				//hello-world-rest-api, we can call it anything
	
		You might be wondering from where this image is comming from, what does it contains, from where is it comming from, how was it created? It is a docker image.   
		To get started quickly, they have already create a docker image and pushed it to the docker hub.

		aniketrajput90@cloudshell:~$ kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE
		deployment.apps/hello-world-rest-api created
	
	This will deploy the application to the kubernetes cluster and it gives a deployment id. 

	Now we want to expose this deployment to the outside world =>	
		
		kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
		
		aniketrajput90@cloudshell:~$ kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
		service/hello-world-rest-api exposed	

	In UI we can go in services and ingresses - check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\application-deployed-services-ingress.jpg

	We can click the endpoint from there and update the url to rest-api => http://35.239.36.17:8080/hello-world
	We can see that reponse is coming as expected. 
	
	Services are sets of Pods with a network endpoint that can be used for discovery and load balancing. Ingresses are collections of rules for routing external HTTP(S) traffic to Services.
	
	From just two command we were able to get entire thing working. We were able to pick up image from docker hub, deploy it to kubernetes and also expose it as a service to outside world. 

	In this step we did - started up the cloud shell, connected to the cluster, we created the deployment, we exposed the deployment and we were able to see a working application. 


Save Your Free Credits

	In the cloud, it is best practice to delete the resources when you are not using them.

	If you do not want to delete and create a new cluster every time, you can reduce the cluster size to zero.

	After finishing your classes of the day you can reduce cluster node size to zero.

	gcloud container clusters resize --zone <name_of_zone> <name_of_your_cluster> --num-nodes=0

	When you are ready to start again, increase the number of nodes:

	gcloud container clusters resize --zone <name_of_zone> <name_of_your_cluster> --num-nodes=3


Step 06 - Quick Look at Kubernetes Concepts - Pods, Replica Sets and Deployment
	
	Getting the application up in kubernetes is easy part, understanding whats happening in the background is messy.
	
	kubectl get events => we can see lot of events has happened. 
	
		aniketrajput90@cloudshell:~$ kubectl get events
		LAST SEEN   TYPE     REASON                 OBJECT                                       MESSAGE
		31m         Normal   Scheduled              pod/hello-world-rest-api-67558677bc-jdxk4    Successfully assigned default/hello-world-rest-api-67558677bc-jdxk4 to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		31m         Normal   Pulling                pod/hello-world-rest-api-67558677bc-jdxk4    Pulling image "in28min/hello-world-rest-api:0.0.1.RELEASE"
		30m         Normal   Pulled                 pod/hello-world-rest-api-67558677bc-jdxk4    Successfully pulled image "in28min/hello-world-rest-api:0.0.1.RELEASE"
		30m         Normal   Created                pod/hello-world-rest-api-67558677bc-jdxk4    Created container hello-world-rest-api
		30m         Normal   Started                pod/hello-world-rest-api-67558677bc-jdxk4    Started container hello-world-rest-api
		31m         Normal   SuccessfulCreate       replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-jdxk4
		31m         Normal   ScalingReplicaSet      deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-67558677bc to 1
		28m         Normal   EnsuringLoadBalancer   service/hello-world-rest-api                 Ensuring load balancer
		27m         Normal   EnsuredLoadBalancer    service/hello-world-rest-api                 Ensured load balancer
		
	We can see kubernetes in background has created a pod, a replicaset, deployment, service, etc. 
	
	kubectl get pods => will show all the pods that were created

		aniketrajput90@cloudshell:~$ kubectl get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		hello-world-rest-api-67558677bc-jdxk4   1/1     Running   0          33m
	
	kubectl get	is a generic command, we can get pod, deployment, replicaset, service, etc. 
	
	kubectl get replicaset => give all the replicaset
		
		aniketrajput90@cloudshell:~$ kubectl get replicaset
		NAME                              DESIRED   CURRENT   READY   AGE
		hello-world-rest-api-67558677bc   1         1         1       34m
	
	kubectl get deployment => 
	
		aniketrajput90@cloudshell:~$ kubectl get deployment
		NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
		hello-world-rest-api   1/1     1            1           36m

	kubectl get service =>
	
		aniketrajput90@cloudshell:~$ kubectl get service
		NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)          AGE
		hello-world-rest-api   LoadBalancer   10.0.14.159   35.239.36.17   8080:31771/TCP   34m
		kubernetes             ClusterIP      10.0.0.1      <none>         443/TCP          3h31m	

	
	We can use either singular or plural to get all resources => kubectl get pod or kubectl get pods
	
	When we executed a 'kubectl create deployment' kubernetes created a deployment, replicaset and pod
	'kubectl expose deployment' created service
	
	Each of these components has a prominent role to play in kubernetes being very sucessful. 
	Each of these concept have one important responsibility that they do really well.
	And as a combination they provide an important responsibility of kubernetes which are to manage workload, to provide external access to workload, to enable scaling and to enable zero downtime deployment.  

	In this step we understood the fact that there are enough important concept involved in how kubernetes achieves its goal.  


Commands Executed During the Course

	If you have any problems executing the commands executed in the course, you can refer back to this master list.

	The complete command list is also present on the github repository.

	https://github.com/in28minutes/kubernetes-crash-course#commands-executed-during-the-course


		docker run -p 8080:8080 in28min/hello-world-rest-api:0.0.1.RELEASE
		 
		kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE
		kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
		kubectl scale deployment hello-world-rest-api --replicas=3
		kubectl delete pod hello-world-rest-api-58ff5dd898-62l9d
		kubectl autoscale deployment hello-world-rest-api --max=10 --cpu-percent=70
		kubectl edit deployment hello-world-rest-api #minReadySeconds: 15
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
		 
		gcloud container clusters get-credentials in28minutes-cluster --zone us-central1-a --project solid-course-258105
		kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE
		kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=DUMMY_IMAGE:TEST
		kubectl get events --sort-by=.metadata.creationTimestamp
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
		kubectl get events --sort-by=.metadata.creationTimestamp
		kubectl get componentstatuses
		kubectl get pods --all-namespaces
		 
		kubectl get events
		kubectl get pods
		kubectl get replicaset
		kubectl get deployment
		kubectl get service
		 
		kubectl get pods -o wide
		 
		kubectl explain pods
		kubectl get pods -o wide
		 
		kubectl describe pod hello-world-rest-api-58ff5dd898-9trh2
		 
		kubectl get replicasets
		kubectl get replicaset
		 
		kubectl scale deployment hello-world-rest-api --replicas=3
		kubectl get pods
		kubectl get replicaset
		kubectl get events
		kubectl get events --sort.by=.metadata.creationTimestamp
		 
		kubectl get rs
		kubectl get rs -o wide
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=DUMMY_IMAGE:TEST
		kubectl get rs -o wide
		kubectl get pods
		kubectl describe pod hello-world-rest-api-85995ddd5c-msjsm
		kubectl get events --sort-by=.metadata.creationTimestamp
		 
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
		kubectl get events --sort-by=.metadata.creationTimestamp
		kubectl get pods -o wide
		kubectl delete pod hello-world-rest-api-67c79fd44f-n6c7l
		kubectl get pods -o wide
		kubectl delete pod hello-world-rest-api-67c79fd44f-8bhdt
		 
		kubectl get componentstatuses
		kubectl get pods --all-namespaces
		 
		gcloud auth login
		kubectl version
		gcloud container clusters get-credentials in28minutes-cluster --zone us-central1-a --project solid-course-258105
		 
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.4-SNAPSHOT
		 
		kubectl rollout history deployment hello-world-rest-api
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.4-SNAPSHOT --record
		 
		kubectl rollout history deployment hello-world-rest-api
		kubectl rollout status deployment hello-world-rest-api
		kubectl rollout undo deployment hello-world-rest-api --to-revision=3
		kubectl rollout status deployment hello-world-rest-api
		kubectl rollout undo deployment hello-world-rest-api --to-revision=3
		kubectl rollout status deployment hello-world-rest-api
		kubectl rollout history deployment hello-world-rest-api
		kubectl get pods
		kubectl logs hello-world-rest-api-67c79fd44f-d6q9z
		kubectl logs hello-world-rest-api-67c79fd44f-d6q9z -f
		 
		kubectl get deployment hello-world-rest-api
		kubectl get deployment hello-world-rest-api -o wide
		kubectl get deployment hello-world-rest-api -o yaml
		kubectl get deployment hello-world-rest-api -o yaml > deployment.yaml
		kubectl get service hello-world-rest-api -o yaml
		kubectl get service hello-world-rest-api -o yaml > service.yaml
		 
		kubectl delete all -l app=hello-world-rest-api
		kubectl get all
		kubectl apply -f deployment.yaml
		kubectl get all
		 
		kubectl diff -f deployment.yaml 
		kubectl apply -f deployment.yaml 
		kubectl delete all -l app=hello-world-rest-api
		kubectl get all -o wide
		 
		mvn clean install
		docker push in28min/todo-web-application-h2:0.0.1-SNAPSHOT
		kubectl delete all -l app=hello-world-rest-api
		 
		kubectl get pods
		kubectl get pods --all-namespaces
		kubectl get pods -l app=todo-web-application-h2
		kubectl get pods -l app=todo-web-application-h2 --all-namespaces
		kubectl get services --all-namespaces
		kubectl get services --all-namespaces --sort-by=.spec.type
		kubectl get services --all-namespaces --sort-by=.metadata.name
		kubectl cluster-info
		kubectl top node
		kubectl top pod
		 
		kubectl get services
		kubectl get svc
		kubectl get ev
		kubectl get rs
		 
		kubectl get ns
		kubectl get nodes
		kubectl get no
		kubectl get po
		 
		docker run --detach --env MYSQL_ROOT_PASSWORD=dummypassword --env MYSQL_USER=todos-user --env MYSQL_PASSWORD=dummytodos --env MYSQL_DATABASE=todos --name mysql --publish 3306:3306 mysql:5.7
		docker run -p 8080:8080 in28min/todo-web-application-mysql:0.0.1-SNAPSHOT
		docker run -p 8080:8080 --link=mysql --env RDS_HOSTNAME=mysql in28min/todo-web-application-mysql:0.0.1-SNAPSHOT
		 
		docker-compose --version
		docker-compose up
		 
		brew install kompose
		kompose convert
		 
		kubectl delete all -l app=todo-web-application-h2
		 
		kubectl apply -f mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml
		kubectl get svc
		kubectl apply -f todo-web-application-deployment.yaml,todo-web-application-service.yaml
		docker push in28min/todo-web-application-mysql:0.0.1-SNAPSHOT
		kubectl logs todo-web-application-b65cc44d9-7h9pr -f
		 
		kubectl apply -f mysql-service.yaml
		kubectl get pv
		kubectl get pvc
		kubectl describe pod/mysql-5ccbbbdcd8-5zjqg 
		 
		kubectl create ConfigMaps todo-web-application-config --from-literal=RDS_DB_NAME=todos
		kubectl get ConfigMaps todo-web-application-config
		kubectl describe ConfigMaps/todo-web-application-config
		 
		kubectl edit ConfigMaps/todo-web-application-config
		kubectl scale deployment todo-web-application --replicas=0
		kubectl scale deployment todo-web-application --replicas=1
		 
		kubectl edit ConfigMaps/todo-web-application-config
		kubectl apply -f todo-web-application-deployment.yaml 
		kubectl edit ConfigMaps todo-web-application-config
		kubectl scale deployment todo-web-application --replicas=0
		kubectl scale deployment todo-web-application --replicas=1
		 
		kubectl create secret generic todo-web-application-secrets --from-literal=RDS_PASSWORD=dummytodos
		kubectl get secret/todo-web-application-secrets
		kubectl describe secret/todo-web-application-secrets
		kubectl apply -f todo-web-application-deployment.yaml 
		 
		kubectl delete -f mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml,todo-web-application-deployment.yaml,todo-web-application-service.yaml
		 
		apiVersion: v1
		data:
		  RDS_DB_NAME: todos
		  RDS_HOSTNAME: mysql
		  RDS_PORT: "3306"
		  RDS_USERNAME: todos-user
		kind: ConfigMaps
		metadata:
		  name: todo-web-application-config
		  namespace: default
		 
		cd /in28Minutes/git/kubernetes-crash-course/04-currency-exchange-microservice-basic 
		mvn clean install
		docker push in28min/currency-exchange:0.0.1-RELEASE
		kubectl apply -f deployment.yaml
		curl 34.67.103.178:8000/currency-exchange/from/USD/to/INR
		 
		kubectl create ConfigMaps currency-conversion --from-literal=YOUR_PROPERTY=value --from-literal=YOUR_PROPERTY_2=value2
		 
		kubectl autoscale deployment currency-exchange --min=1 --max=3 --cpu-percent=10 
		kubectl get events
		kubectl get hpa
		kubectl get hpa -o yaml
		kubectl get hpa -o yaml > 01-hpa.yaml
		kubectl get hpa currency-exchange -o yaml > 01-hpa.yaml
		 
		kubectl set image deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.4-SNAPSHOT
		kubectl apply -f ingress.yaml
		kubectl get ingress
		kubectl describe gateway-ingress
		kubectl describe gateway gateway-ingress
		kubectl describe ingress gateway-ingress
		kubectl apply -f rbac.yml
		 
		docker push in28min/currency-conversion:0.0.5-RELEASE
		 
		kubectl create ConfigMaps currency-conversion --from-literal=YOUR_PROPERTY=value --from-literal=YOUR_PROPERTY_2=value2
		 
		kubectl describe ConfigMaps/currency-conversion
		 
		 
		kubectl label namespace default istio-injection=enabled
		 
		kubectl get svc --namespace istio-system
		kubectl apply -f 01-helloworld-deployment.yaml 
		kubectl apply -f 02-creating-http-gateway.yaml 
		kubectl apply -f 03-creating-virtualservice-external.yaml 
		kubectl get svc --namespace istio-system
		kubectl get svc istio-ingressgateway --namespace istio-system
		kubectl scale deployment hello-world-rest-api --replicas=4
		kubectl delete all -l app=hello-world-rest-api
		kubectl apply -f 04-helloworld-multiple-deployments.yaml 
		kubectl apply -f 05-helloworld-mirroring.yaml 
		kubectl apply -f 06-helloworld-canary.yaml 
		watch -n 0.1 curl 35.223.25.220/hello-world
		 
		gcloud container clusters get-credentials in28minutes-cluster-istio --zone us-central1-a --project solid-course-258105
		kubectl create namespace istio-system
		curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.2 sh -
		ls istio-1.2.2
		ls istio-1.2.2/install/kubernetes/helm/istio-init/files/crd*yaml
		cd istio-1.2.2
		for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
		helm template install/kubernetes/helm/istio --name istio --set global.mtls.enabled=false --set tracing.enabled=true --set kiali.enabled=true --set grafana.enabled=true --namespace istio-system > istio.yaml
		kubectl apply -f istio.yaml
		kubectl get pods --namespace istio-system
		kubectl get services --namespace istio-system
		 
		 
		docker push in28min/currency-exchange:3.0.0-RELEASE
		kubectl apply -f deployment.yaml 
		kubectl apply -f 11-istio-scripts-and-configuration/07-hw-virtualservice-all-services.yaml 
		kubectl get secret -n istio-system kiali
		kubectl create secret generic kiali -n istio-system --from-literal=username=admin --from-literal=passphrase=admin
		kubectl get svc --namespace istio-system
		 
		 
		gcloud container clusters get-credentials helm-cluster --zone us-central1-a --project solid-course-258105
		helm init
		kubectl get deploy,svc tiller-deploy -n kube-system
		clear
		unzip 12-helm.zip
		ls helm-tiller.sh
		chmod +x helm-tiller.sh
		 
		gcloud container clusters get-credentials helm-cluster --zone us-central1-a --project solid-course-258105
		./helm-tiller.sh
		cat helm-tiller.sh 
		kubectl get deploy,svc tiller-deploy -n kube-system
		helm install ./currency-exchange/ --name=currency-services
		helm install ./currency-conversion/ --name=currency-services-1
		helm install ./currency-conversion/ --name=currency-services-3 --debug --dry-run
		helm history currency-services-1
		helm upgrade currency-services-1 ./currency-conversion/
		helm rollback currency-services-1 1
		helm upgrade currency-services-1 ./currency-conversion/ --debug --dry-run
		helm upgrade currency-services-1 ./currency-conversion/
		helm history currency-services-1


Step 07 - Understanding Pods in Kubernetes

	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\Pods-Kubernetes.jpg	

	A pod is the smallest deployable unit in kubernetes. We might think that containers are the smallest deployable unit, but no pods are actually the smallest deployable unit. 

	Lets consider a question, I want to create a container in kubernetes. Can we do it without a pod? The answer is no.
	
	We cannot have a container in kubernetes without a pod. 
	
	Our continaer lives inside a pod.
	
	Now what does a pod have and why do we need a pod? 
	
	kubectl get pods -o wide =>	
	
		aniketrajput90@cloudshell:~$ kubectl get pods -o wide
		NAME                                    READY   STATUS    RESTARTS   AGE   IP         NODE                                                 NOMINATED NODE   READINESS GATES
		hello-world-rest-api-67558677bc-jdxk4   1/1     Running   0          73m   10.4.2.4   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
	
	Here we can see an ip address of an pod. Each pod has a unique ip address. 
	
	Another thing we see READY - 1/1, it is the number of containers present inside hte pod and how many of them are ready.
	
	Pod can container multiple containers. 
	All the containers that are present in a pod, share resources. 
	Within the same pod, the contianers can talk to each other using localhost. 
	
	kubectl explain pod =>
		
		aniketrajput90@cloudshell:~$ kubectl explain pods
		KIND:     Pod
		VERSION:  v1
		DESCRIPTION:
			 Pod is a collection of containers that can run on a host (Single Node). This resource is
			 created by clients and scheduled onto hosts.
		FIELDS:
		   apiVersion   <string>
			 APIVersion defines the versioned schema of this representation of an
			 object. Servers should convert recognized schemas to the latest internal
			 value, and may reject unrecognized values. More info:
			 https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
		   kind <string>
			 Kind is a string value representing the REST resource this object
			 represents. Servers may infer this from the endpoint the client submits
			 requests to. Cannot be updated. In CamelCase. More info:
			 https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
		   metadata     <Object>
			 Standard object's metadata. More info:
			 https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
		   spec <Object>
			 Specification of the desired behavior of the pod. More info:
			 https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status
		   status       <Object>
			 Most recently observed status of the pod. This data may not be up to date.
			 Populated by the system. Read-only. More info:
			 https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status
	
	In cloud shell - 
		Scroll => Ctrl-Shift-PageUp / Ctrl-Shift-PageDown
	
	
	Pod is a collection of containers that can run on a host (Single Node).
	
	A kubernetes node can container multiple pods and each of these pods can multiple container. These pods can be of different application or these pods can be of the same application.
	
	kubectl describe pod {name of pod} =>
	
		aniketrajput90@cloudshell:~$ kubectl get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		hello-world-rest-api-67558677bc-jdxk4   1/1     Running   0          85m
		
		aniketrajput90@cloudshell:~$ kubectl describe pod hello-world-rest-api-67558677bc-jdxk4
		Name:               hello-world-rest-api-67558677bc-jdxk4
		Namespace:          default
		Priority:           0
		PriorityClassName:  <none>
		Node:               gke-in28minutes-cluster-default-pool-c8956bc6-7s5g/10.128.0.3
		Start Time:         Mon, 16 Mar 2020 14:51:31 +0530
		Labels:             app=hello-world-rest-api
							pod-template-hash=67558677bc
		Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container hello-world-rest-api
		Status:             Running
		IP:                 10.4.2.4
		Controlled By:      ReplicaSet/hello-world-rest-api-67558677bc
		Containers:
		  hello-world-rest-api:
			Container ID:   docker://b42811acd11dc3c3f658329158fa4b8e999cf96b7a45d5ca6d229ff248a8a4d4
			Image:          in28min/hello-world-rest-api:0.0.1.RELEASE
			Image ID:       docker-pullable://in28min/hello-world-rest-api@sha256:00469c343814aabe56ad1034427f546d43bafaaa11208a1eb0720993743f72be
			Port:           <none>
			Host Port:      <none>
			State:          Running
			  Started:      Mon, 16 Mar 2020 14:51:38 +0530
			Ready:          True
			Restart Count:  0
			Requests:
			  cpu:        100m
			Environment:  <none>
			Mounts:
			  /var/run/secrets/kubernetes.io/serviceaccount from default-token-2l729 (ro)
		Conditions:
		  Type              Status
		  Initialized       True
		  Ready             True
		  ContainersReady   True
		  PodScheduled      True
		Volumes:
		  default-token-2l729:
			Type:        Secret (a volume populated by a Secret)
			SecretName:  default-token-2l729
			Optional:    false
		QoS Class:       Burstable
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
						 node.kubernetes.io/unreachable:NoExecute for 300s
		Events:          <none>

	
	We can also see that the pod is running in a 'default' namespace. Namespace provides isolations for part of the cluster from other part of the cluster. 
	Means, let say we have a Development and a QA enviornment running inside a same cluster, how do separate the resources of Development from the resources of QA? One of the options is to create a separate namespaces for QA and Development and associate each of the resources with that specific namespace. For now we have a default namespace. 
	
	We can also see that pod have labels, we are attaching a label called => Labels: app=hello-world-rest-api with this particular pod.
	Earlierr we talked about number of concepts like- pod, replicaset, deployment, services, etc. In kubernetes the way we link all this together is by something called selectors and labels. 
	So labels are very important when we are tie up a pod with replicaset, service, etc. 
	
	Other things that are associated with pod are annotations. Annotations are meta information about that specific pod. What was the release id, build id, authour name, etc. 
	
	Pod provides a way to put our containers together. It gives them an ip address. And also it provides a categarization for all these containers by associating them with labels. 
	
	
Step 08 - Understanding ReplicaSets in Kubernetes		

	aniketrajput90@cloudshell:~$ kubectl get replicaset
	NAME                              DESIRED   CURRENT   READY   AGE
	hello-world-rest-api-67558677bc   1         1         1       142m

	We can also say kubectl get rs

	Why do we need replicaset? What is the role that they play?
	
	Replicaset ensures that a specific number of pods are running at all time. 
	In above logs we can see that the desired number of pods we want running is 1. And currently there is one container in ready status. 
	
	aniketrajput90@cloudshell:~$ kubectl get pods -o wide
	NAME                                    READY   STATUS    RESTARTS   AGE    IP         NODE                                                 NOMINATED NODE   READINESS GATES
	hello-world-rest-api-67558677bc-jdxk4   1/1     Running   0          145m   10.4.2.4   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
	
	kubectl get pods -o wide => displays little more information than, kubectl get pods
	
	Kill/delete the pod => kubectl delete pod {pod name}
	
		aniketrajput90@cloudshell:~$ kubectl delete pod  hello-world-rest-api-67558677bc-jdxk4
		pod "hello-world-rest-api-67558677bc-jdxk4" deleted

	Now if we do => kubectl get pods -o wide, again
	
		aniketrajput90@cloudshell:~$ kubectl get pods -o wide
		NAME                                    READY   STATUS    RESTARTS   AGE   IP         NODE                                                 NOMINATED NODE   READINESS GATES
		hello-world-rest-api-67558677bc-6wp8w   1/1     Running   0          61s   10.4.2.5   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>

	We would see that our Earlierr pod (hello-world-rest-api-67558677bc-jdxk4) no longer exists, however a new pod(hello-world-rest-api-67558677bc-6wp8w) has started. It automatically launched up and is running. And if we give it a little bit of time and if we refresh our url it will be still running. 
	So even though we killed a pod, within a minute our url still continues to work.
	This is happening because of replicaset.
	
	The replicaset keeps monitoring the pods and if there are lesser number of pods than what is needed then it creates the pods. 
	
	As we said we would want one pod running at all time, and when it goes down to zero, immediately the replicaset looks at it and says, okay one pod is missing and it will create a new pod and start it up.
	
	Now lets say in a replicaset I would want to have three pods. So how can we tell replicaset to maintain a higher number of pods. The way we can do that in kubernetes is  =>
		
		kubectl scale deployment hello-world-rest-api --replicas=3
		
		aniketrajput90@cloudshell:~$ kubectl scale deployment hello-world-rest-api --replicas=3
		deployment.extensions/hello-world-rest-api scaled
		
		aniketrajput90@cloudshell:~$ kubectl get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		hello-world-rest-api-67558677bc-6wp8w   1/1     Running   0          10m
		hello-world-rest-api-67558677bc-n2wvb   1/1     Running   0          17s
		hello-world-rest-api-67558677bc-qwx8s   1/1     Running   0          17s

	We can see that the older pod still exists and two new pods were launched up. 
	
	So what we have now is three instances of application running and they also have load distribution between them.
	
	aniketrajput90@cloudshell:~$ kubectl get replicaset
	NAME                              DESIRED   CURRENT   READY   AGE
	hello-world-rest-api-67558677bc   3         3         3       166m

	Now we can see that desired is 3.
	
	Now we want to see what happened in the background, how did all this happened => 
	
		aniketrajput90@cloudshell:~$ kubectl get events
		LAST SEEN   TYPE     REASON              OBJECT                                       MESSAGE
		19m         Normal   Scheduled           pod/hello-world-rest-api-67558677bc-6wp8w    Successfully assigned default/hello-world-rest-api-67558677bc-6wp8w to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		19m         Normal   Pulled              pod/hello-world-rest-api-67558677bc-6wp8w    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		19m         Normal   Created             pod/hello-world-rest-api-67558677bc-6wp8w    Created container hello-world-rest-api
		19m         Normal   Started             pod/hello-world-rest-api-67558677bc-6wp8w    Started container hello-world-rest-api
		19m         Normal   Killing             pod/hello-world-rest-api-67558677bc-jdxk4    Stopping container hello-world-rest-api
		9m20s       Normal   Scheduled           pod/hello-world-rest-api-67558677bc-n2wvb    Successfully assigned default/hello-world-rest-api-67558677bc-n2wvb to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		9m19s       Normal   Pulled              pod/hello-world-rest-api-67558677bc-n2wvb    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		9m18s       Normal   Created             pod/hello-world-rest-api-67558677bc-n2wvb    Created container hello-world-rest-api
		9m18s       Normal   Started             pod/hello-world-rest-api-67558677bc-n2wvb    Started container hello-world-rest-api
		9m20s       Normal   Scheduled           pod/hello-world-rest-api-67558677bc-qwx8s    Successfully assigned default/hello-world-rest-api-67558677bc-qwx8s to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		9m19s       Normal   Pulled              pod/hello-world-rest-api-67558677bc-qwx8s    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		9m19s       Normal   Created             pod/hello-world-rest-api-67558677bc-qwx8s    Created container hello-world-rest-api
		9m18s       Normal   Started             pod/hello-world-rest-api-67558677bc-qwx8s    Started container hello-world-rest-api
		19m         Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-6wp8w
		9m20s       Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-qwx8s
		9m20s       Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-n2wvb
		9m20s       Normal   ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-67558677bc to 3


	We can see that above logs are not in sorted order by time. We can get them in some what sorted by =>
		
		kubectl get events --sort-by=.metadata.creationTimestamp			//.meta.creationTimestamp is path to creationTimestamp

		aniketrajput90@cloudshell:~$ kubectl get events --sort-by=.metadata.creationTimestamp
		LAST SEEN   TYPE     REASON              OBJECT                                       MESSAGE
		23m         Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-6wp8w
		23m         Normal   Scheduled           pod/hello-world-rest-api-67558677bc-6wp8w    Successfully assigned default/hello-world-rest-api-67558677bc-6wp8w to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		23m         Normal   Killing             pod/hello-world-rest-api-67558677bc-jdxk4    Stopping container hello-world-rest-api
		23m         Normal   Pulled              pod/hello-world-rest-api-67558677bc-6wp8w    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		23m         Normal   Started             pod/hello-world-rest-api-67558677bc-6wp8w    Started container hello-world-rest-api
		23m         Normal   Created             pod/hello-world-rest-api-67558677bc-6wp8w    Created container hello-world-rest-api
		13m         Normal   Scheduled           pod/hello-world-rest-api-67558677bc-n2wvb    Successfully assigned default/hello-world-rest-api-67558677bc-n2wvb to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		13m         Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-n2wvb
		13m         Normal   SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-qwx8s
		13m         Normal   Scheduled           pod/hello-world-rest-api-67558677bc-qwx8s    Successfully assigned default/hello-world-rest-api-67558677bc-qwx8s to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
		13m         Normal   ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-67558677bc to 3
		13m         Normal   Created             pod/hello-world-rest-api-67558677bc-qwx8s    Created container hello-world-rest-api
		13m         Normal   Pulled              pod/hello-world-rest-api-67558677bc-qwx8s    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		13m         Normal   Pulled              pod/hello-world-rest-api-67558677bc-n2wvb    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
		13m         Normal   Started             pod/hello-world-rest-api-67558677bc-qwx8s    Started container hello-world-rest-api
		13m         Normal   Created             pod/hello-world-rest-api-67558677bc-n2wvb    Created container hello-world-rest-api
		13m         Normal   Started             pod/hello-world-rest-api-67558677bc-n2wvb    Started container hello-world-rest-api

	replicaset starts creating new pods after the scale command.
	
	aniketrajput90@cloudshell:~$ kubectl explain replicaset
	KIND:     ReplicaSet
	VERSION:  extensions/v1beta1

	DESCRIPTION:
		 DEPRECATED - This group version of ReplicaSet is deprecated by
		 apps/v1beta2/ReplicaSet. See the release notes for more information.
		 ReplicaSet ensures that a specified number of pod replicas are running at
		 any given time.

	FIELDS:
	   apiVersion   <string>
		 APIVersion defines the versioned schema of this representation of an
		 object. Servers should convert recognized schemas to the latest internal
		 value, and may reject unrecognized values. More info:
		 https://git.k8s.io/community/contributors/devel/api-conventions.md#resources

	   kind <string>
		 Kind is a string value representing the REST resource this object
		 represents. Servers may infer this from the endpoint the client submits
		 requests to. Cannot be updated. In CamelCase. More info:
		 https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds

	   metadata     <Object>
		 If the Labels of a ReplicaSet are empty, they are defaulted to be the same
		 as the Pod(s) that the ReplicaSet manages. Standard object's metadata. More
		 info:
		 https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

	   spec <Object>
		 Spec defines the specification of the desired behavior of the ReplicaSet.
		 More info:
		 https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

	   status       <Object>
		 Status is the most recently observed status of the ReplicaSet. This data
		 may be out of date by some window of time. Populated by the system.
		 Read-only. More info:
		 https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status


	So when we scale a deployment, what happens is, the deployment updates the replicaset and says ok I want three instances. Replicaset will then check the current number of pods and if number of pods is less than what is needed, it would automatically start scaling up.
	
	So in kubernetes if any talks about replicaset, its all about maintaining the number of pods.
	
	Pod is where our contianer runs. A pod provides a grouping for the container (using namespace). 
	And replicaset ensures that a specified number of pods are always running. 
	

Step 09 - Understanding Deployment in Kubernetes

	Why do we need a deployment?
	
	Lets say we have a v1 version of application deployed. And lets say we would want to update to a new version v2. 
	Typically what we want during updating application is zero downtime. 
	
	Current status => 	
	
		aniketrajput90@cloudshell:~$ kubectl get replicaset
		NAME                              DESIRED   CURRENT   READY   AGE
		hello-world-rest-api-67558677bc   3         3         3       3h8m

	More details =>
		
		aniketrajput90@cloudshell:~$ kubectl get replicaset -o wide
		NAME                              DESIRED   CURRENT   READY   AGE    CONTAINERS             IMAGES                                       SELECTOR
		hello-world-rest-api-67558677bc   3         3         3       3h9m   hello-world-rest-api   in28min/hello-world-rest-api:0.0.1.RELEASE   app=hello-world-rest-api,pod-template-hash=67558677bc

		We can now see details of images that are associated with the replicaset
		So we can see that this replicaset is manage this specific release which is using this specific image. 

	Now we want to deploy a new version of this hello-world-rest-api.
	Do that =>
		
		kubectl set image deployment hello-world-rest-api(deployment name, can be anything) hello-world-rest-api(container name)=DUMMY_IMAGE:TEST(some false image for now)
		
		What we are doing is, inside the hello-world-rest-api deployment, we want to set the image path for the hello-world-rest-api container to a error. Why we are setting it to error? Because we would want to see if this deployment would go down. Typically when error happens at deployment, our application would go down. 
		
		aniketrajput90@cloudshell:~$ kubectl set image deployment hello-world-rest-api hello-world-rest-api=DUMMY_IMAGE:TEST
		deployment.extensions/hello-world-rest-api image updated

		Now the deployment have errors, and if you check by refreshing the url, you can see that it still up and running and you will be getting responses from all the current instances.
		
		So even when we made mistakes in our deployment we see that the application is still up and running and the old version is still running. 
		
	Lets see whats happening in the background =>
	
		aniketrajput90@cloudshell:~$ kubectl get replicaset -o wide
		NAME                              DESIRED   CURRENT   READY   AGE     CONTAINERS             IMAGES                                       SELECTOR
		hello-world-rest-api-67558677bc   3         3         3       3h23m   hello-world-rest-api   in28min/hello-world-rest-api:0.0.1.RELEASE   app=hello-world-rest-api,pod-template-hash=67558677bc
		hello-world-rest-api-6f79646b44   1         1         0       3m34s   hello-world-rest-api   DUMMY_IMAGE:TEST                             app=hello-world-rest-api,pod-template-hash=6f79646b44
	
	We now see that there are two replicasets. One with 0.0.1.RELEASE release with 3 instances running right now and second with TEST release, however we can see its desired is 1 and ready is 0, so none of the containers are in ready state. They are not in ready state because the image part has an error.  
		
	aniketrajput90@cloudshell:~$ kubectl get pods
	NAME                                    READY   STATUS             RESTARTS   AGE
	hello-world-rest-api-67558677bc-6wp8w   1/1     Running            0          59m
	hello-world-rest-api-67558677bc-n2wvb   1/1     Running            0          48m
	hello-world-rest-api-67558677bc-qwx8s   1/1     Running            0          48m
	hello-world-rest-api-6f79646b44-6dss5   0/1     InvalidImageName   0          7m31s						//this new pod with new replicaset has status InvalidImageName.

	hello-world-rest-api-6f79646b44-6dss5 = substring contains replicaset name.

	So kubernetes has discovered that the image name that is associated with this specific pod is not right.

		
	To check more details on pod =>
	
		aniketrajput90@cloudshell:~$ kubectl describe pod hello-world-rest-api-6f79646b44-6dss5
		Name:               hello-world-rest-api-6f79646b44-6dss5
		Namespace:          default
		Priority:           0
		PriorityClassName:  <none>
		Node:               gke-in28minutes-cluster-default-pool-c8956bc6-k34n/10.128.0.2
		Start Time:         Mon, 16 Mar 2020 18:11:46 +0530
		Labels:             app=hello-world-rest-api
							pod-template-hash=6f79646b44
		Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container hello-world-rest-api
		Status:             Pending
		IP:                 10.4.0.5
		Controlled By:      ReplicaSet/hello-world-rest-api-6f79646b44
		Containers:
		  hello-world-rest-api:
			Container ID:
			Image:          DUMMY_IMAGE:TEST
			Image ID:
			Port:           <none>
			Host Port:      <none>
			State:          Waiting
			  Reason:       InvalidImageName
			Ready:          False
			Restart Count:  0
			Requests:
			  cpu:        100m
			Environment:  <none>
			Mounts:
			  /var/run/secrets/kubernetes.io/serviceaccount from default-token-2l729 (ro)
		Conditions:
		  Type              Status
		  Initialized       True
		  Ready             False
		  ContainersReady   False
		  PodScheduled      True
		Volumes:
		  default-token-2l729:
			Type:        Secret (a volume populated by a Secret)
			SecretName:  default-token-2l729
			Optional:    false
		QoS Class:       Burstable
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
						 node.kubernetes.io/unreachable:NoExecute for 300s
		Events:
		  Type     Reason         Age                   From                                                         Message
		  ----     ------         ----                  ----                                                         -------
		  Normal   Scheduled      10m                   default-scheduler                                            Successfully assigned default/hello-world-rest-api-6f79646b44-6dss5 to gke-in28minutes-cluster-default-pool-c8956bc6-k34n
		  Warning  Failed         5m29s (x26 over 10m)  kubelet, gke-in28minutes-cluster-default-pool-c8956bc6-k34n  Error: InvalidImageName
		  Warning  InspectFailed  34s (x49 over 10m)    kubelet, gke-in28minutes-cluster-default-pool-c8956bc6-k34n  Failed to apply default image tag "DUMMY_IMAGE:TEST": couldn't parse image reference "DUMMY_IMAGE:TEST": invalid reference format: repository name must be lowercase


	.....
	
	aniketrajput90@cloudshell:~$ kubectl get events --sort-by=.metadata.creationTimestamp
	LAST SEEN   TYPE      REASON              OBJECT                                       MESSAGE
	54m         Normal    Scheduled           pod/hello-world-rest-api-67558677bc-qwx8s    Successfully assigned default/hello-world-rest-api-67558677bc-qwx8s to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
	54m         Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-67558677bc to 3
	54m         Normal    SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-n2wvb
	54m         Normal    SuccessfulCreate    replicaset/hello-world-rest-api-67558677bc   Created pod: hello-world-rest-api-67558677bc-qwx8s
	54m         Normal    Scheduled           pod/hello-world-rest-api-67558677bc-n2wvb    Successfully assigned default/hello-world-rest-api-67558677bc-n2wvb to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
	54m         Normal    Pulled              pod/hello-world-rest-api-67558677bc-qwx8s    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
	54m         Normal    Created             pod/hello-world-rest-api-67558677bc-qwx8s    Created container hello-world-rest-api
	54m         Normal    Pulled              pod/hello-world-rest-api-67558677bc-n2wvb    Container image "in28min/hello-world-rest-api:0.0.1.RELEASE" already present on machine
	54m         Normal    Started             pod/hello-world-rest-api-67558677bc-qwx8s    Started container hello-world-rest-api
	54m         Normal    Created             pod/hello-world-rest-api-67558677bc-n2wvb    Created container hello-world-rest-api
	54m         Normal    Started             pod/hello-world-rest-api-67558677bc-n2wvb    Started container hello-world-rest-api
	13m         Normal    Scheduled           pod/hello-world-rest-api-6f79646b44-6dss5    Successfully assigned default/hello-world-rest-api-6f79646b44-6dss5 to gke-in28minutes-cluster-default-pool-c8956bc6-k34n
	13m         Normal    SuccessfulCreate    replicaset/hello-world-rest-api-6f79646b44   Created pod: hello-world-rest-api-6f79646b44-6dss5
	13m         Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-6f79646b44 to 1
	3m13s       Warning   InspectFailed       pod/hello-world-rest-api-6f79646b44-6dss5    Failed to apply default image tag "DUMMY_IMAGE:TEST": couldn't parse image reference "DUMMY_IMAGE:TEST": invalid reference format: repository name must be lo
	wercase
	8m8s        Warning   Failed              pod/hello-world-rest-api-6f79646b44-6dss5    Error: InvalidImageName

	We can see that new deployment was created, and deployment has created a new replicaset saying it need 1 instance. Then replicaset created a pod. And then we can see that pod failed to start up.

	Check diagram => C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\kubernetes-Deployments.jpg

	From diagram=>
	
		We had a deployment which had created a Replica Set 1 and Replica Set 1 had created 3 Pod instances which were tied to that replica set 1. And this Replica Set 1 was associted with the v1 version of our application.
		And now when we are trying to do v2 version of application, we said create a new deployment, this deployment created a new Replica Set 2 and what Replica Set 2 was trying to do is lauch up Pods for that replica set and right now that pod instance is in a failed status. 

	So now lets update =>
					
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE

		aniketrajput90@cloudshell:~$ kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
		deployment.extensions/hello-world-rest-api image updated

		
	aniketrajput90@cloudshell:~$ kubectl get pods
	NAME                                    READY   STATUS        RESTARTS   AGE
	hello-world-rest-api-67558677bc-6wp8w   0/1     Terminating   0          80m		//here there were more 2 pods terminating i.e. total 3 of the old replicaset.
	hello-world-rest-api-6cc488c49b-dhrgd   1/1     Running       0          14s
	hello-world-rest-api-6cc488c49b-rwsqm   1/1     Running       0          22s
	hello-world-rest-api-6cc488c49b-z7nj4   1/1     Running       0          29s

	All the pods that are in running are new ones. 
	
	aniketrajput90@cloudshell:~$ kubectl get replicaset
	NAME                              DESIRED   CURRENT   READY   AGE
	hello-world-rest-api-67558677bc   0         0         0       3h50m						//One with older version v1
	hello-world-rest-api-6cc488c49b   3         3         3       108s						//New replicaset with 3 instances v2
	hello-world-rest-api-6f79646b44   0         0         0       30m						//One with error image associated with it. 

	
	Now if we hit - http://35.239.36.17:8080/hello-world
	We can see now everthing is being hit to v2 of application. The v2 of application is now up and running. 

	
	aniketrajput90@cloudshell:~$ kubectl get events --sort-by=.metadata.creationTimestamp
	LAST SEEN   TYPE      REASON              OBJECT                                       MESSAGE
	32m         Normal    SuccessfulCreate    replicaset/hello-world-rest-api-6f79646b44   Created pod: hello-world-rest-api-6f79646b44-6dss5
	32m         Normal    Scheduled           pod/hello-world-rest-api-6f79646b44-6dss5    Successfully assigned default/hello-world-rest-api-6f79646b44-6dss5 to gke-in28minutes-cluster-default-pool-c8956bc6-k34n
	32m         Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-6f79646b44 to 1
	12m         Warning   InspectFailed       pod/hello-world-rest-api-6f79646b44-6dss5    Failed to apply default image tag "DUMMY_IMAGE:TEST": couldn't parse image reference "DUMMY_IMAGE:TEST": invalid reference format: repository name must be lowercase
	7m46s       Warning   Failed              pod/hello-world-rest-api-6f79646b44-6dss5    Error: InvalidImageName
	4m29s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled down replica set hello-world-rest-api-6f79646b44 to 0
	4m29s       Normal    SuccessfulDelete    replicaset/hello-world-rest-api-6f79646b44   Deleted pod: hello-world-rest-api-6f79646b44-6dss5
	4m29s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-6cc488c49b to 1
	4m28s       Normal    SuccessfulCreate    replicaset/hello-world-rest-api-6cc488c49b   Created pod: hello-world-rest-api-6cc488c49b-z7nj4
	4m28s       Normal    Scheduled           pod/hello-world-rest-api-6cc488c49b-z7nj4    Successfully assigned default/hello-world-rest-api-6cc488c49b-z7nj4 to gke-in28minutes-cluster-default-pool-c8956bc6-k34n
	4m28s       Normal    Pulling             pod/hello-world-rest-api-6cc488c49b-z7nj4    Pulling image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m24s       Normal    Pulled              pod/hello-world-rest-api-6cc488c49b-z7nj4    Successfully pulled image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m22s       Normal    Started             pod/hello-world-rest-api-6cc488c49b-z7nj4    Started container hello-world-rest-api
	4m22s       Normal    Created             pod/hello-world-rest-api-6cc488c49b-z7nj4    Created container hello-world-rest-api
	4m21s       Normal    SuccessfulDelete    replicaset/hello-world-rest-api-67558677bc   Deleted pod: hello-world-rest-api-67558677bc-qwx8s
	4m19s       Normal    Killing             pod/hello-world-rest-api-67558677bc-qwx8s    Stopping container hello-world-rest-api
	4m21s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-6cc488c49b to 2
	4m21s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled down replica set hello-world-rest-api-67558677bc to 2
	4m20s       Normal    Pulling             pod/hello-world-rest-api-6cc488c49b-rwsqm    Pulling image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m20s       Normal    Scheduled           pod/hello-world-rest-api-6cc488c49b-rwsqm    Successfully assigned default/hello-world-rest-api-6cc488c49b-rwsqm to gke-in28minutes-cluster-default-pool-c8956bc6-1ldm
	4m20s       Normal    SuccessfulCreate    replicaset/hello-world-rest-api-6cc488c49b   Created pod: hello-world-rest-api-6cc488c49b-rwsqm
	4m16s       Normal    Pulled              pod/hello-world-rest-api-6cc488c49b-rwsqm    Successfully pulled image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m14s       Normal    Created             pod/hello-world-rest-api-6cc488c49b-rwsqm    Created container hello-world-rest-api
	4m14s       Normal    Started             pod/hello-world-rest-api-6cc488c49b-rwsqm    Started container hello-world-rest-api
	4m13s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled down replica set hello-world-rest-api-67558677bc to 1
	4m13s       Normal    SuccessfulCreate    replicaset/hello-world-rest-api-6cc488c49b   Created pod: hello-world-rest-api-6cc488c49b-dhrgd
	4m13s       Normal    Scheduled           pod/hello-world-rest-api-6cc488c49b-dhrgd    Successfully assigned default/hello-world-rest-api-6cc488c49b-dhrgd to gke-in28minutes-cluster-default-pool-c8956bc6-7s5g
	4m13s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled up replica set hello-world-rest-api-6cc488c49b to 3
	4m13s       Normal    SuccessfulDelete    replicaset/hello-world-rest-api-67558677bc   Deleted pod: hello-world-rest-api-67558677bc-n2wvb
	4m13s       Normal    Killing             pod/hello-world-rest-api-67558677bc-n2wvb    Stopping container hello-world-rest-api
	4m12s       Normal    Pulling             pod/hello-world-rest-api-6cc488c49b-dhrgd    Pulling image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m11s       Normal    Pulled              pod/hello-world-rest-api-6cc488c49b-dhrgd    Successfully pulled image "in28min/hello-world-rest-api:0.0.2.RELEASE"
	4m11s       Normal    Created             pod/hello-world-rest-api-6cc488c49b-dhrgd    Created container hello-world-rest-api
	4m10s       Normal    Killing             pod/hello-world-rest-api-67558677bc-6wp8w    Stopping container hello-world-rest-api
	4m10s       Normal    Started             pod/hello-world-rest-api-6cc488c49b-dhrgd    Started container hello-world-rest-api
	4m10s       Normal    SuccessfulDelete    replicaset/hello-world-rest-api-67558677bc   Deleted pod: hello-world-rest-api-67558677bc-6wp8w
	4m10s       Normal    ScalingReplicaSet   deployment/hello-world-rest-api              Scaled down replica set hello-world-rest-api-67558677bc to 0

	Here we can see that new deployment created, which creates a new replica set and says have 1 instance. So the replica set will immediately create a pot. 
	As soon as the pod is created, the deployment says, ok I have one instance of v2 up and running, I don't need 3 instances of v1 any more. So it scales down v1 replicaset. The v1 replicaset is now scale down to 2 instance. 
	And it scales up the v2 replicaset to 2 instances.
	(It scale up and scale down pods one by one - with each different deployment)

	In summary, a deployment is very important to make that you are able to update new release of applications without down time. 
	
	The strategy which deployment is using by default is called Rolling Update. What it does is, lets say I have 5 instances of v1 and I want to update to v2, the Rolling Update strategy, it updates one pod at a time. So it launches a new pod for v2 and once its up and running it brings down one pod of v1. And again it will bring up one pod of v2 and again bring down one pod of v1, and so on and so forth, until number of pods for v1 becomes zero. And all the traffic goes to v2.
	
	There are other deployment strategies which we will talk about later. 
	
	
Step 10 - Quick Review of kubernetes Concepts - Pods, Replica Sets and Deployment

	Check diagram = C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\Review-Pods-ReplicaSet-Deployments.jpg
	
	A Pod is nothing but a wrapper for a set of containers. 
	A pod has a ip address and it has things like labels, annotations, etc. 
	
	A Replica Set ensures that a specific number of pods are always running. 
	So if we say 3 instances, replica set will always make sure that 3 instances are always running. 
	Even if we kill one of the instance, replica set will observe that and it will create a new instance. 
	In practice we had seen that a Replica Set is always tied with a specific release version.
	So will have a replica set v1 and that would be maintaining a specified number of instances for the v1 release. So a replica set v1 is responsible for making sure that that specified number of instances of vesion 1 of the application are always running. 
	
	A deployment ensures that a release upgrade, a switch from v1 to v2 happens without a hitch. 
	You don't really want to have a downtimes when we want to release a new version of applications.
	There are variety of deployment strategies. 
	When I am releaseing a new version of application, I might want to actually send 50% of traffic to v1 and 50% of traffic to v2. 
	Or I would like to do something like the Rolling Update. Where I would first want to create one instance of v2, test it and if it is fine, then I would reduce the number of instances of v1 and so on, until the v1 instances becomes 0. 
	Default deployment strategy is Rolling Update. 
	
	
Step 11 - Understanding Services in Kubernetes
	
	In this step we will understand the need for a service.
	
	kubectl get pods - wide => 

		aniketrajput90@cloudshell:~ (boreal-gravity-271305)$ kubectl get pods -o wide
		NAME                                    READY   STATUS    RESTARTS   AGE   IP         NODE                                                 NOMINATED NODE   READINESS GATES
		hello-world-rest-api-6cc488c49b-dhrgd   1/1     Running   0          40h   10.4.2.8   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
		hello-world-rest-api-6cc488c49b-rwsqm   1/1     Running   0          40h   10.4.1.8   gke-in28minutes-cluster-default-pool-c8956bc6-1ldm   <none>           <none>
		hello-world-rest-api-6cc488c49b-z7nj4   1/1     Running   0          40h   10.4.0.6   gke-in28minutes-cluster-default-pool-c8956bc6-k34n   <none>           <none>

	We can see that each of the pod has its own ip addess. 
	
	And if we keep refreshing the url, we can see that the load is being distributed automatically between these pods. 
	
	Now lets kill one of these pod. 
	
		aniketrajput90@cloudshell:~ (boreal-gravity-271305)$ kubectl delete pod hello-world-rest-api-6cc488c49b-z7nj4
		pod "hello-world-rest-api-6cc488c49b-z7nj4" deleted

	A pod is deleted. And if we do kubectl get pods -o wide, we can see that a new pod has been created - 

		aniketrajput90@cloudshell:~ (boreal-gravity-271305)$ kubectl get pods -o wide
		NAME                                    READY   STATUS    RESTARTS   AGE   IP         NODE                                                 NOMINATED NODE   READINESS GATES
		hello-world-rest-api-6cc488c49b-4b5jg   1/1     Running   0          13s   10.4.2.9   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
		hello-world-rest-api-6cc488c49b-dhrgd   1/1     Running   0          40h   10.4.2.8   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
		hello-world-rest-api-6cc488c49b-rwsqm   1/1     Running   0          40h   10.4.1.8   gke-in28minutes-cluster-default-pool-c8956bc6-1ldm   <none>           <none>

	kubectl delete **** can be used to delete any of kubernetes resources - pod, replica set, deployment, service, etc. 
	
	This new pod has a new id and also new ip address. Whenever a pod start up newly, it gets a new ip address. 
	
	Now think about the consumers, do you want to keep giving them new ip addresses, no we would want to give them one url. And irrespective of the pods in background we would want that url to work. 
	If we check our existing url, it continues to work, at time time when pods are deleted or launched up, etc. Its always up and running. This magic is happening due to Service.
	
	In kubernetes a Pod is a throw away unit, our pods may go down, come up, there might be several new pods and old pod gone away, irrespective of all the changes happening with the pod, we don't want consumer or user to use different url or get affected. That a role of a service. 
	A role of a service is to provide a always available external interface to the applications which are running inside the pods. 
	
	A service basically allows our application to receive traffic through a permanent lifetime ip adress. 
	
	When this/our service was created? It was created when we did an kubectl expose deployment. This is when wan service was created with an ip address and that an ip address, we are using to access the application. 
	
	In the Ui if we go in Service and Ingress, we can see details of service like its type is load balancer, endpoints, etc.
	If we click on it, we can see which deployments are associated with it, which are the pods running right now and serving the request. 
	
	Now how is this specific service implemented in Google Cloud? There is a Google Cloud load balancer created, So if we search for load balancing and go to network services - load balancing, we would see that there is a specific load balancer which was created for this service. 
	If we click on it, we can see the frontend with the same url we are accessing and the backend. So as we delete pod, create pod everything gets updated in backend, however frontend will remain the same. 

	In summary, A service basically allows our application to receive traffic through a permanent lifetime ip adress. 
	A service remains up and running, it provides a constant frontend interface, irrespective of whatever changes are happening to the backend which are all our pods where our applications are running. 
	
	The greeat thing with kubernetes is the fact that it provides excellent integration with the different cloud providers specific load balancers. 
	We were using a specific type of service called Load Balancer and we saw that a Google Cloud platform Load Balancer was created for us. 
	This shows how well kubernetes is integrated with google cloud platform to create load balancers. 
	If we were working with AWS or Azure then that clod provider specific load balancer would have been created for us as a service.

	
	aniketrajput90@cloudshell:~$ kubectl get services
	NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)          AGE
	hello-world-rest-api   LoadBalancer   10.0.14.159   35.239.36.17   8080:31771/TCP   45h
	kubernetes             ClusterIP      10.0.0.1      <none>         443/TCP          2d				

	ClusterIp type of service is a service that can only be access from inside the cluster. You will not be able to access this service from outside the cluster. We can see that there no External-ip. So if we have some services that are directly consumed inside our cluster, we can create them as ClusterIp.
	
	There is one more type of service called NodePort. We will talk about it later. 
	

Step 12 - Quick Review of GKE on Google Cloud Console

	Go to Workloads - we can see that hello-world-rest-api as a Deployment there. 
	In this step we will see what we can do using the UI of Google CLoud Platform. What we can do through the cloud console. 
	
	When we click on Workloads -> hello-world-rest-api Deployment, we can see there is lot of thing we can do, like Edit, Delete, and if we go in Actions we do Autoscale a deployment, Expose a deployment, Rolling update, Scale. 
	
	If we click Scale, we can expect to ask, how many instances we want. 
	
	Using Rolling Update, we can update to a new release. This is similar to set image command we used Earlierr.
	
	We can use Expose to expose a service, we have already exposed it, so if we want to create a new port mapping and expose it then we can create a new service as well. 
	
	If we click Edit, then we can edit yml of it. 
	
	In other tabs we can see more details of deployment, like we made three changes, we can see history, events related, etc. 
	
	The idea is whatever we are doing through kubectl command, we can also do it through cloud console.
	
	We would use command line whereever possible, we don't want to depend on tool which is cloud specific, rather than we will use kubectl which is cloud neutral way of interacting with our kubernetes clusters. 
	
	
Step 13 - Understanding Kubernetes Architecture - Master Node and Nodes

	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\kubernetes-Cluster-Architechture.jpg

	What are the important components of the master node and what are the import components of worker node, let look at that. 
	
	Important components that are running as part of our master node - 
	
	Check Diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\Master-Node-Components.jpg
	
	The most important component that is running as the part of your master node is Distributed Database (etcd).
	
	All the configuration changes that we are making, all the deployments that we are creating. all the scaling operations that we are performing, all the details of those are stored in a distributed database. 
	
	What we are doing when we are executing those commands is setting the desired state for kubernetes. 
	We are telling kubernetes I would want 5 instances of application A, I would want 10 instance of application B, thats what is called the desired state. 
	Desired state is stored in the distributed database etcd. 
	So all the kubernetes resources, deployment, services, configuration, etc are stored in distributed database. 
	The great thing about this db is that it is distributed. 
	Typically we would recommend you to have 3-5 replicas of these database, so that the kubernetes cluster state is not lost. 
	
	Second important component in master node is API Server (kube-apiserver).
	Earlierr we were executing commands from the kubectl, we were making changes from google cloud console, from the interface provided by the google cloud. How does kubectl talks to kubernetes cluster? How does google cloud console talk to the kubernetes cluster? The way they make there changes is through API Server. 
	If I try to make change through kubectl or google cloud console, the change is submitted to this API Server and process from here.
	
	The other two important components are Scheduler(kube-scheduler) and Controller Manager(kube-controller-manager)
	
	The Scheduler is responsible for scheduling the pod onto the nodes. 
	In the kubernetes clusters we will have several nodes. And when we are creating a new pod we need to decide which node the pod has to be scheduled on to. The decision might be based on, how much cpu is available, how much memory is available, are there any port conflicts and lot of such factors. So scheduler considers all those factors and schedules a pod on appropriate node.

	The Controller manager manages the overall health of the cluster. 
	Whenever we are executing the kubectl command we are updating the desired state. The kubectl manager, makes sure that whatever the desired state we have - we would want 10 instances of application A, 2 instances of release 2, etc. All those changes needs to be executed into the cluster. And the controller manager is responsible for that. 
	It makes sure that the actual state of the kubernetes cluster matches the desired state.
	
	The important thing about master node is typically the user applications like our hello-world-rest-api, will not be schedule on the master node. All the user applications will be running typically in pods, inside the Worker Nodes or Node.
	
	
	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\Worker-Node-Components.jpg
	
	So the one of the important components of the node is the application that we would want to run like hello-world-rest-api. They would be running inside pods. 
	On a single Node we might have several pods. 
	
	Other components on the node are - 
	
	Node Agent (kubelet).  The job of kubelet is to make sure that it monitors what happening on the node and it communicate it back to the master node. (master node khabrri)
	So if a pod goes down, node agent will report it to controller manager. 
	
	Other component is a networking component - Networking Component (kube-proxy)
	Earlierr we had created a deployment and we had exposed a deployment as a service. That was possible through the networking component. 
	It helps you in exposing services around our nodes and our pods. 
	
	The other important component of worker node is Container Runtime(CRI - docker, rkt etc)
	We would want to run containers inside our pods and these needs Container Runtime. 
	Most frequently used container runtime is docker. 
	Actually we can use kubernetes with any OCI(Open Container Interface) runtime specs implementations.

	Does the master node run any of the application related containers? Does it run hello-world-rest-api and things like that?
		No, the master node is typically have only the stuff which is realted to waht is needed to control you worker nodes. 
	
	Can you only run docker containers in kubernetes? 
		No, if our container is compatible with OCI, that fine, we can run those containers in kubernetes.
		
	What happens if the master node goes down? Or what happens if a specific service on a master node goes down? Will the applications go down?
		No, the applications can continue to run working, even with the master node down. When we are executing a url to access the application, the master node doesn't get involved at all. The only thing that will get involved are the worker nodes. The nodes which are doing the work. So even if master node goes down, or the API-Server goes down, our application will continue to be working. 
		We wont be able to make any changes to them but the existing application will continue to run.
		
	aniketrajput90@cloudshell:~$ kubectl get componentstatuses
	NAME                 STATUS    MESSAGE              ERROR
	etcd-1               Healthy   {"health": "true"}
	etcd-0               Healthy   {"health": "true"}
	controller-manager   Healthy   ok
	scheduler            Healthy   ok	
	
			
Step 14 - Understand Google Cloud Regions and Zones
	
	When we were creating a cluster, we saw that it was asking for a region or a zone. What were we choosing and why do we need mutiple regions and mutiple zones. 
	
	Do a google search on google cloud regions - https://cloud.google.com/about/locations
	
	The most important reason why we need region is latency. 
	
	Lets say we have most of user base in India, what will happen if I deploy the application in a region in US? There would be some latency in accessing the application and that not good. And this is the reason why, we would want our application in one of the regions that is nearer to our users. 
	
	Second reason is availability. Lets say we have our application deployed in one of the region and there is some natural calamities that happens in that specific region. What happens, our application is not available any more. That's the reason why we would want to distribute our application accross mutiple regions. And this enables us to be able to provide more availability for our users.
	
	The third reason is legal requirements, sometimes a specific country might not want to store data related to its citizens outside that country. For example US might say I don't want to store data related to US customers outside US. And that where we can use region within US to store there data. 
	
	So what the use of zones, even in regions we can see few numbers there. All these things are what are called zones. AWS calls it availability zones, google calls it zones.
	Within the region we have the number of different physically isolated data centers. So within the India region - Mumbai, there are 3 physically isolated data centers which are present within the same region. This allows us to do, even within the region, we can be highly available. If some datacenter gos down and we had deployed our application across mutiple datacenters then there would be more availability. 
	
	
Section 3: GKE - Using Kubernetes and Docker with Spring Boot
Hello World Rest API
	
	
Step 01 - Importing First 3 Spring Boot Projects into Eclipse
	
	
	
Step 02 - Setting up 01 Spring Boot Hello World Rest API in Local

	We will take the project code, then create the docker image then put it to docker repository and then we will play around with it in kubernetes. 
	
	Check diagram - C:\Users\inarajp\Desktop\Notes\Kubernetes-Udemy\hello-world-kubernetes-deployment
	
	We will also understand what a deployment yml file is and how you can create a yml file where we will define our kubernetes resources.
	
	
	@Service
	public class InstanceInformationService {

		private static final String HOST_NAME = "HOSTNAME";

		private static final String DEFAULT_ENV_INSTANCE_GUID = "LOCAL";

		// @Value(${ENVIRONMENT_VARIABLE_NAME:DEFAULT_VALUE})
		@Value("${" + HOST_NAME + ":" + DEFAULT_ENV_INSTANCE_GUID + "}")
		private String hostName;

		public String retrieveInstanceInfo() {
			return hostName.substring(hostName.length()-5);
		}

	}
	
	InstanceInformationService looks for an environment variable with name 'HOSTNAME'. 
	You would see that whenever you run an application inside a docker container or in kubernetes, what those runtime enviornments do is they inject an enviornment variable with the name as 'HOSTNAME'.
	What we would do is, we would want to make use of this HOSTNAME to find out where our service is running. 
	So our service might be running in 5 instances, 10 instances, how do you know which instance is returning the response back? So we have taken the last five character of the HOSTNAME and we are retuning it back. 
	And if HOSTNAME enviornment variable is not present then we are by default returning the value as 'LOCAL'
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>mvn clean install

	Docker image will be create. 
	
	
Step 03 - Build Docker Image and Push to Docker Hub for Hello World Rest API	
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>docker run -p 8080:8080 in28min/hello-world-rest-api:0.0.4-SNAPSHOT
	
	Now we want to push docker image to a container repository. We will use docker hub which is public docker image repository. We would to push our image to docker hub.  
	
	Make change in dockerfile-maven-plugin, put your id there.
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>docker login
	Authenticating with existing credentials...
	Login Succeeded

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>docker push aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT
	The push refers to repository [docker.io/aniketrajput90/hello-world-rest-api]
	b406f20f7755: Pushed
	ceaf9e1ebef5: Layer already exists
	9b9b7f3d56a0: Layer already exists
	f1b5933fe4b5: Layer already exists
	0.0.4-SNAPSHOT: digest: sha256:cd16367c6e87e01a647e03c4cad5540abf3d4770f66b35a30bea56c11d777fd0 size: 1159
	
	
Step 04 - Installing GCloud
	
	In the previous steps we were using cloud shell to deploy our application. 

	Thing is we can deploy the application from the terminal or from command prompt as well.
	And to be able to do that we need to install two tools, one is GCloud which is basically a command line interface to google cloud and other is kubectl which is basically the command line interface for kubernetes. 
	
	So download and install gcloud, follow the steps like login, selecting project, etc. 
	If later in point of time we need to login then use command - gcloud auth login
	
	
Step 05 - Installing Kubectl
	
	Before installing, I would recommend to see if you have kubectl from Docker.
	Execute "kubectl version" to check in command prompt/terminal => it was installed.
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl version
	Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.5", GitCommit:"20c265fef0741dd71a66480e35bd69f18351daea", GitTreeState:"clean", BuildDate:"2019-10-15T19:16:51Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"windows/amd64"}
	Error from server (NotFound): the server could not find the requested resource

	Now go in our cluster - copy the connect url and paste in our cmd. 
	
	C:\Users\inarajp\AppData\Local\Google\Cloud SDK>gcloud container clusters get-credentials in28minutes-cluster --zone us-central1-a --project boreal-gravity-271305
	Fetching cluster endpoint and auth data.
	kubeconfig entry generated for in28minutes-cluster.

	C:\Users\inarajp\AppData\Local\Google\Cloud SDK>kubectl version
	Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.5", GitCommit:"20c265fef0741dd71a66480e35bd69f18351daea", GitTreeState:"clean", BuildDate:"2019-10-15T19:16:51Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"windows/amd64"}
	Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.10-gke.17", GitCommit:"bdceba0734835c6cb1acbd1c447caf17d8613b44", GitTreeState:"clean", BuildDate:"2020-01-17T23:10:13Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platform:"linux/amd64"}

	So we have now set up GCloud and kubectl, so that from our local machine we can connect to our google cloud cluster.
	The intresting about connecting from local is the fact that it gives us a lot more flexibility. We can create files more easily in our local machine and we can actually put them in version control much more easily. 
	We will start playing with lot of configuration files, yml files and from local it would be much easier than using the cloud console or cloud shell.

	
Step 06 - Deploy 01 Spring Boot Hello World Rest API to Kubernetes

	Now lets start the deployment from the local.
	
	kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT
		
		
		hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT => means inside the deployment which container needs to be updated and set to which image. 
	
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT
	deployment.extensions/hello-world-rest-api image updated

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                    READY   STATUS        RESTARTS   AGE
	hello-world-rest-api-6cc488c49b-4b5jg   0/1     Terminating   0          32h
	hello-world-rest-api-6cc488c49b-dhrgd   0/1     Terminating   0          3d
	hello-world-rest-api-6cc488c49b-rwsqm   0/1     Terminating   0          3d
	hello-world-rest-api-cb698c498-b25mv    1/1     Running       0          12s
	hello-world-rest-api-cb698c498-qh428    1/1     Running       0          4s
	hello-world-rest-api-cb698c498-wrk5s    1/1     Running       0          8s

	After giving few more second we can again execute kubectl get pods and we can see tht all 3 containers are now up and running - 
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                   READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-cb698c498-b25mv   1/1     Running   0          101s
	hello-world-rest-api-cb698c498-qh428   1/1     Running   0          93s
	hello-world-rest-api-cb698c498-wrk5s   1/1     Running   0          97s


	Now hit - http://35.239.36.17:8080/hello-world => server url, we can see that application from our local machine is now deployed to server. 
	
	kubectl rollout history deployment hello-world-rest-api => 
		
		hello-world-rest-api is name of deployment
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout history deployment hello-world-rest-api
	deployment.extensions/hello-world-rest-api
	REVISION  CHANGE-CAUSE
	1         <none>
	2         <none>
	3         <none>
	4         <none>

	We can see all the four versions of deployments which are present. The change cause is none because we have not update the chage cause each time. 

	So if we add --record, it will record the command which caused the change =>
	
		kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT --record

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api> kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT --record
	deployment.extensions/hello-world-rest-api image updated

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout history deployment hello-world-rest-api
	deployment.extensions/hello-world-rest-api
	REVISION  CHANGE-CAUSE
	1         <none>
	2         <none>
	3         <none>
	4         kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT --record=true

	
	We can also check the status of the rollout, if we want to check the status of the latest deployment for hello-world-rest-api => 
		kubectl rollout status deployment hello-world-rest-api
		
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout status deployment hello-world-rest-api
	deployment "hello-world-rest-api" successfully rolled out

	We can also undo deployment =>
		
		kubectl rollout undo deployment hello-world-rest-api --to-revision=3

		--to-revision=3 -> specify which revision
		hello-world-rest-api -> name of deployment

		I think undo deployment means it will again start up that deployment. 
		For now if we hit url - http://35.239.36.17:8080/hello-world, the reponse is for v4 ->  Hello World  V4 98s9g
		But now when we undo deployment for revision 3, it was for v2 and after that when we hit url again we will get v2 response ->  Hello World  V2 98s9g
		
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout undo deployment hello-world-rest-api --to-revision=3
	deployment.extensions/hello-world-rest-api rolled back

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout status deployment hello-world-rest-api
	deployment "hello-world-rest-api" successfully rolled out

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl rollout history deployment hello-world-rest-api
	deployment.extensions/hello-world-rest-api
	REVISION  CHANGE-CAUSE
	1         <none>
	2         <none>
	4         kubectl set image deployment hello-world-rest-api hello-world-rest-api=aniketrajput90/hello-world-rest-api:0.0.4-SNAPSHOT --record=true
	5         <none>

	We can see that revision 3 which was v2 is now revision 5.
	
	
	If we have a large deployment, like 100 deployments we can also pause and unpause a deployment => 
		
		kubectl rollout pause deployment hello-world-rest-api 
		
		kubectl rollout unpause deployment hello-world-rest-api 

	Another important command is to see the logs of the application =>
		
		kubectl logs {pod_name}

		C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		hello-world-rest-api-6cc488c49b-98s9g   1/1     Running   0          11m
		hello-world-rest-api-6cc488c49b-j5txh   1/1     Running   0          11m
		hello-world-rest-api-6cc488c49b-nldbw   1/1     Running   0          11m

		C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl logs hello-world-rest-api-6cc488c49b-98s9g
		
		And we can see logs comming from that specific pod. 
		
		if you want to follow the logs then add -f option =>
			
			kubectl logs {pod_name} -f
			
	
	Now that we are able to connect to the kubernetes cluster from our local machine, we would start using the cloud shell in a different way. 
	
	One of the interesting thing on cloud shell is a command called 'watch'.
	Typically, when we want to run it from local then we need to install watch. 
	The importance of watch is that we can continuously execute a specific url again and again =>
		
		watch curl http://35.239.36.17:8080/hello-world
	
	So we want to execute curl command every 2 seconds, that's what we are doing.
	We can see which instance response is coming from. 
	If we are following the logs then we can see log being updated. 


Step 07 - Generate Kubernetes YAML Configuration for Deployment
and Service

	Until now we have been using command to start a deployment, create a service and variety of things with kubectl. 

	However kubernetes supports the yml format for us to define our deployments, services and everything. 
	
	We would see that in real world projects we would actually be using ymls to create our services and deployments. 

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get deployment hello-world-rest-api
	NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
	hello-world-rest-api   3/3     3            3           4d19h

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get deployment hello-world-rest-api -o wide
	NAME                   READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS             IMAGES                                       SELECTOR
	hello-world-rest-api   3/3     3            3           4d19h   hello-world-rest-api   in28min/hello-world-rest-api:0.0.2.RELEASE   app=hello-world-rest-api


	Now to get the yml of our deployment, replace wide with yaml =>
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get deployment hello-world-rest-api -o yaml
	apiVersion: extensions/v1beta1
	kind: Deployment
	metadata:
	  annotations:
		deployment.kubernetes.io/revision: "5"
	  creationTimestamp: "2020-03-16T09:21:31Z"
	  generation: 7
	  labels:
		app: hello-world-rest-api
	  name: hello-world-rest-api
	  namespace: default
	  resourceVersion: "1002010"
	  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/hello-world-rest-api
	  uid: 85e52f37-6767-11ea-bc3a-42010a800253
	spec:
	  progressDeadlineSeconds: 600
	  replicas: 3
	  revisionHistoryLimit: 10
	  selector:
		matchLabels:
		  app: hello-world-rest-api
	  strategy:
		rollingUpdate:
		  maxSurge: 25%
		  maxUnavailable: 25%
		type: RollingUpdate
	  template:
		metadata:
		  creationTimestamp: null
		  labels:
			app: hello-world-rest-api
		spec:
		  containers:
		  - image: in28min/hello-world-rest-api:0.0.2.RELEASE
			imagePullPolicy: IfNotPresent
			name: hello-world-rest-api
			resources: {}
			terminationMessagePath: /dev/termination-log
			terminationMessagePolicy: File
		  dnsPolicy: ClusterFirst
		  restartPolicy: Always
		  schedulerName: default-scheduler
		  securityContext: {}
		  terminationGracePeriodSeconds: 30
	status:
	  availableReplicas: 3
	  conditions:
	  - lastTransitionTime: "2020-03-18T05:30:47Z"
		lastUpdateTime: "2020-03-18T05:30:47Z"
		message: Deployment has minimum availability.
		reason: MinimumReplicasAvailable
		status: "True"
		type: Available
	  - lastTransitionTime: "2020-03-16T13:10:08Z"
		lastUpdateTime: "2020-03-19T14:22:37Z"
		message: ReplicaSet "hello-world-rest-api-6cc488c49b" has successfully progressed.
		reason: NewReplicaSetAvailable
		status: "True"
		type: Progressing
	  observedGeneration: 7
	  readyReplicas: 3
	  replicas: 3
	  updatedReplicas: 3

	Instead of viewing it in cmd we can take it into a file => 
	
		kubectl get deployment hello-world-rest-api -o yaml > deployment.yaml
		
	Earlierr when we started, there were two important commands that we executed, one was create deployment and other one was expose deployment.  

	So using above cmd => kubectl get deployment hello-world-rest-api -o yaml
	we are getting deployment which we created using create deployment. 
	And now we will get the service details which we created using expose deployment. 

	
	kubectl get service hello-world-rest-api -o yaml > service.yaml

		hello-world-rest-api is service name
		
	For now we have created separate files deployment.yaml and service.yaml, later we will create a single combined file for our deployment.
	
	Intresting thing is we can use these files also to make changes. 
	For example, if we change spec -> replicas(no of pods) to 2 in deployment.yaml =>
		
		kubectl apply -f deployment.yaml
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl apply -f deployment.yaml
	Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
	deployment.extensions/hello-world-rest-api configured
	
	Now if we do kubectl get pods we can see that only 2 pods are running =>
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-6cc488c49b-98s9g   1/1     Running   0          38h
	hello-world-rest-api-6cc488c49b-j5txh   1/1     Running   0          38h	
	
	So instead of using the commands, like scaling, we can also modify the yaml and use apply command to make the changes. 


Step 08 - Understand and Improve Kubernetes YAML Configuration

	We will combine the both our deployment.yaml and service.yaml files into one yaml file. In one yaml file we can define multiple kubernetes resources. 
	So all we need to do is just cut paste the service.yaml into deployment.yaml and we can separete them with ---

	So now we have only one deployment.yaml with both deployment and service details. 

	In this step will will clean the content, this is something which we have generated and there are lot of things which are not needed, so we will remove them out. 
	
	Things removed => 
	
	annotations:
		deployment.kubernetes.io/revision: "5"
	creationTimestamp: "2020-03-16T09:21:31Z"
	generation: 7

	resourceVersion: "1002010"
	
	selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/hello-world-rest-api
	uid: 85e52f37-6767-11ea-bc3a-42010a800253

	progressDeadlineSeconds: 600
	
	revisionHistoryLimit: 10
	  
	template -> metadata -> creationTimestamp: null
	
	spec -> container -> resources: {}

	spec -> container -> terminationMessagePath: /dev/termination-log
    spec -> container -> terminationMessagePolicy: File

	spec -> dnsPolicy: ClusterFirst
	
	spec -> schedulerName: default-scheduler
    spec -> securityContext: {}


	status:
	  availableReplicas: 3
	  conditions:
	  - lastTransitionTime: "2020-03-18T05:30:47Z"
		lastUpdateTime: "2020-03-18T05:30:47Z"
		message: Deployment has minimum availability.
		reason: MinimumReplicasAvailable
		status: "True"
		type: Available
	  - lastTransitionTime: "2020-03-16T13:10:08Z"
		lastUpdateTime: "2020-03-19T14:22:37Z"
		message: ReplicaSet "hello-world-rest-api-6cc488c49b" has successfully progressed.
		reason: NewReplicaSetAvailable
		status: "True"
		type: Progressing
	  observedGeneration: 7
	  readyReplicas: 3
	  replicas: 3
	  updatedReplicas: 3
	
	status is something dynamic so we have removed entire status. 
	
	
	Things removed from Service => 
	
	metadata -> creationTimestamp: "2020-03-16T09:23:58Z"
	
	metadata -> resourceVersion: "37752"
	metadata -> selfLink: /api/v1/namespaces/default/services/hello-world-rest-api
	metadata -> uid: ddc9f8f1-6767-11ea-bc3a-42010a800253

	spec:
	  clusterIP: 10.0.14.159
	  externalTrafficPolicy: Cluster

	status:
	  loadBalancer:
		ingress:
		- ip: 35.239.36.17


	Now we will delete the application which is actually there on the server, and once it is deleted we can again deploy using our yaml file.
	
	We can use labels attached to deployment, service, pods to delete them. 
	
		kubectl delete all -l app=hello-world-rest-api
		
			Everything related to label hello-world-rest-api would be deleted
			
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-6cc488c49b-98s9g   1/1     Running   0          39h
	hello-world-rest-api-6cc488c49b-j5txh   1/1     Running   0          39h

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get deployment
	NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
	hello-world-rest-api   2/2     2            2           4d20h

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get service
	NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)          AGE
	hello-world-rest-api   LoadBalancer   10.0.14.159   35.239.36.17   8080:31771/TCP   4d20h
	kubernetes             ClusterIP      10.0.0.1      <none>         443/TCP          4d23h

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl delete all -l app=hello-world-rest-api
	pod "hello-world-rest-api-6cc488c49b-98s9g" deleted
	pod "hello-world-rest-api-6cc488c49b-j5txh" deleted
	service "hello-world-rest-api" deleted
	deployment.apps "hello-world-rest-api" deleted

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                    READY   STATUS        RESTARTS   AGE
	hello-world-rest-api-6cc488c49b-qhx75   1/1     Terminating   0          25s

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get deployment
	No resources found.

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get service
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   4d23h

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	No resources found.

		
	kubectl get all => it is a shortcut command, that will return all the resources that re present. 
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get all
	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   4d23h
	
	
	Now lets create using yaml =>
	
		kubectl apply -f deployment.yaml
			
			make sure you are cd into a right folder.
			
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get all
	
	NAME                                        READY   STATUS    RESTARTS   AGE
	pod/hello-world-rest-api-6cc488c49b-2ftpw   1/1     Running   0          59s
	pod/hello-world-rest-api-6cc488c49b-4ch9q   1/1     Running   0          60s


	NAME                           TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE
	service/hello-world-rest-api   LoadBalancer   10.0.2.50    35.239.36.17   8080:31771/TCP   60s
	service/kubernetes             ClusterIP      10.0.0.1     <none>         443/TCP          4d23h


	NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/hello-world-rest-api   2/2     2            2           62s

	NAME                                              DESIRED   CURRENT   READY   AGE
	replicaset.apps/hello-world-rest-api-6cc488c49b   2         2         2       62s

	
	We can see everything is created. 
	
	Now lets take the expernal ip and check in browser => http://35.239.36.17:8080/hello-world
	
	We can also do a curl command from cloud shell or from cmd if curl is installed on local machine - 
		
		aniketrajput90@cloudshell:~$ curl http://35.239.36.17:8080/hello-world
		
		if we had added a watch to curl it would have kept hitting the url every 2 seconds.
		
	In this step we were able to deploy hello-world-rest-api just using deployment template. 

	In the next section we will everything that is present in yml. 
	
		
Section 4: GKE - Playing with Declarative Configuration for
Kubernetes
	
	
Step 01 - Understanding Kubernetes YAML Configuration - Labels
and Selectors


	Understand high level structure of yaml file. 
	
	Inside spec, we put all the definition of a deployment. 
	One of the important definition which is present inside the spec of a deployment is the definition of our pod. 
	
	spec -> 
		...
		
		template:
			metadata:
			  labels:
				app: hello-world-rest-api
			spec:
			  containers:
			  - image: in28min/hello-world-rest-api:0.0.2.RELEASE
				imagePullPolicy: IfNotPresent
				name: hello-world-rest-api
			  restartPolicy: Always
			  terminationGracePeriodSeconds: 30

	Important thing when you are defining the pod is defining the containers which are actually presnt inside the pod. 
	That's why -image is and array, we can put multiple containers as below - 
	
		template:
			metadata:
			  labels:
				app: hello-world-rest-api
			spec:
			  containers:
			  - image: in28min/hello-world-rest-api:0.0.2.RELEASE
				imagePullPolicy: IfNotPresent
				name: hello-world-rest-api1
			  - image: aniket90/hello-world-rest-api:0.0.4.RELEASE
				imagePullPolicy: IfNotPresent
				name: hello-world-rest-api2
			  - image: in28min/hello-world-rest-api:0.0.5.RELEASE
				imagePullPolicy: IfNotPresent
				name: hello-world-rest-api3
			  restartPolicy: Always
			  terminationGracePeriodSeconds: 30
	
	
	imagePullPolicy means what would happen if image is already present, 'IfNotPresent' will use whatever is already present. If we always want to update image from the docker hub then we say say 'Always'. 
	
	Other thing is 'name', it is the name of he contianer. 
	
	restartPolicy is Always, if the container doesn't start up, we are saying try and restart it again and again. 
	
	terminationGracePeriodSeconds, so when the this container is stopped, this container will be given 30 seconds to clean up all its work. 
	
	Now, how are we matching the pod with the deployment? We are matching pods with deployments using labels. 
	Each of the pod would have the label of app: hello-world-rest-api, so what we are doing is, inside specs of deployment we are saying matchLabels     
		
		spec:
		  replicas: 2
		  selector:
			matchLabels:
			  app: hello-world-rest-api

	So the selector would define how deployment will match against the pods. Which deployment matches against which pod. And for selector we can define multiple labels. Later we will see the example. Now we can see that the pod also have the same label. 

		strategy:
			rollingUpdate:
			  maxSurge: 25%
			  maxUnavailable: 25%
			type: RollingUpdate
	
	Assume replicas = 200
		   maxUnavailable = 20%
		   maxSurge = 20%
		   
	Max pods that can be unavailable during release = 20%(maxUnavailable) * 200 = 40
	
	Max pods used during release = 200 + 20% (maxSurge) * 200 = 240
	
	
	For service also, it is very similar. For service also we have specs where actual service is defined -
	
		spec:
		  ports:
		  - nodePort: 31771
			port: 8080
			protocol: TCP
			targetPort: 8080
		  selector:
			app: hello-world-rest-api
		  sessionAffinity: None
		  type: LoadBalancer
	
	For service important features are what type of service it is. 
	
	sessionAffinity: None, means if we have a web application, if we have sessions for each users, then we would want all the request to go to a single pod. You don't want different requests from same user to go to different pods, or else we would lose this session. And that's where sessionAffinity comes into picture. 
	If we put it as 'None', then there is no sessionAffinity at all. 
	Later we will set it to an appropriate value. 
	
	Other thing is ports, what is the port on which you want to expose the service on.
	
	Last important part of the spec is selector. How do you match a service to a pod? 
	So what we are saying is, take all the pods which has this lable - app: hello-world-rest-api
	So all the pods with that label will be matched with that service. 
	We can see that the service is not tied with the deployment at all. 
	Service is actually directly tied with the pods. 
	
	
Step 02 - Quick Fix to reduce release downtime with minReadySeconds	
	
	Whenever we do a deployment you would see that there is some down time. We will look in this step how to avaid it.
	
	We are updating to 0.0.3.RELEASE image. 
	
	spec:
      containers:
      - image: in28min/hello-world-rest-api:0.0.3.RELEASE  //instead of 0.0.0.RELEASE
        imagePullPolicy: IfNotPresent
        name: hello-world-rest-api
      restartPolicy: Always
      terminationGracePeriodSeconds: 30

	To get difference between existing deployment and new deployment.yaml =>
		
		kubectl diff -f deployment.yaml
		
	
	When I try to execute command kubectl diff -f deployment.yaml, I am getting following error:
	error: executable file not found in %PATH%
		
		After installing DiffUtils for Windows (http://gnuwin32.sourceforge.net/packages/diffutils.htm), it worked.
	
	We can check if the our service is up and running - 
		
		watch -n 0.1 curl 35.239.36.17:8080/hello-world
		
			This will send request every 0.1 second
			
	Now lets do the apply cmd to change our deployment =>
		
		kubectl apply -f deployment.yaml
		
	This will update our application from v2 to v3
	But aas you execute this command we can see that in watch output that few requests failed with connection refused 8080, even though our pods were up and running - 
	
		C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		hello-world-rest-api-7897b99c89-g2n2q   1/1     Running   0          64s
		hello-world-rest-api-7897b99c89-w8jz2   1/1     Running   0          67s
		
	The applications takes some time to startup. 
	So one of the quick fix that we can give is to add a parameter to our specs definition - minReadySeconds.
		
		spec:
		  replicas: 2
		  minReadySeconds: 45
	
	Later we will also add probes.
	
	Above will ensure that the pods gets the first 45 seconds to actually startup, till then traffic will one on v2 only, after 45 seconds traffic will go to v3.
	
	Now to see how this works again we will change from v3 to v2 and execute apply cmd. 
	
	Some of the requests would still fail but 95% of them will work. 
	
	Adding minReadySeconds is a quick fix. The perfect solution will be to add readiness and liveliness probes which we will discuss a little later. 
	
	
Step 03 - Understanding Replica Sets in Depth - Using Kubernetes
YAML Config
	
	Until now we were using deployments to create replica sets. 
	Relica sets can live on there own without a deployment. And we can actually acttach the pods which are created by replica set to the service. 
	
	So were are changing 'kind' value from 'Deployment' to 'ReplicaSet'
	
		apiVersion: extensions/v1beta1
		kind: ReplicaSet
	
	First we will delete the existing deployment and all stuff related to label 'hello-world-rest-api'
	
		kubectl delete all -l app=hello-world-rest-api

	Until now we had been creating a deployment, which is attached to a pods, and were were creating a service along with it. 
	
	(deployment was attached to pods
	 ReplicaSet was attached to pods
	 Service was attached to pods)

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl delete all -l app=hello-world-rest-api
	pod "hello-world-rest-api-7897b99c89-cgk6f" deleted
	pod "hello-world-rest-api-7897b99c89-nrfpj" deleted
	service "hello-world-rest-api" deleted
	deployment.apps "hello-world-rest-api" deleted

	Right now we will create a RelicaSet and its template is exactly same as Deployment.
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl apply -f deployment.yaml
	error: error validating "deployment.yaml": error validating data: ValidationError(ReplicaSet.spec): unknown field "strategy" in io.k8s.api.extensions.v1beta1.ReplicaSetSpec; if you choose to ignore these errors, turn validation off with --validate=false
	
	We can see that the apply command as failed. It says unkonwn field strategy. 
	Thing is deployment is responsible for new releases. Replica Set is only responsible for ensuring that a specific set of pods are running, it doesn't know anything about strategy, how to do releases. 
	
	So we will comment all the stuff related to strategy.
	
		  #strategy:
			#rollingUpdate:
			  #maxSurge: 25%
			  #maxUnavailable: 25%
			#type: RollingUpdate
		 
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl apply -f deployment.yaml
	replicaset.extensions/hello-world-rest-api created
	service/hello-world-rest-api created

	We can see that the Replica Set is created and Service is created. 


	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get all
	NAME                             READY   STATUS    RESTARTS   AGE
	pod/hello-world-rest-api-fdqfx   1/1     Running   0          53s
	pod/hello-world-rest-api-kwbwl   1/1     Running   0          53s

	NAME                           TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE
	service/hello-world-rest-api   LoadBalancer   10.0.4.251   35.239.36.17   8080:31771/TCP   54s
	service/kubernetes             ClusterIP      10.0.0.1     <none>         443/TCP          5d7h

	NAME                                   DESIRED   CURRENT   READY   AGE
	replicaset.apps/hello-world-rest-api   2         2         2       55s
	
	We can see that its not showing deployments as we have not created it. 
	
	Even we are able to access the application - http://35.239.36.17:8080/hello-world
	We can also do curl and check - watch -n 0.1 curl 35.239.36.17:8080/hello-world
	
	So we can see the fact that to be able to create Service we don't need the Deployment at all. 
	All we need is pods, the service is directly attached to pods using the labels of the pods and the replica set is attached to the pod. 
	
	But there is one important feature that deployment brings in i.e. is release. 
	
	Now lets update the release, currently we have 0.0.3.RELEASE, we will update to 0.0.2.RELEASE
	
	After doing => kubectl apply -f deployment.yaml
	we can see that v2 version is still not comming up (using curl command)
	
	If we check the pods, we can see that new pods are also not getting created, only the old ones are there. 
	
	This is because the ReplicaSet does not worries about the versions. Even though we said update to new version, it does not know about it, all it knows is there are 2 pods running. Its duty is that there are 2 pods running, there is no duty for me now. 
	
	If we want to see ReplicaSet in action, we can delete one of the pod =>
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                         READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-fdqfx   1/1     Running   0          7m40s
	hello-world-rest-api-kwbwl   1/1     Running   0          7m40s

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl delete pod hello-world-rest-api-fdqfx
	pod "hello-world-rest-api-fdqfx" deleted

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                         READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-g8d28   1/1     Running   0          7s
	hello-world-rest-api-kwbwl   1/1     Running   0          13m

	We can see that new pod has been created by ReplicaSet. 
	This new pod will be of our new version i.e. v2. 
	So now we have one pod with v3 and another with v2. 
	We can check with curl command, load being share between v2 and v3 instances.
	
	Only when we delete the old pod, we can see that the entire new release deployment would be successfully. 
	With Deployment, this happens automatically.
	
	We can see that ReplicaSet doesn't worry about releases. 
	It only worries about pods. If we give it a configuration, it will keep making sure that those number of pods are running. Even if we update the configuration in between it doesn't worry about anything.
	Only when the pods are killed, it would start creating new pods with new configuration.
	That's why we always use a Deployment, which actually creates a ReplicaSet and the pods. 
	
	
Step 04 - Configure Multiple Kubernetes Deployments with One
Service

	So what we want to do is, at a time we would want to create separate deployments for two releases of hello-world-rest-api and have one service connecting to those deployments. 
	
	The reson why this is possible is because the service is tied to the pods. So what we are putting in Service is labels of pods. 
		
		kind: Service
		metadata:
		  labels:			#labels of pod
			app: hello-world-rest-api
	
	So first we will add a label in deployment to associate it with a specific release. 
	
		kind: Deployment
		metadata:
		  labels:
			app: hello-world-rest-api
			version: v1					#new label
			
	Now lets go to matchLabels, deployment is tied to pod using matchLabels,
	
		spec:
		  replicas: 2
		  minReadySeconds: 45
		  selector:
			matchLabels:
			  app: hello-world-rest-api
			  version: v1					#added newly
			  
	Also give a pod new label,
	
		template:
		metadata:
		  labels:
			app: hello-world-rest-api
			version: v1						#added newly
			
	So what we did is we created a v1 label for pod and we are asking the deployment to tie up with the pod(using matchLabels) only if you have two labels (app: hello-world-rest-api, version: v1) and we are giving this deployment also two labels (using labels)
	
	We will also call this deployment as hello-world-rest-api-v1,
	
		metadata:
		  labels:
			app: hello-world-rest-api
			version: v1
		  name: hello-world-rest-api-v1			#updated

	We will also change the release to 0.0.1.RELEASE
	
	Now we can copy the entire deployment and paste it down, so we will have 2 deployment, update the same things just with v2 and release 0.0.2.RELEASE
	
	The names of these two deployments now are - hello-world-rest-api-v1 and hello-world-rest-api-v2
	
	We will not change anything on the service. The service is going by one label, so its not checking for the version, its just checking the label on the pod. 
		
		kind: Service
		metadata:
		  labels:	
			app: hello-world-rest-api	#labels of pod
	
	First lets delete everyting => 
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl delete all -l app=hello-world-rest-api
	pod "hello-world-rest-api-g8d28" deleted
	pod "hello-world-rest-api-zh5jz" deleted
	service "hello-world-rest-api" deleted
	replicaset.apps "hello-world-rest-api" deleted

	
	Now, lets run apply command => 
		
		C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl apply -f deployment.yaml
		deployment.extensions/hello-world-rest-api-v1 created
		deployment.extensions/hello-world-rest-api-v2 created
		service/hello-world-rest-api created
	
	We can see that two deployments and one service has been created.

	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get pods
	NAME                                       READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-v1-ffb76b89b-g59gl    1/1     Running   0          58s
	hello-world-rest-api-v1-ffb76b89b-wkcpl    1/1     Running   0          58s
	hello-world-rest-api-v2-587d7bbb4d-4k9gk   1/1     Running   0          57s
	hello-world-rest-api-v2-587d7bbb4d-jm4gn   1/1     Running   0          57s
	
	We can observe that four pods now, 2 of v1 and 2 of v2 
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get all
	NAME                                           READY   STATUS    RESTARTS   AGE
	pod/hello-world-rest-api-v1-ffb76b89b-g59gl    1/1     Running   0          2m6s
	pod/hello-world-rest-api-v1-ffb76b89b-wkcpl    1/1     Running   0          2m6s
	pod/hello-world-rest-api-v2-587d7bbb4d-4k9gk   1/1     Running   0          2m5s
	pod/hello-world-rest-api-v2-587d7bbb4d-jm4gn   1/1     Running   0          2m5s


	NAME                           TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE
	service/hello-world-rest-api   LoadBalancer   10.0.7.20    34.68.185.83   8080:31771/TCP   2m5s
	service/kubernetes             ClusterIP      10.0.0.1     <none>         443/TCP          5d8h


	NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/hello-world-rest-api-v1   2/2     2            2           2m9s
	deployment.apps/hello-world-rest-api-v2   2/2     2            2           2m7s

	NAME                                                 DESIRED   CURRENT   READY   AGE
	replicaset.apps/hello-world-rest-api-v1-ffb76b89b    2         2         2       2m11s
	replicaset.apps/hello-world-rest-api-v2-587d7bbb4d   2         2         2       2m9s

	We can see that 2 replica sets are created one associate with each deployment. 
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl get all -o wide
	NAME                                           READY   STATUS    RESTARTS   AGE     IP          NODE                                                 NOMINATED NODE   READINESS GATES
	pod/hello-world-rest-api-v1-ffb76b89b-g59gl    1/1     Running   0          4m44s   10.4.2.30   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
	pod/hello-world-rest-api-v1-ffb76b89b-wkcpl    1/1     Running   0          4m44s   10.4.2.31   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>
	pod/hello-world-rest-api-v2-587d7bbb4d-4k9gk   1/1     Running   0          4m43s   10.4.0.14   gke-in28minutes-cluster-default-pool-c8956bc6-k34n   <none>           <none>
	pod/hello-world-rest-api-v2-587d7bbb4d-jm4gn   1/1     Running   0          4m43s   10.4.2.32   gke-in28minutes-cluster-default-pool-c8956bc6-7s5g   <none>           <none>


	NAME                           TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE     SELECTOR
	service/hello-world-rest-api   LoadBalancer   10.0.7.20    34.68.185.83   8080:31771/TCP   4m43s   app=hello-world-rest-api
	service/kubernetes             ClusterIP      10.0.0.1     <none>         443/TCP          5d8h    <none>


	NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS             IMAGES                                       SELECTOR
	deployment.apps/hello-world-rest-api-v1   2/2     2            2           4m47s   hello-world-rest-api   in28min/hello-world-rest-api:0.0.1.RELEASE   app=hello-world-rest-api,version=v1
	deployment.apps/hello-world-rest-api-v2   2/2     2            2           4m45s   hello-world-rest-api   in28min/hello-world-rest-api:0.0.2.RELEASE   app=hello-world-rest-api,version=v2

	NAME                                                 DESIRED   CURRENT   READY   AGE     CONTAINERS             IMAGES                                       SELECTOR
	replicaset.apps/hello-world-rest-api-v1-ffb76b89b    2         2         2       4m47s   hello-world-rest-api   in28min/hello-world-rest-api:0.0.1.RELEASE   app=hello-world-rest-api,pod-template-hash=ffb76b89b,version=v1
	replicaset.apps/hello-world-rest-api-v2-587d7bbb4d   2         2         2       4m45s   hello-world-rest-api   in28min/hello-world-rest-api:0.0.2.RELEASE   app=hello-world-rest-api,pod-template-hash=587d7bbb4d,version=v2
	
	
	We can see that deployment and replica set using two different labels. 
	However for service there only just one label. 
	We can also see that 2 of the pods have v1 label and 2 of the pods have v2 label.
	
	
	If we fire the request using load balancer, curl, we can see load is being distributed between 4 different instance of 2 different versions => 
		
		watch -n 0.1 curl 34.68.185.83:8080/hello-world 
	
	
	Now lets say that I want to only send load to v2 instance. 
	We can do this by updating the selectot in SERVICE. 
	Right now the selector is, just one lable - 
		
		spec:
		  ....
		  
		  selector:
			app: hello-world-rest-api
		
	We will add another label version: v2, because of this that service will only point to those pods which have those two labels applied (app: hello-world-rest-api and version: v2)
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\01-hello-world-rest-api>kubectl apply -f deployment.yaml
	deployment.extensions/hello-world-rest-api-v1 unchanged
	deployment.extensions/hello-world-rest-api-v2 unchanged
	service/hello-world-rest-api configured
	
	Now using curl command we can see that traffic going only to v2 version of instances. 
	We can again update version to v1 and see traffic only going to v1.
	
	So we can actually have 2 versions of application deployed and then decide which version should have the traffic. Cool. 
	
	That's how powerful labels are.
	
	Labels are what tie the deployment to replica set, tie deployment to a pod, and a service to a pods (like we saw just now).
	So pod is linked to everything else using labels and selectors.
	
	
Section 5: GKE - Using Kubernetes and Docker with Java Spring Boot Todo Web Application

Step 01 - Setting up 02 Spring Boot Todo Web Application in Local
	
	We will deploy a web application and we would deploy the web application as a war file. We will use 02-todo-web-application-h2.
	
	On a overview level, what we have is a web application, which is secured using Spring Security, which is build using Spring Boot, and which uses JPA and Hibernate to talk to the in memory database h2.
	
	Packaging is war. We want to run it in a web server like tomcat. 
	One of the intresting thing with Spring Boot is, when I want to package something as a war, I need to exclude tomcat from the list of dependencies. 
	The typical starter which typically is used when we develop a web applications with spring boot is spring-boot-starter-web and spring-boot-starter-web would automatically include tomcat into our deployable unit. And when i want to run application as war, I would not want tomcat in dependencies. And thats the reason why we make scope of spring-boot-starter-tomcat as provided. So this spring-boot-starter-tomcat would not be included into the war file that we create. 
	
	Second change to run it as war file, is to extend our application class with SpringBootServletInitializer and override the configure method of that class. 
	 
	We can easily create projects of this kind using start.spring.io and selecting packing as war. 
		
	
tep 02 - Pushing Docker Image to Docker Hub for Spring Boot
Todo Web App	
	
	We are pushing this image to docker hub, beacuse when we will deploy using kubectl cmd, or using yaml, we specify the image name and that image will be then downloaded from docker hub.
	
	We build a docker image for the application, tried to run out image as a container in our local using docker run command. 
	Later we pushed the image to our docker hub account.
	
	Now we would want to deploy the application into kubernetes.
	
	
Step 03 - Using Kubernetes YAML Config to Deploy Spring Boot Todo Web App
	
	In this step we will see how to deploy the web application on kubernetes. 
	
	Once we create a contianer, how we deploy the application to kubernetes is exactly same. 
	We would create the deployment.yaml file and deploy the application.
	
		kubectl delete all -l app=hello-world-rest-api
		
	We will copy the deployment.yaml from hello-world-rest-api and copy in todo-web-application-h2, and update the deployment.yaml with only one deployment. 
	Remove all the references of v2 label. 
	
	apiVersion: v1
	kind: Service
	metadata:
	  labels:
		app: todo-web-application-h2
	  name: todo-web-application-h2
	  namespace: default
	spec:
	  ports:
	  - nodePort: 31771
		port: 8080
		protocol: TCP
		targetPort: 8080
	  selector:					#Selectors for pods
		app: todo-web-application-h2
	  sessionAffinity: ClientIP		
	  type: LoadBalancer
	
	sessionAffinity - because this is a web application, we would want the seesion to be tied to a specific ip. So if I am getting a request from specific ip, all the rest of the request I want to go to the same pod. So if pod-1 got the first request, I would want pod-1 to also get the subsequent request. 
	Because we have sessions inside the web applications, it is important to have the right sessionAffinity configured. 
	
	sessionAffinity: ClientIP - so we want the client ip to map the sessions. So all the requests from the single clientIp we would want them to go to the same pod. 
	
	So lets deploy the application now - 
		
		kubectl apply -f deployment.yaml
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\02-todo-web-application-h2>kubectl apply -f deployment.yaml
	deployment.extensions/todo-web-application-h2 created
	service/todo-web-application-h2 created
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\02-todo-web-application-h2>kubectl get all
	NAME                                           READY   STATUS    RESTARTS   AGE
	pod/todo-web-application-h2-867b4c45b5-j6phh   1/1     Running   0          61s
	pod/todo-web-application-h2-867b4c45b5-vz9r5   1/1     Running   0          61s


	NAME                              TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE
	service/kubernetes                ClusterIP      10.0.0.1     <none>          443/TCP          6d4h
	service/todo-web-application-h2   LoadBalancer   10.0.4.126   34.69.198.144   8080:31771/TCP   61s


	NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/todo-web-application-h2   2/2     2            2           63s

	NAME                                                 DESIRED   CURRENT   READY   AGE
	replicaset.apps/todo-web-application-h2-867b4c45b5   2         2         2       65s

	
	We can pick up the external ip from above and check - http://34.69.198.144:8080/login
	
	Now we have deployed the application but the problem here is, we are talking to in memory database h2. 
	
	And what happens is these two instances of todo-web-application now has there own databases. 
	So depending on which pod or instance we are talking to, our data might be different.
	
	To fix it, we will use mysql db and we will have multiple instance, talking to that db.
	
	
Step 04 - Playing with Kubernetes Commands - Top Node and Pod	
	
	Fetchs all the pods inside the current namespace =>
			
			kubectl get pods 
	
	If we want to get all the pods from all the namespaces =>
		
			kubectl get pods --all-namespaces

		This will retrieve all the things that are running in all the namespaces which are present. 
		
	We can also pass a filter based on a label =>
	
		kubectl get pods -l app=todo-web-application-h2
		
		So this will return only those things that would match those labels. 
		
	Same thing will happen with all namespaces =>
		
		kubectl get pods -l app=todo-web-application-h2 --all-namespaces
		
	
	kubectl get services --all-namespaces
		
	Lets say we want to sort the above services by type, the way we can do that is using our yaml definition. 'type' in yaml is at, spec -> type: LoadBalancer
	
		kubectl get services --all-namespaces --sort-by=.spec.type
		
	Like this, we can pick any other thing as well, to sort on. 
		
		kubectl get services --all-namespaces --sort-by=.metadata.name
		
	kubectl cluster-info => gives general information of different things that are running as part of our cluster. 
	
	kubectl top node => it retrieves all the stuff based on the CPU utilization.
						So its getting the node and listing it down with its CPU and memory utilization. 
						
	kubectl top pod => Will return all the pods based on their utilization. 
	
	Few shortcuts =>
	
	kubectl get services  =  kubectl get svc
	
	Similar to this there are number of other shortcuts.
	
	kubectl get event  =  kubectl get ev
	
	kubectl get replicaset  =  kubectl get rs
	
	kubectl get namespaces  =  kubectl get ns
	
	kubectl get nodes  =  kubectl get no
	
	kubectl get pods  =  kubectl get po
	
	
Section 6: GKE - Using Kubernetes and Docker with Java Todo
Web Application using MySQL		
		

Step 01 - Code Review of 03 Java Todo Web Application MySQL


	We will now connect mysql and use 03-todo-web-application-mysql
	
	Spring boot makes it very easy to connect to mysql. Changes which needs to be done are there in readme.md
	
	Unit Tests use H2 database, so we have changed the its scope to test in pom.xml
	
	We won't have to install mysql because we will be using its image. 
	
	
Step 02 - Running MySQL as Docker Container on Local	
	
	We will run mysql database on our local machine.
	
	We can install mysql by using traditional approach by using installer, or we can use modern approach by using docker to run mysql in our local.
	To be able to do that we will find docker image for mysql. 
	
	We have done this in docker, you will find notes there. 
	
	To run mysql, if image is not present it will pull it =>
	
		docker run -e MYSQL_ROOT_PASSWORD=dummypassword mysql:5.7
	
		This command would lauch up the mysql server, but inside the mysql server, we would want to create an database, we would want to configure a userid, password, to that =>

		docker run --detach --env MYSQL_ROOT_PASSWORD=dummypassword --env MYSQL_USER=todos-user --env MYSQL_PASSWORD=dummytodos --env MYSQL_DATABASE=todos --name mysql --publish 3306:3306 mysql:5.7

		--publish 3306:3306, if don't do this then we won't be able to connect to localhost from mysql shell.

		Same steps we did in docker.
		
		Connect the mysql using mysql shell => \connect todos-user@localhost:3306
		
		 MySQL  localhost:3306 ssl  JS > \sql
		Switching to SQL mode... Commands end with ;
		Error during auto-completion cache update: ClassicSession.runSql: Lost connection to MySQL server during query
		The global session got disconnected..
		Attempting to reconnect to 'mysql://todos-user@localhost:3306'..
		The global session was successfully reconnected.

		 MySQL  localhost:3306 ssl  SQL > use todos;
		Default schema set to `todos`.
		Fetching table and column names from `todos` for auto-completion... Press ^C to stop.

		 MySQL  localhost:3306 ssl  todos  SQL > select * from todo;
		 
		We are now lauching up mysql as a docker container.
		
		
Step 03 - Connect Spring Boot Java Todo Web App to MySQL on
Local

	He just ran the application from IDE and then using browser inserted todo, check the table data using mysql shell.
	
	
Step 04 - Create Docker Image for 03 Todo Web Application and
Use Link to connect

	In the last step we launched up mysql as a container in our local machine and we launched up the web application directly from IDE.
	
	In this step we will launch up the web application as container and we will try to connect it to the mysql container running.
	
	After that we will deploy this setup exactly to kubernetes.
	
	We will build the image by doing mvn clean install.
	
	Then we will run the application an we would want to connect it to mysql.
	
	docker container run -p 8080:8080 in28min/todo-web-application-mysql:0.0.1-SNAPSHOT
		
	Using this command the application won't be able to connect to mysql, because docker uses and internal network of its own and in the internal network localhost is something else, therefore we cannot use localhost to connect. (check docker tutorial for more details)
	To run this we will have to link to the container, in addition to that we will have to use a environment variable and tell that host name is not localhost(picks localhost from application.properties), its mysql => 

	docker container run -p 8080:8080 --link=mysql -e RDS_HOSTNAME=mysql  in28min/todo-web-application-mysql:0.0.1-SNAPSHOT
	
	
	So till now we have the application in the container, mysql in container and they are able to talk to each other.
	
	
Update to Step 04 - Launching Containers in Custom Network

	A better alternative to --link is to launch both the applications into a custom docker network.

	Here are the commands you can use:



	docker network ls
	docker network create web-application-mysql-network
	docker inspect web-application-mysql-network
	 
	docker run --detach --env MYSQL_ROOT_PASSWORD=dummypassword --env MYSQL_USER=todos-user --env MYSQL_PASSWORD=dummytodos --env MYSQL_DATABASE=todos --name mysql --publish 3306:3306 mysql:5.7
	
	above command should also contain --network. Check.	
	
	 
	docker run --detach --env MYSQL_ROOT_PASSWORD=dummypassword --env MYSQL_USER=todos-user --env MYSQL_PASSWORD=dummytodos --env MYSQL_DATABASE=todos --name mysql --publish 3306:3306 --network=web-application-mysql-network mysql:5.7
	 
	docker inspect web-application-mysql-network


Step 05 - Playing with Docker Compose

	We know this, check docker notes.
	
	One of the intresting thing about docker-compose is that once we write the docker-compose configuration file, we can easily convert that into a kubernetes configuration file as well. 
	

Step 06 - Using Kompose to generate Kubernetes Deployment
Configuration

	There is a tool which helps in converting the docker-compose configuration file to kubernetes. The tool is called Kompose. 
	
	# Linux
	curl -L https://github.com/kubernetes/kompose/releases/download/v1.21.0/kompose-linux-amd64 -o kompose

	# macOS
	curl -L https://github.com/kubernetes/kompose/releases/download/v1.21.0/kompose-darwin-amd64 -o kompose

	# Windows
	curl -L https://github.com/kubernetes/kompose/releases/download/v1.21.0/kompose-windows-amd64.exe -o kompose.exe

	Go to path where docker-compose.yml is present and execute => kompose convert
	
	
Step 07 - Review Kubernetes YAML for MySQL and Java Web Application

	We can see that using Kompose 3 files for mysql and 2 files for todo-web-application were generated. 
	
	Kompose is a command line tool, means we need to copy the exe where our docker-compose.yml is present and then do 'kompose convert' from there.
	
	To execute kompose command, the kompose.exe must be present in cmd working dir.
	
	C:\Users\inarajp\Desktop\temp\kubernetes-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kompose convert
	[36mINFO[0m Kubernetes file "mysql-service.yaml" created
	[36mINFO[0m Kubernetes file "todo-web-application-service.yaml" created
	[36mINFO[0m Kubernetes file "mysql-deployment.yaml" created
	[36mINFO[0m Kubernetes file "mysql-database-data-volume-persistentvolumeclaim.yaml" created
	[36mINFO[0m Kubernetes file "todo-web-application-deployment.yaml" created


	We can clean up few things in kompose generated configuration. You can find original before clean up file in folder - kompose-generated-before-clean-up.
	
	From mysql we want to store data into a permanent volume. 
	We want to actually have some persistence kind of storage where we would want to store all the mysql data into. And that's why we had created a volumes Earlierr. 
	Thing is when we have volumes then we have to attach it to a persistentvolumeclaim. 
	So what happens in kubernetes is we create a persistentvolumeclaim, saying I would want this much amount of storage, and we are linking this from our mysql deployment.
	
	There one small change which is needed inside our mysql-deployment.yaml. 
	There is a bug which is open, when deploying this image of mysql:5.7 to kubernetes, there is one small configuration that we need to add in, you need to add a argument. 
	
	args:
          - "--ignore-db-dir=lost+found"
		  
	What happens is this lost+found dir is created in our persistent storage and mysql does not like any directory being present when its initializing, that's why it would start failing. So this specific argument is to ensure that mysql doesn't crash and it ignore that lost+found directory.
	
	In this step we looked into all the configuration files that there are, that will enable us to deploy our application to kubernetes.
	
	
Step 08 - Deploy MySQL Database to Kubernetes Cluster

	In the previous step we created all the configuration that is needed to start up our mysql server and the todo-web-application in kubernetes. 
	
	So lets start with creating our mysql database instances on kubernetes. 
	
	What we are looking at right now is, how to manage a database using kubernetes, this is not something ideal. Typically in real world senario we would want to use something which is already created on google cloud, we would want to actually use a managed database which is present in the cloud.
	What we want to learn right now is we would want to create a persistent volumes and we would actually want to connect the database to the persistent volume and have a application talk to this database.

	kubectl apply -f mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml
	(don't keep space after comma)

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl apply -f mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml
	persistentvolumeclaim/mysql-database-data-volume created
	deployment.extensions/mysql created
	service/mysql created
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get all
	NAME                         READY   STATUS    RESTARTS   AGE
	pod/mysql-5f567c76cc-nfnn2   1/1     Running   0          3m44s


	NAME                 TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE
	service/kubernetes   ClusterIP      10.0.0.1     <none>          443/TCP          9d
	service/mysql        LoadBalancer   10.0.3.168   35.193.24.122   3306:32301/TCP   3m43s


	NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/mysql   1/1     1            1           3m45s

	NAME                               DESIRED   CURRENT   READY   AGE
	replicaset.apps/mysql-5f567c76cc   1         1         1       3m46s

	
	kubectl get service --watch => will keep running this command again and again.
	
	
	We have got the external ip - 35.193.24.122, so we will use it to connect to mysql database. 
	
	So open the mysql shell => 
	
	MySQL  localhost:3306 ssl  todos  SQL > \connect todos-user@35.193.24.122
	
	Creating a session to 'todos-user@35.193.24.122'
	Please provide the password for 'todos-user@35.193.24.122': **********
	
	Save password for 'todos-user@35.193.24.122'? [Y]es/[N]o/Ne[v]er (default No): n
	
	Fetching schema names for autocompletion... Press ^C to stop.
	Closing old connection...
	Your MySQL connection id is 2
	Server version: 5.7.29 MySQL Community Server (GPL)
	No default schema selected; type \use <schema> to set one.

	MySQL  35.193.24.122:3306 ssl  SQL > \sql
	MySQL  35.193.24.122:3306 ssl  SQL > use todos;
	Default schema set to `todos`.
	Fetching table and column names from `todos` for auto-completion... Press ^C to stop.

	MySQL  35.193.24.122:3306 ssl  todos  SQL > select * from todo;
	ERROR: 1146 (42S02): Table 'todos.todo' doesn't exist
	
	So we are able to connect to mysql and once we launch up the application the todo table will be created.
	
	
	Now lets launch up the application =>
		
		kubectl apply -f todo-web-application-deployment.yaml,todo-web-application-service.yaml

	Before running this the image should be present in the docker hub, else there would be problems with pods. 
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl apply -f todo-web-application-deployment.yaml,todo-web-application-service.yaml
	
	deployment.extensions/todo-web-application created
	service/todo-web-application created

	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get all
	NAME                                        READY   STATUS    RESTARTS   AGE
	pod/mysql-5f567c76cc-nfnn2                  1/1     Running   0          19m
	pod/todo-web-application-54cb765748-479nd   1/1     Running   0          87s


	NAME                           TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE
	service/kubernetes             ClusterIP      10.0.0.1     <none>          443/TCP          9d
	service/mysql                  LoadBalancer   10.0.3.168   35.193.24.122   3306:32301/TCP   19m
	service/todo-web-application   LoadBalancer   10.0.6.164   34.69.198.144   8080:30525/TCP   87s


	NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/mysql                  1/1     1            1           19m
	deployment.apps/todo-web-application   1/1     1            1           89s

	NAME                                              DESIRED   CURRENT   READY   AGE
	replicaset.apps/mysql-5f567c76cc                  1         1         1       19m
	replicaset.apps/todo-web-application-54cb765748   1         1         1       89s

	
	Check or follow logs =>
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl logs todo-web-application-54cb765748-479nd -f
	
	
	For now we have mysql database which is manage by kubernetes and also todo-web-application application managed by kubernetes. 
	
	
Step 09 - Understanding Persistent Storage with Kubernetes - PV
and PVC

	In previous step we defined few yaml files where we asked for some data storage. Let's look at that in depth right now.
		
	So we would want to create a mysql database and the mysql database needs to store the data somewhere on the hard disk. 
	
	So typically when we talk about cloud there are variety of storages that are provided. 
	And typically when you want to store some kind of database information we would probably go for something like a block storage.
	So when we look at the block storage options, which are present on the cloud, typically AWS provides EBS(Elastic Block Store), Google Cloud provides GCE(Google Computing Engine) Persistent Disk - GCEPD.
	So we would want a persistent disk so that we can actually store our mysql databases.
	
	We what we did was we created a PersistentVolumeClaim and we said we want some amount of storage. 
	From the deployment of mysql database i.e. mysql-deployment.yaml we linked to it by asking for volumes. So we said we would want a volume and linked to a PersistentVolumeClaim.
	
		volumes:
		- name: mysql-database-data-volume
		  persistentVolumeClaim:
			claimName: mysql-database-data-volume
	
	Now lets see whats happening in the background. 
	
	kubectl get pv	=> pv stands for Persistent Volume
	
	kubectl get pvc	=> pvc stands for a Persistent Volume Cliam
	
	What we had created was PersistentVolumeClaim, we said we needed some amount of hard disk space.
	
	Lets look at the big picture, right now we are requesting for harddisk but you might want any kind of storage. 
	So typically whenever we talk about any kind of storage in kubernetes cluster what we are asking for is volume. 
	Any storage that you would request for on a kubernetes cluster is called a volume. The volume can  be absolutely anything, AWS Elastic Block Store or a GCEPD. 
	To talk to a volume we would need a Persistent Volume. 
	So a Persistent Volume is how we map external storage into our cluster.
	And a Persistent Volume Claim is how a pod can ask for the Persistent volume
	
	So in summary we have the volume which is the storage, and when you would want to use the volume you would need a persistent volume and the persistent volume is kubernetes cluster's access to a disk. 
	
	So whenever we want to use a storage in kubernetes cluster, you would need to create a Persistent Volume (pv).
	This pv can be shared accross multiple deployments in the same cluster. 
	
	And how does the pods get access to persistent volume? They get access to it through the Persistent Volume Claim. I need some amount of space I would ask for Persistent Volume Claim.
	
	
Step 10 - Using Config Maps for Centralized Configuration with
Kubernetes

	One of the thing that we observed is that all our configuration is actually present directly in our deployment.yaml 
	This is the case for mysql and this is also the case for todo-web-application and all the configuration is directly in here, and that's not good. The password is here, the db name is here, username is here, etc.
	
	To fix this we will use centralized configuration which is provided by kubernetes and it is called Config Maps.
	
	We will store all the typical configurations to config maps and we will store things like username and passwords to Secrets. 

	Any application has lot of configurations, and typically you would want a centralized management solution so that we can manage all our configuration for a specific enviornment at a single place and that's what Config Maps and Secrets allows us to do. 

	First step is to create a Config Map =>
		
		kubectl create ConfigMaps todo-web-application-config --from-literal=RDS_DB_NAME=todos
		
		todo-web-application-config -> name of the ConfigMaps
		RDS_DB_NAME -> literal in todo-web-application-deployment.yaml 
		todos -> value
		
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl create ConfigMaps todo-web-application-config --from-literal=RDS_DB_NAME=todos
	
	ConfigMaps/todo-web-application-config created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get ConfigMaps todo-web-application-config
	NAME                          DATA   AGE
	todo-web-application-config   1      44s

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl describe todo-web-application-config
	error: the server doesn't have a resource type "todo-web-application-config"

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl describe ConfigMaps/todo-web-application-config
	Name:         todo-web-application-config
	Namespace:    default
	Labels:       <none>
	Annotations:  <none>

	Data
	====
	RDS_DB_NAME:
	----
	todos
	Events:  <none>
		

	SO now we want to pick the values from ConfigMaps so we will update the configuration file => 
	
		- name: RDS_DB_NAME
          #value: todos
          valueFrom:
            ConfigMapsKeyRef:
              key: RDS_DB_NAME			#name of the literal which we gave
              name: todo-web-application-config		#name of the ConfigMaps
	
	
	So what we are doing now is we are picking up the configuration which is configured on a kubernetes cluster. 
	
	So all the config map values would be directly configured on the kubernetes cluster, they can be managed by the administrator. It does not need to be part of our application code or deployment files, we can directly put them in kubernetes cluster ConfigMaps.

	kubectl apply -f todo-web-application-deployment.yaml

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl apply -f todo-web-application-deployment.yaml
	deployment.extensions/todo-web-application configured

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	mysql-5f567c76cc-nfnn2                  1/1     Running   0          22h
	todo-web-application-7b996658c5-89dkx   1/1     Running   0          12s


	We can see that new pod has been started 12s 
	We can also access the url in browser and check if everything is working fine. 

	Now lets switch all the other values also to our existing config map => 
	
		kubectl edit ConfigMaps/todo-web-application-config
		
		After this command a vi editor is opened in CLoud Shell. 
		In vi editor we can press 'i' to get into insert mode and add our values in 'data'.
		
		We didn't add RDS_PASSWORD because we will add it in secret. 
	
		After adding values press esc, then you will be outside edit mode and then we can save it by entering ':wq' 
		If we don't want to save we can do esc and then ':q!'
		
	If we have issues in windows while editing, we can go in cloud shell and do it from there. 	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl describe ConfigMaps/todo-web-application-config
	Name:         todo-web-application-config
	Namespace:    default
	Labels:       <none>
	Annotations:  <none>

	Data
	====
	RDS_DB_NAME:
	----
	todos
	RDS_HOSTNAME:
	----
	mysql
	RDS_PORT:
	----
	3360
	RDS_USERNAME:
	----
	todos-user
	Events:  <none>
	
	Now we need to also make changes in todo-web-application-deployment.yaml =>
	
	spec:
      containers:
      - env:
        - name: RDS_DB_NAME
          #value: todos
          valueFrom:
            ConfigMapsKeyRef:
              key: RDS_DB_NAME
              name: todo-web-application-config
        - name: RDS_HOSTNAME
          #value: mysql
          valueFrom:
            ConfigMapsKeyRef:
              key: RDS_HOSTNAME
              name: todo-web-application-config
        - name: RDS_PASSWORD
          value: dummytodos
        - name: RDS_PORT
          #value: "3306"
          valueFrom:
            ConfigMapsKeyRef:
              key: RDS_PORT
              name: todo-web-application-config
        - name: RDS_USERNAME
          #value: todos-user
          valueFrom:
            ConfigMapsKeyRef:
              key: RDS_USERNAME
              name: todo-web-application-config

	Now intresting thing now is how do we restart the application. In kubernetes it is always declarative. There are no imparative styles. I cannot give commands saying restart this application, stop this application. 
	One of the ways you can scale a deployment is by decreasing the number of instances and increasing it back. So lets get it to zero and then back to one.
	
	kubectl scale deployment todo-web-application --replicas=0
	
		todo-web-application -> name of deployment. 

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl scale deployment todo-web-application --replicas=0
	deployment.extensions/todo-web-application scaled

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get pods
	NAME                     READY   STATUS    RESTARTS   AGE
	mysql-5f567c76cc-nfnn2   1/1     Running   0          23h

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl scale deployment todo-web-application --replicas=1
	deployment.extensions/todo-web-application scaled

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	mysql-5f567c76cc-nfnn2                  1/1     Running   0          23h
	todo-web-application-7b996658c5-89v8w   1/1     Running   0          9s
	
	Evrything should work fine now.
	
	In this step we made use of important kubernetes concept - ConfigMaps.
	ConfigMaps allows us to configure values directly on the kubernetes cluster. 
	These values can change from one enviornment to another environment. 
	So instead of having these values inside our deployable unit or instide our deployment files its better to have them on the cluster configuration. 
	This will help the administrator to easily configure these value whenever they change. So lets say password to db is changed, he can directly change it to kubernetes cluster. 
	This also provides a centralized configuration, where all the configuration of all the applications in the cluster is stored in the single place - ConfigMaps


Step 11 - Using Secrets with Kubernetes

	Lets say we have something like password, how do you store it, that's where we will use Secrets. 
	
	Instead of using directly value in configuration file, we want to store it on kubernetes and use it down from there.
	
	kubectl create secret generic todo-web-application-secrets --from-literal=RDS_PASSWORD=dummytodos
	
		generic -> type of secret
		
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl create secret generic todo-web-application-secrets --from-literal=RDS_PASSWORD=dummytodos
	secret/todo-web-application-secrets created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get secret/todo-web-application-secrets
	NAME                           TYPE     DATA   AGE
	todo-web-application-secrets   Opaque   1      37s

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl describe secret/todo-web-application-secrets
	Name:         todo-web-application-secrets
	Namespace:    default
	Labels:       <none>
	Annotations:  <none>

	Type:  Opaque

	Data
	====
	RDS_PASSWORD:  10 bytes


	We can see that kubectl describe doesn't actually show value of RDS_PASSWORD
	
	We will now use this in our application =>
	
		- name: RDS_PASSWORD
          #value: dummytodos
          valueFrom:
            secretKeyRef:
              key: RDS_PASSWORD
              name: todo-web-application-secrets

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl apply -f todo-web-application-deployment.yaml
	deployment.extensions/todo-web-application configured

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get pods
	NAME                                    READY   STATUS        RESTARTS   AGE
	mysql-5f567c76cc-nfnn2                  1/1     Running       0          24h
	todo-web-application-5ccf8fdcf5-r25nt   1/1     Running       0          7s
	todo-web-application-8589995bfb-7c6kd   0/1     Terminating   0          17m

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	mysql-5f567c76cc-nfnn2                  1/1     Running   0          24h
	todo-web-application-5ccf8fdcf5-r25nt   1/1     Running   0          25s

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl logs todo-web-application-5ccf8fdcf5-r25nt -f


	We can check the logs and see that there are no errors, and also able to access the application properly.
	
	
Step 12 - Creating a ClusterIP Kubernetes Service for MySQL
Database
	
	When we launched up mysql database in mysql service, we used a LoadBalancer. That means external people will also be able to get to the database. So we might not want that to happen always. 
	That's the reason we might prefer ClusterIP
	In ClusterIP, nobody else outside the cluster would be able to access the database directly. 
	So only your application which is deployed into the cluster will be able to talk to the database.
	
	spec:
		type: LoadBalancer
	
	
	Ideally we would change the LoadBalancer to ClusterIP, but there are few problems with this in kubernetes right now, so we will have to delete this service and create another service with clusterIP. 
	We can change easily to NodePort without deleting.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl delete service mysql
	service "mysql" deleted

	Change the service file =>
	
		spec:
			type: ClusterIP

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl apply -f mysql-service.yaml
	service/mysql created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get services
	NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)          AGE
	kubernetes             ClusterIP      10.0.0.1     <none>          443/TCP          10d
	mysql                  ClusterIP      10.0.8.115   <none>          3306/TCP         9s
	todo-web-application   LoadBalancer   10.0.6.164   34.69.198.144   8080:30525/TCP   24h

	We can see that ClusterIP has been attached to the mysql service now.
	
	Now we will delete everything that running.
	
		kubectl delete -f  mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml,todo-web-application-deployment.yaml,todo-web-application-service.yaml

	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl delete -f  mysql-database-data-volume-persistentvolumeclaim.yaml,mysql-deployment.yaml,mysql-service.yaml,todo-web-application-deployment.yaml,todo-web-application-service.yaml

	persistentvolumeclaim "mysql-database-data-volume" deleted
	deployment.extensions "mysql" deleted
	service "mysql" deleted
	deployment.extensions "todo-web-application" deleted
	service "todo-web-application" deleted

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl get all

	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   10d
	
	
	We will also delete the ConfigMaps and the Secret =>
	
		kubectl delete ConfigMaps/todo-web-application-config
		
		kubectl delete secret/todo-web-application-secrets

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl delete ConfigMaps/todo-web-application-config
	configmap "todo-web-application-config" deleted

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\03-todo-web-application-mysql>kubectl delete secret/todo-web-application-secrets
	secret "todo-web-application-secrets" deleted

	
Section 7: GKE - Using Kubernetes and Docker with Java Spring
Boot Microservices

Step 01 - Introduction to Microservices


Step 02 - Advantages of Microservices


Step 03 - Microservices - Challenges


Step 04 - Overview of CCS and CES Spring Boot Microservices

	This is we will look how we can use simple microservices with all awesome kubernetes features like kubernetes naming features, and also kubernetes automatic load distribution features. 
	
	We will also see how a microservice build with spring cloud can make affective use of whatever features that are present in kubernetes. 
	
	After that we will look at important kubernetes concept called service mesh. We will look at one of the implementation of service mesh called Istio.
	
	Two microservices which we will be using are CCS and CES.
	
	
Step 05 - Push Docker Images and Create Kubernetes Config for
Microservices

	Create docker image for both the project by going in appropriate repository and then mvn clean install.
	
	Later push the docker image to docker hub, so we will use the imgae from docket hub.
	
	About deployment.yaml =>
		
		When we start up the application now we have specified minReadySeconds: 45
		What kubernetes does is it waits for 45 seconds and then sends the request to the application. It doesn't know if the url is up or not. 
		So if our application is slower than 45 second, what will happen is our application will be getting request when it might not be ready to process them. 
		And this is where the readinessProbe comes into picture. 
		SO what happens is kubernetes will fire the request to the root / and if it gets proper response back then it will start sending requests to it. 
		And it gives initialDelaySeconds of 50 seconds. 
		It marks it as a failure only if you fail 5 times in sending a request to / (failureThreshold: 5) in intervals of 10 seconds (periodSeconds: 10)
		
			readinessProbe:
			  httpGet:
				path: /
				port: liveness-port
			  failureThreshold: 5
			  periodSeconds: 10
			  initialDelaySeconds: 60
		
	Other thing configured is livelinessProbe. 
	It is typically used to check the health of an application. 
	So lets say the load balancer wants to check if the application is up and running or not, so it will send request to /
	If the load balancer is balancing the load between 15 pods lets say, it wants to find out whether each pod is running, so it sends the request to / every 10 seconds (periodSeconds: 10). 
	And the failureThreshold is 10, so it will wait until it fails 5 times before it marks the pod as dead and it will create a new pod and kill this pod. 

		livenessProbe:
          httpGet:
            path: /
            port: liveness-port
          failureThreshold: 5
          periodSeconds: 10
          initialDelaySeconds: 60
	
	So these livelinessProbe and readinessProbe are very usefull.
	Whenever we have multiple instances, we want them up and running. And if there is something going wrong, you are running out of memory or something, the livelinessProbe helps us in detecting them.
	
	
Step 06 - Deploying Spring Boot Microservices to Kubernetes
Cluster

	In this step we will deploy these two application to kubernetes.
	
		kubectl apply -f deployment.yaml

	Go in both appropriate folder and execute above command.
	
	
	Have asked a question in this lecture. Check.
	
	.....
	
	EnvironmentConfigurationLogger.java logs all the stuff that is configures in our application.properties, in enviornment variables, evrything is printed out in the logs. 
	
	
	We will start CCS also,
		
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl apply -f deployment.yaml
	deployment.extensions/currency-conversion created
	service/currency-conversion created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl get all
	NAME                                       READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-847cbfbb97-jttbx   1/1     Running   0          65s
	pod/currency-conversion-847cbfbb97-rtdhr   1/1     Running   0          65s
	pod/currency-exchange-7677985bdc-n6tnd     1/1     Running   0          47m
	pod/currency-exchange-7677985bdc-tm2vr     1/1     Running   0          47m


	NAME                          TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)          AGE
	service/currency-conversion   LoadBalancer   10.0.0.165    35.225.86.185    8100:31271/TCP   65s
	service/currency-exchange     LoadBalancer   10.0.11.182   35.224.129.100   8000:31492/TCP   47m
	service/kubernetes            ClusterIP      10.0.0.1      <none>           443/TCP          15d


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   2/2     2            2           66s
	deployment.apps/currency-exchange     2/2     2            2           47m

	NAME                                             DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-847cbfbb97   2         2         2       67s
	replicaset.apps/currency-exchange-7677985bdc     2         2         2       47m


	aniketrajput90@cloudshell:~$ curl 35.225.86.185:8100/currency-conversion/from/USD/to/INR/quantity/10
	{"id":10001,"from":"USD","to":"INR","conversionMultiple":65.00,"quantity":10,"totalCalculatedAmount":650.00,"exchangeEnvironmentInfo":"currency-exchange-7677985bdc-n6tnd v1 n6tnd","conversionEnvironmentInfo":"currency-conversion-847cbfbb97-rtdhr v1 rtdhr"}



Step 07 - Microservices and Kubernetes Service Discovery - Part 1
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl get all
	NAME                                       READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-847cbfbb97-jttbx   1/1     Running   0          65s
	pod/currency-conversion-847cbfbb97-rtdhr   1/1     Running   0          65s
	pod/currency-exchange-7677985bdc-n6tnd     1/1     Running   0          47m
	pod/currency-exchange-7677985bdc-tm2vr     1/1     Running   0          47m


	NAME                          TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)          AGE
	service/currency-conversion   LoadBalancer   10.0.0.165    35.225.86.185    8100:31271/TCP   65s
	service/currency-exchange     LoadBalancer   10.0.11.182   35.224.129.100   8000:31492/TCP   47m
	service/kubernetes            ClusterIP      10.0.0.1      <none>           443/TCP          15d


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   2/2     2            2           66s
	deployment.apps/currency-exchange     2/2     2            2           47m

	NAME                                             DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-847cbfbb97   2         2         2       67s
	replicaset.apps/currency-exchange-7677985bdc     2         2         2       47m

	
	aniketrajput90@cloudshell:~$ curl 35.225.86.185:8100/currency-conversion/from/USD/to/INR/quantity/10
	{"id":10001,"from":"USD","to":"INR","conversionMultiple":65.00,"quantity":10,"totalCalculatedAmount":650.00,"exchangeEnvironmentInfo":"currency-exchange-7677985bdc-n6tnd v1 n6tnd","conversionEnvironmentInfo":"currency-conversion-847cbfbb97-rtdhr v1 rtdhr"}
	
	Here we can see that the response contains name of the pod of CES which is returning the response to CCS and also the name of the pod which is being used by CCS.
	
	We can see that the CCS is automatically able to call the CES, so how is CCS able to find out where it is and able to talk? 
	
	@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_URI:http://localhost}:8000")
	public interface CurrencyExchangeServiceProxy {
		//http://localhost:8000/currency-exchange/from/USD/to/INR
		@GetMapping("/currency-exchange/from/{from}/to/{to}")
		public CurrencyConversionBean retrieveExchangeValue(@PathVariable("from") String from,
				@PathVariable("to") String to);
	}

	Here we said this is the enviornment variable name - CURRENCY_EXCHANGE_URI, we didn't configure it at all. So how is it getting configured?
	
	In logs we can find this -
	
	2020-03-31 06:37:33.564  INFO [currency-conversion,,,] 1 --- [           main] i.m.c.u.e.EnvironmentConfigurationLogger : CURRENCY_EXCHANGE_SERVICE_HOST - 10.0.11.182
	
	10.0.11.182 is the cluster ip for the load balancer. 
	
	EnvironmentConfigurationLogger.java has logged this log.
	
	What happens in kubernetes is, when any pod is starting up, it gets enviornment variables based on whatever services are up and ruinning at that particular time. 	
	So if at the time when this pod is launching up, there are three service available, enviornment variables based on these 3 services are prepopulated into enviornment variables. 
	Thats why we can see in logs -
	
	2020-03-31 06:37:33.564  INFO [currency-conversion,,,] 1 --- [           main] i.m.c.u.e.EnvironmentConfigurationLogger : CURRENCY_EXCHANGE_SERVICE_HOST - 10.0.11.182
	
	2020-03-31 06:37:33.562  INFO [currency-conversion,,,] 1 --- [           main] i.m.c.u.e.EnvironmentConfigurationLogger : CURRENCY_CONVERSION_SERVICE_HOST - 10.0.0.165
	
	The naming is the Name_of_the_service_in_Upper_case + SERVICE_HOST
	
	So all these enviornment variables are injected in by kubernetes when a pod is starting up. And we can use this specific enviornment variables to find another service. 
	
	So when the CCS is starting up, kubernetes injects in the CURRENCY_EXCHANGE_SERVICE_HOST enviornment variable and thats how we are finding the service. 
	
	
Step 08 - Microservices and Kubernetes Service Discovery - Part 2
DNS	

	There is one problem with using an environment variable like CURRENCY_EXCHANGE_SERVICE_HOST depending upon the service on kubernetes cluster, lets say CCS is started first and at that time CES was not really running. So this CURRENCY_EXCHANGE_SERVICE_HOST environment variable will not be injected in as there is not associated service running as kubernetes only knows services which are up and running at the point where your application is started up. It doesn't know that in future there is another service which will be started. 
	So this is not really an ideal solution, the ideal solution is to use a service discovery which is provided by kubernetes.
	
	So instead of =>
		@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_SERVICE_HOST:http://localhost}:8000")
		
	we can use something like - http://currency-exchange  (http://Service-Name)
	
	We can see using kubectl get services that service name is currency-exchange. If we have something like this defined kubernetes service discovery will helps us to find the location of this particular service. 
	
	So how does this works, whenever the new kubernetes service is started up it will automatically register with this DNS(http://currency-exchange), so when the CES comes up it would register with the DNS, when CCS comes up it would register with the DNS, when kubernetes comes up it would register with DNS. 
	So we can as the DNS in this format - http://currency-exchange and it would return the location of this service. 
	
	Now the question is, how can we use http://currency-exchange when we are in kubernetes enviornment and when we are on local enviornment we use localhost:8000? 
	We can do it by =>
		
		@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_URI:http://localhost}:8000")
		
	We need to configure http://currency-exchange this value for CURRENCY_EXCHANGE_URI in deployment.yaml of CCS=>
	
		spec:
		  containers:
		  - image: in28min/currency-exchange:0.0.1-RELEASE #CHANGE
			imagePullPolicy: IfNotPresent
			name: currency-exchange
			env:
			  - name: CURRENCY_EXCHANGE_URI
			    value: http://currency-exchange
			  
		We can also add this in configmap

	Other option is to configure a profile instead of configuiring enviornment variable.
	
	That is we can have a application-kubernetes.properties file and update the deployment.yaml of CCS as - 
	
	
		spec:
		  containers:
		  - image: in28min/currency-exchange:0.0.1-RELEASE #CHANGE
			imagePullPolicy: IfNotPresent
			name: currency-exchange
			env:
			  - name: CURRENCY_EXCHANGE_URI
			    value: http://currency-exchange
			  - name: SPRING_PROFILES_ACTIVE
			    value: kubernetes
	 
		And the application-kubernetes.properties file will contain =>
		
				CURRENCY_EXCHANGE_URI = http://currency-exchange
		
	Making use of Service Discovery provided by DNS in kubernetes is the best possible option because even if service comes up in later point of time the service can interact with it. 
	
	In logs we can see - 
	
	2020-03-31 06:37:33.556  INFO [currency-conversion,,,] 1 --- [           main] i.m.c.u.e.EnvironmentConfigurationLogger : CURRENCY_EXCHANGE_URI - http://currency-exchange

	We can always check which version pods are running by doing kubectl get rs -o wide, check desired and current
	
	In these steps we saw how Service Discovery is automatically enabled by kubernetes. 
	We saw two ways for doing service discovery, one was through enviornment variables other one is through domain name service or DNS which is directly provided by kubernetes.
	We saw that every service that is active, at the lauch of the application is injected in as enviornment variables. We can use those enviornment variables to access the applications but thats not awesome, because what would happen if service is not up when the second service is launching up, then it has no way to knowing about the first service even if it launches up little later. That's where Service Discovery based on DNS really helps. 
	We also saw two different ways to configure a Service Discovery based on DNS with your application, we configured an enviornment variable and we used that in ServiceProxy class. Other option was to configure a SPRING_PROFILES_ACTIVE and configure a property file based on it. 
	
	Awesome thing about kubernetes is that Load Balancing and Service Discovery are free. 
	
	
Step 09 - Microservice Discovery, Centralized Configuration and
Load Balancing	

	In addition to Service Discovery and Load Balancing kubernetes also provides Centralized Configuration. We saw configmap. 

	So 3 of the most important challenges in microservices world are free in kubernetes. 


Step 10 - Using Kubernetes Ingress to Simplify Microservice Access

	In the previous steps we created services and deployment for our CCS and CES. 
	And the service type which we gave was LoadBalancer. 
	Image we have 100 microservices, do we really want to have 100 microservices having 100 LoadBalancers? 
	It would be too expensive, because LoadBalancer is one of the expensive resources on the cloud. Whether its google cloud, whether its AWS or Azure, if we create a LoadBalancer we would be always charged, becasue its always running, ready and available.
	And that's the reason why we would want one LoadBalancer, which is highly available, which is taking all the request from all the microservices. 
	
	So how do we create a one LoadBalancer which would process requests for all the microservices? 
	That's where concept in kubernetes Ingress comes into picture. 
	Ingress allows us to have one LoadBalancer and from there we can actually route request to indiviual microservices and distribute the load amongst them. 
	
	How does it work?
	There a file called ingress.yaml in our project.
	
	apiVersion: extensions/v1beta1
	kind: Ingress
	metadata:
	  name: gateway-ingress
	  annotations:
		nginx.ingress.kubernetes.io/rewrite-target: /
	spec:
	  rules:
	  - http:
		  paths:
		  - path: /currency-exchange/*
			backend:
			  serviceName: currency-exchange
			  servicePort: 8000          
		  - path: /currency-conversion/*
			backend:
			  serviceName: currency-conversion
			  servicePort: 8100

	kind is Ingress, Ingress is just like a Service, Deployment, ReplicaSet. 
	
	In rule configuration we are defining what an Ingress should do. 
	When a request comes to path: /currency-exchange/* then it should send the request to service serviceName: currency-exchange
	If path: /currency-conversion/* then serviceName: currency-conversion
	
	Here 
	annotations:
		nginx.ingress.kubernetes.io/rewrite-target: /
	
	we are configuring that the entire url should be send to service as it is. 

	To create this =>
		
		kubectl apply -f ingress.yaml
		
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl apply -f ingress.yaml
	ingress.extensions/gateway-ingress created
	
	After sometime if you go in Google Cloud Platform and click on Services and Ingress you can see that the Ingress has been created successfully. 
	
	
Step 11 - Review Google Cloud Load Balancer Backend and
Frontends with Ingress

	
	After sometime if you go in Google Cloud Platform and click on Services and Ingress you can see that the Ingress has been created successfully. 
	
	We can see that enpoints are exposed as below -
	34.98.116.11/currency-exchange/* 
	34.98.116.11/currency-conversion/* 

	We can pick the IP and form the Url without port as below and check in browser - 
		
		http://34.98.116.11/currency-exchange/from/USD/to/INR
		http://34.98.116.11/currency-conversion/from/USD/to/INR/quantity/10
		
		We can see that we are getting the response.
		
	
	annotations:
		nginx.ingress.kubernetes.io/rewrite-target: /
		
		will remove the Ingress url part i.e http://34.98.116.11 and then check for which service the request is for and accordingly form the url with service-name and its port and then append the complete remaing part of url like - /currency-exchange/from/USD/to/INR
		This annotation tell to send complete remaining part.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl get services
	NAME                  TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)          AGE
	currency-conversion   LoadBalancer   10.0.0.165    35.225.86.185    8100:31271/TCP   6h43m
	currency-exchange     LoadBalancer   10.0.11.182   35.224.129.100   8000:31492/TCP   7h29m
	kubernetes            ClusterIP      10.0.0.1      <none>           443/TCP          15d

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl get ingress
	NAME              HOSTS   ADDRESS        PORTS   AGE
	gateway-ingress   *       34.98.116.11   80      17m


	Now in google cloud if we search for Load Balancing, we would see that there are 3 LoadBalancer right now. 
	Below 2 LoadBalancers are for the services. Thing is the first ingress LoadBalancer also maps to the services in backend. 
	However difference is that ingress one is in global region so it will be highly available. 
	And if we go inside it we can see Host and Path rules, all the backends which are configured, the health status of each one of them.
	Frontend is the url which we are using to access the application. 
	So we have frontend and backend and we have rules to match the frontend url to the backend. Thats what Ingress does thorugh the LoadBalancer which is created through google cloud. 
	

	So now we will remove the LoadBalancer for microservices and change them to NodePort, as we have a Ingress LoadBalancer with us now.
	
	To do that open both deployment.yaml of both services and change the Service section spec -> type from LoadBalancer to NodePort

	Do - kubectl apply -f deployment.yaml in both folders.

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl apply -f deployment.yaml
	deployment.extensions/currency-conversion unchanged
	service/currency-conversion configured

	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\05-currency-conversion-microservice-basic>kubectl get services
	NAME                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
	currency-conversion   NodePort    10.0.0.165    <none>        8100:31271/TCP   6h56m
	currency-exchange     NodePort    10.0.11.182   <none>        8000:31492/TCP   7h42m
	kubernetes            ClusterIP   10.0.0.1      <none>        443/TCP          15d

	
	We can see that the service has been changed to NodePort. 
	And if we go on Load Balancing page in google cloud we can see the below 2 LoadBalancers has been deleted and urls are still working.
	
	So now we don't really have loadBalancers we just have one default Ingress which is handling all the requests across multiple backend services. So we can have hundred microservices and all the request would flow through this specific ingress gateway
	

Section 8: GKE - Integrating Java Spring Cloud Kubernetes with
Spring Boot Microservices


Step 01 - Using Spring Cloud Kubernetes with Microservices -
Project Review

	We will make use of Spring Cloud Kubernetes.
	
	Import porject CCS-cloud - 06. 
	
	We added 2 new dependencies - 
	
		<dependency> <!-- CHANGE -->
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
		</dependency>

	Ribbon for client side load balancing. 
	The usefulness of client side load balancing when we already have Service Discovery and Load Balancing on Kubernetes is debatable.

		<dependency> <!-- CHANGE -->
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-starter-kubernetes-all</artifactId>
		</dependency>

	pring-cloud-starter-kubernetes-all brings its own feature in integrating kubernetes with the spring boot project.
	
	Other changes is we enabled Discovery Client. 
	With Spring Cloud Strater Kubernetes you can integrate directly with kubernetes discovery system. To do that we need to enable discovery client - @EnableDiscoveryClient. 

	And last thing changes is from - 
		
		@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_URI:http://localhost}:8000")//
		//@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_SERVICE_HOST:http://localhost}:8000")
		//@FeignClient(name = "currency-exchange-service")//Kubernetes Service Name
		public interface CurrencyExchangeServiceProxy {
			//http://localhost:8000/currency-exchange/from/USD/to/INR
			@GetMapping("/currency-exchange/from/{from}/to/{to}")
			public CurrencyConversionBean retrieveExchangeValue(@PathVariable("from") String from,
					@PathVariable("to") String to);
		}
		
	to - 
		
		
		//@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_URI:http://localhost}:8000")//http://currency-exchange
		//@FeignClient(name = "currency-exchange", url = "${CURRENCY_EXCHANGE_SERVICE_HOST:http://localhost}:8000")
		@FeignClient(name = "currency-exchange")//Kubernetes Service Name
		@RibbonClient(name = "currency-exchange")
		public interface CurrencyExchangeServiceProxy {
			@GetMapping("/currency-exchange/from/{from}/to/{to}")
			public CurrencyConversionBean retrieveExchangeValue(@PathVariable("from") String from,
					@PathVariable("to") String to);
		}
	
	
	We are using Feign with combination with Ribbon.

	@RibbonClient(name = "currency-exchange")	-> we have given name as whatever the kubernetes service name is.
	
	We want CCS to talk to CES service using Ribbon. 
	
	Check the changes in deployment.yaml file. 
	
	We have changed the version, so we can build the image and push to docker hub. 
	
	
Step 02 - Deploying Spring Cloud Kubernetes Microservices

	In this step we will deploy this project on kubernetes.
	
	kubectl apply -f deployment.yaml
	
	In deployment.yaml we have kept the deployment name and service name same as Earlierr, so what kubernetes think is that we are making a new release of CCS. It doesn't know that we have a separate code base and all, it doesn't worry about that. 
	So what will happen is whatever we put in will be replaced to whatever is present there.
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl get all
	NAME                                       READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-847cbfbb97-jttbx   1/1     Running   0          2d6h
	pod/currency-conversion-847cbfbb97-rtdhr   1/1     Running   0          2d6h
	pod/currency-exchange-7677985bdc-n6tnd     1/1     Running   0          2d7h
	pod/currency-exchange-7677985bdc-tm2vr     1/1     Running   0          2d7h


	NAME                          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
	service/currency-conversion   NodePort    10.0.0.165    <none>        8100:31271/TCP   2d6h
	service/currency-exchange     NodePort    10.0.11.182   <none>        8000:31492/TCP   2d7h
	service/kubernetes            ClusterIP   10.0.0.1      <none>        443/TCP          17d


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   2/2     2            2           2d6h
	deployment.apps/currency-exchange     2/2     2            2           2d7h

	NAME                                             DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-847cbfbb97   2         2         2       2d6h
	replicaset.apps/currency-exchange-7677985bdc     2         2         2       2d7h


	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl apply -f deployment.yaml
	deployment.extensions/currency-conversion configured
	service/currency-conversion unchanged

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl get all
	NAME                                       READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-56fd8454f7-cb852   1/1     Running   0          47s
	pod/currency-conversion-847cbfbb97-jttbx   1/1     Running   0          2d6h
	pod/currency-conversion-847cbfbb97-rtdhr   1/1     Running   0          2d6h
	pod/currency-exchange-7677985bdc-n6tnd     1/1     Running   0          2d7h
	pod/currency-exchange-7677985bdc-tm2vr     1/1     Running   0          2d7h


	NAME                          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
	service/currency-conversion   NodePort    10.0.0.165    <none>        8100:31271/TCP   2d6h
	service/currency-exchange     NodePort    10.0.11.182   <none>        8000:31492/TCP   2d7h
	service/kubernetes            ClusterIP   10.0.0.1      <none>        443/TCP          17d


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   3/2     2            2           2d6h
	deployment.apps/currency-exchange     2/2     2            2           2d7h

	NAME                                             DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-56fd8454f7   2         2         2       61s
	replicaset.apps/currency-conversion-847cbfbb97   1         1         1       2d6h
	replicaset.apps/currency-exchange-7677985bdc     2         2         2       2d7h

	So the setup we have right now is, we have Ingress which is routing down to the two different microservices. 
	What we did now is, CCS wasn't using spring cloud Earlierr, we have upgraded it to Spring Cloud.
	Things this brings in are the features we already had. Earlier we were doing server side load balancing as we were making use of kubernetes DNS, right now we have switched to use Ribbon client to do Client side Load Balancing. And we enabled Discovery Client to integrate with the kubernetes DNS system.

	There are couple of other features which Spring Cloud enables which we will discover later. 
	
	We can see one new pod and a replicaset. 
	
	We can hit urls and check - 
	
		http://34.98.116.11/currency-exchange/from/USD/to/INR
		
		CES works fine but 
		
		http://34.98.116.11/currency-conversion/from/USD/to/INR/quantity/10

		throws an error saying, Ribbon client is not able to talk to load balancer. Its trying to find out endpoint for CES but it says that Service account doesn't have access. So we need to configure a service account to have access to this. 
		

Step 03 - Using RBAC to allow Ribbon to access Service Discovery
APIs

	We got error in the Earlierr step that the Ribbon was not able to talk to the kubernetes discovery system. It says the configured service account doesn't have access. 
	
	We have a file rbac.yaml - 


		# NOTE: The service account `default:default` already exists in k8s cluster.
		# You can create a new account following like this:
		#---
		#apiVersion: v1
		#kind: ServiceAccount
		#metadata:
		#  name: <new-account-name>
		#  namespace: <namespace>

		---
		apiVersion: rbac.authorization.k8s.io/v1beta1
		kind: ClusterRoleBinding
		metadata:
		  name: fabric8-rbac
		subjects:
		  - kind: ServiceAccount
			# Reference to upper's `metadata.name`
			name: default
			# Reference to upper's `metadata.namespace`
			namespace: default
		roleRef:
		  kind: ClusterRole
		  name: view
		  apiGroup: rbac.authorization.k8s.io


	We are defining a ClusterRoleBinding. 
	We are mapping a service account to a role on the cluster, to be able to view all the information in the kubernetes cluster.
	
	So we would want to call k8s api from our application. 
	
	We already have a service account default with namespace default on the k8s cluster. 
	
	So if we do kubectl get serviceaccount we can see the account, 
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl get serviceaccount
	NAME      SECRETS   AGE
	default   1         17d
	
	
	And what we are doing is giving it a permission to access the api on k8s cluster. So we are providing the access of this discovery group - apiGroup: rbac.authorization.k8s.io, which contains the service discovery thing as well. And we are providing a view access - name: view
	
	So we can apply them as follows - 
	
		kubectl apply -f 02-rbac.yaml
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl apply -f 02-rbac.yml
	clusterrolebinding.rbac.authorization.k8s.io/fabric8-rbac unchanged

	As soon as we do this and access the url, it will work and error will be gone. 
	
	http://34.98.116.11/currency-conversion/from/USD/to/INR/quantity/10
	
	Also when we keep refreshing we can see load distribution is happening automatically.
	
	In this step we created a ClusterRoleBinding which enables Ribbon to be able to talk to the k8s discovery apis.
	We had to create a cluster role binding mapping our default service account which was already existing, to a clusterRole with access of view to a api group.
	
	
Step 04 - Using Spring Cloud Kubernetes Config to load ConfigMaps


	In the previous step we saw how spring cloud kubernetes actually helps in client side load balancing through Ribbon. 
	Ribbon was automatically able to talk to k8s service discovery system and distribute the load automatically.
	
	So one of the use of adding spring-cloud-starter-kubernetes-all was that it helped in client side load balancing using Ribbon. 
	Lets look at other features it brings in - 
	
	One of the important feature that spring cloud k8s brings in is integration with config maps. 
	So if we create a config map with the name that is in application.properties - spring.application.name=currency-conversion
	then spring cloud k8s can talk to k8s using the apis and get the information which is present inside the config maps and load it up at the application start up.
	
	Earlierr we use to pick up config map value as follow - 

		- env:
			- name: RDS_DB_NAME
			  valueFrom:
				configMapKeyRef:
				  key: RDS_DB_NAME
				  name: todo-web-application-config
	
	
	So to demonstrate it we will create a config map with the name matching with that in application.properties
	
		kubectl create configmap currency-conversion --from-literal=YOUR_PROPERTY=value --from-literal=YOUR_PROPERTY_2=value2
	
	We are creating a configmap in default namespace.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl create configmap currency-conversion --from-literal=YOUR_PROPERTY=value --from-literal=YOUR_PROPERTY_2=value2
	configmap/currency-conversion created
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl describe configmap/currency-conversion
	Name:         currency-conversion
	Namespace:    default
	Labels:       <none>
	Annotations:  <none>

	Data
	====
	YOUR_PROPERTY_2:
	----
	value2
	YOUR_PROPERTY:
	----
	value
	Events:  <none>
	
	
	If we check the logs of the pod, and search for configmap, then we can see that at the start of the application when it was deploying it, it tried to load a composite-config map from a propertySource with name configmap.currency-conversion. 
	It also tries to load secrets as well, secrets.currency-conversion
	So what happens is when the application is loading up, once we added spring cloud k8s, it bring in something called Spring cloud kubernetes config which integrates with the configmap and the secret system which is available in k8s. 
	So at the start up its trying to load all configuration up. 
	
	Now to use the new configuration we need to restart the application - 
	
		kubectl scale deployment currency-conversion --replicas=0
		
		kubectl scale deployment currency-conversion --replicas=1
		
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl scale deployment currency-conversion --replicas=0
	deployment.extensions/currency-conversion scaled

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\06-currency-conversion-microservice-cloud>kubectl scale deployment currency-conversion --replicas=1
	deployment.extensions/currency-conversion scaled

	
	Now, we can check if configuiration is loading up by again checking the logs of new pod.
	
	  .   ____          _            __ _ _
	 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
	( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
	 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
	  '  |____| .__|_| |_|_| |_\__, | / / / /
	 =========|_|==============|___/=/_/_/_/
	 :: Spring Boot ::        (v2.1.7.RELEASE)

	2020-04-03 06:30:18.730  INFO [currency-conversion,,,] 1 --- [           main] b.c.PropertySourceBootstrapConfiguration : Located property source: CompositePropertySource {name='composite-configmap', propertySources=[ConfigMapPropertySource {name='configmap.currency-conversion.default'}]}
	2020-04-03 06:30:18.734  INFO [currency-conversion,,,] 1 --- [           main] b.c.PropertySourceBootstrapConfiguration : 
	
	.....
	.....
	
	100-exec-1] i.m.c.u.e.EnvironmentConfigurationLogger : YOUR_PROPERTY_2 - value2
	2020-04-03 06:30:46.659  INFO [currency-conversion,5cf69334b6d6d041,5cf69334b6d6d041,false] 1 --- [nio-8100-exec-1] i.m.c.u.e.EnvironmentConfigurationLogger : YOUR_PROPERTY - value
	
	So we access all the properties that are configured in configmap without tying it up with application at all. 
	
	
Step 05 - Understanding Auto Scaling approaches with Kubernetes

	Until now we were manually scaling up the pods. We did scale and set replicas to 0 or 1, or we ser replica value in deployment.yaml file.
	This is manual scaling, we decided how many instances we want running at a particular time. 
	
	So manual scaling is not always good, we don't want to get a call in middle of night saying you are running out of resources and you need to scale up your application. 
	This is where we configure auto scaling. 
	And k8s supports auto scaling at multiple levels. 
	
	Check the diagram - 
	
	So we know We have a node and in that node number of pods are running and inside pods we have number of containers running. 
	And we can have number of such nodes in a k8S cluster and these node can be distributed across different regions or zones. 
	
	Different options we have to scale - 
	
		- Increase the number of nodes based on the load. This is basically called Cluster auto scaling.
		
		- Increase the number of pods. Lets say we have a node, where resources are not fully utilized, there are just two pods running and there is space to run more pods. So we can increase the number of pods when there is high demand. 
		This is called horizontal pod auto scaling. 
		Basically we are increasing the number of instances, number of pods that are available for a specific application. 
		
		So lets say we have one pod for CES and there is high load, we increase the number of pods to 2 or 3. 
		
		Obviously we can increase number of pods, if we have sufficient amount of resources available on the node or in another node in cluster. 
		
		- Vertical pod auto scaling. 
			
		Each of the pod we create with resource limits, we say this pod should use this much amount of cpu and this much memory. 
		Vertical auto scaling is all about increasing the number of resources available for a specific pod. 
		So we can say if the amount of cpu or if the amount of memory that is allocated to a pod is not sufficient, increase the amount of cpu, increase the amount of memory allocated to that pod.
		Thats basically increasing the size of the pod. 
		
		
	Inside CES-basic project we can find small documentation on auto scaling =>
		
		## Auto Scaling

		### Cluster Auto Scaling - we need to enable the cluster auto scaling on google cloud when we are creating our cluster. So below is the example command to create a cluster and we have specified that we want to do auto scaling and we want minimum of 1 node and maximum of 4 nodes available. And we have set default number of node to 2.   

		```
		gcloud container clusters create example-cluster \
		--zone us-central1-a \
		--node-locations us-central1-a,us-central1-b,us-central1-f \
		--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4
		```
		
		### Vertical Pod Auto Scaling
		- Vertical pod auto scaling is only available in version 1.14.7-gke.10 or higher and in 1.15.4-gke.15 or higher
		
		There are two ways to enable vertical auto scaling - 
			- when cluster is being created.
			
		#### Enable on Cluster

		```
		gcloud container clusters create [CLUSTER_NAME] --enable-vertical-pod-autoscaling --cluster-version=1.14.7
		
		If we have an existing cluster we can also update it to enable vertical pod auto scaling =>
		
		gcloud container clusters update [CLUSTER-NAME] --enable-vertical-pod-autoscaling
		```
		
		Just enabling is not sufficient, so we need to create a k8s resource called Vertical Pod Autoscaler VPA =>
		
		#### Configure VPA

		```
		apiVersion: autoscaling.k8s.io/v1
		kind: VerticalPodAutoscaler
		metadata:
		  name: currency-exchange-vpa
		spec:
		  targetRef:
			apiVersion: "apps/v1"
			kind:       Deployment
			name:       currency-exchange
		  updatePolicy:
			updateMode: "Off"
		```

		So we created a VerticalPodAutoscaler resource tying up our VerticalPodAutoscaler to a Deployment. What it does is, it keeps monitoring the deployment. 
		We have kept - updateMode: "Off"
		One of the options VerticalPodAutoscaler provides is to not directly update but to provide recommendations. 
		So lets say we are running the application inside the pod, VerticalPodAutoscaler can monitor it and suggest that right amount of cpu for your application is this, right amount og memory is this, etc. 
		And if we want to get recommendations we can set updateMode to false. 
		So instead of directly scaling up and down pod, it provides recommendations. 
		We can see recommendations using below command - 

		#### Get Recommendations

		```
		kubectl get vpa currency-exchange-vpa --output yaml
		```
		
		We can then use these recommendations to manually update the cluster. 
		
		To enable VPA to automatically update pods, updateMode: "Auto".

		
		### Horizontal Pod Auto Scaling

		```
		kubectl autoscale deployment currency-exchange-service --max=3 --min=1 --cpu-percent=50
		```
	
	
Step 06 - Implementing Horizontal Pod Autoscaling with
Kubernetes	
	
	We would want to increase the pods based on number of requests coming in or based on the load on the application. 
	We will use CES-basic to demo this.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl get pods
	NAME                                   READY   STATUS    RESTARTS   AGE
	currency-conversion-56fd8454f7-94vzk   1/1     Running   0          128m
	currency-exchange-7677985bdc-n6tnd     1/1     Running   0          3d2h
	currency-exchange-7677985bdc-tm2vr     1/1     Running   0          3d2h
	
	We change the replicas to 1, set the cpu value in deployment.yaml
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl apply -f deployment.yaml
	deployment.extensions/currency-exchange configured
	service/currency-exchange unchanged
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl get pods
	NAME                                   READY   STATUS    RESTARTS   AGE
	currency-conversion-56fd8454f7-94vzk   1/1     Running   0          133m
	currency-exchange-6b7c676f87-4gv4s     0/1     Running   2          2m59s
	currency-exchange-7677985bdc-n6tnd     1/1     Running   0          3d2h
	
	
	We can see that new CES pod is there. 
	
	kubectl get pods => he observed that the pods was getting restarted again and again. When he followed the logs, he saw the app start was very slow.
	
	The cpu limits which we set up is so low that application is not even starting up. So we set cpu to start with 100 and then limit it till 500
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl apply -f deployment.yaml
	deployment.extensions/currency-exchange configured
	service/currency-exchange unchanged
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl get pods
	NAME                                   READY   STATUS    RESTARTS   AGE
	currency-conversion-56fd8454f7-94vzk   1/1     Running   0          138m
	currency-exchange-7677985bdc-n6tnd     1/1     Running   0          3d2h	
	
	
	Now pod has started, as we had only 1 replica we have one pod. 
	
	Now we want to find out, how much cpu is being made use of =>
	
		kubectl top pods
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl top pods
	NAME                                   CPU(cores)   MEMORY(bytes)
	currency-conversion-56fd8454f7-94vzk   4m           251Mi
	currency-exchange-7677985bdc-n6tnd     2m           224Mi
	
	If we do => watch -n 0.1 curl http://34.98.116.11/currency-exchange/from/USD/to
/INR      

	which will call the CES every 0.1 second we can see that CPU utilization has increased.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl top pods
	NAME                                   CPU(cores)   MEMORY(bytes)
	currency-conversion-56fd8454f7-94vzk   2m           251Mi
	currency-exchange-7677985bdc-n6tnd     46m          231Mi
	
		
	So we want to configure auto scaling based on the load. 
	So in production environment we can set that if a cpu utilization is greater than 70% I would want to bring up a new pod. 
	
	For now for demo purpose we will set it to low value. 
	
		kubectl autoscale deployment currency-exchange --min=1 --max=3 --cpu-percent=10
	
		Autoscale if cpu-percent is 10
		In production it might be 70 something.
		
	(Above we are doing it imperative way i.e. using above command to create HorizontalPodAutoscaler, instead we can do it in declarative way as well i.e. using yaml file for HorizontalPodAutoscaler. Below we will see.)
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl autoscale deployment currency-exchange --min=1 --max=3 --cpu-percent=10
	horizontalpodautoscaler.autoscaling/currency-exchange autoscaled	
	
	We can see that a horizontal pod autoscaler has been created for us. 
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl get pods
	NAME                                   READY   STATUS    RESTARTS   AGE
	currency-conversion-56fd8454f7-94vzk   1/1     Running   0          151m
	currency-exchange-7677985bdc-mhgl5     1/1     Running   0          66s
	currency-exchange-7677985bdc-n6tnd     1/1     Running   0          3d3h
	currency-exchange-7677985bdc-s7vsd     1/1     Running   0          66s
	
	
	We can see that 2 new pods has been created as cpu utilization is above 10%

	If we do kubectl get events we can see some logs like - 
	
	3m56s       Normal    SuccessfulRescale   horizontalpodautoscaler/currency-exchange   New size: 3; reason: cpu resource utilization (percentage of request) above target
	3m56s       Normal    ScalingReplicaSet   deployment/currency-exchange                Scaled up replica set currency-exchange-7677985bdc to 3
	
	In backgroung HorizontalPodAutoscaler is scaling up the replicaset.
	
	
	What we are doing is kind of an imperative style where we are actually executing a command, if you want to properly do this, you would want to do it in declarative style.
	
	kubectl get hpa (Horizontal pod autoscaler)
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\04-currency-exchange-microservice-basic>kubectl get hpa
	NAME                REFERENCE                      TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
	currency-exchange   Deployment/currency-exchange   18%/10%   1         3         3          9m29s
	
	We can see target where current utilization is 18% and we set autoscale to 10%
	And we have 3 replica active right now. 
	
	We would want to export it.
	
		kubectl get hpa -o yaml > 01-hpa.yaml
	
		This will give below things in 01-hpa.yaml file, but it will give list of hpa, we don't want list, we want specific hpa so use below command,
		
		kubectl get hpa currency-exchange -o yaml > 01-hpa.yaml
		
	apiVersion: autoscaling/v1
	kind: HorizontalPodAutoscaler
	metadata:
	  annotations:
		autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","lastTransitionTime":"2020-04-03T08:59:57Z","reason":"ScaleDownStabilized","message":"recent
		  recommendations were higher than current one, applying the highest recent recommendation"},{"type":"ScalingActive","status":"True","lastTransitionTime":"2020-04-03T08:59:57Z","reason":"ValidMetricFound","message":"the
		  HPA was able to successfully calculate a replica count from cpu resource utilization
		  (percentage of request)"},{"type":"ScalingLimited","status":"True","lastTransitionTime":"2020-04-03T09:00:58Z","reason":"TooManyReplicas","message":"the
		  desired replica count is more than the maximum replica count"}]'
		autoscaling.alpha.kubernetes.io/current-metrics: '[{"type":"Resource","resource":{"name":"cpu","currentAverageUtilization":8,"currentAverageValue":"8m"}}]'
	  creationTimestamp: "2020-04-03T08:59:42Z"
	  name: currency-exchange
	  namespace: default
	  resourceVersion: "5478270"
	  selfLink: /apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/currency-exchange
	  uid: 7531b24c-7589-11ea-8fef-42010a8000b5
	spec:
	  maxReplicas: 3
	  minReplicas: 1
	  scaleTargetRef:
		apiVersion: extensions/v1beta1
		kind: Deployment
		name: currency-exchange
	  targetCPUUtilizationPercentage: 10
	status:
	  currentCPUUtilizationPercentage: 8
	  currentReplicas: 3
	  desiredReplicas: 3
	  lastScaleTime: "2020-04-03T08:59:57Z"


	We can delete few things like annotation, creationTimestamp, status, etc. => 
	
	
	apiVersion: autoscaling/v1
	kind: HorizontalPodAutoscaler
	metadata:
	  name: currency-exchange
	  namespace: default
	spec:
	  maxReplicas: 3
	  minReplicas: 1
	  scaleTargetRef:
		apiVersion: extensions/v1beta1
		kind: Deployment
		name: currency-exchange
	  targetCPUUtilizationPercentage: 10

	
	So we can see that it is very easy to configure a horizontal pod autoscaler even in a declarative way.
	We can store this in our configuration and apply it wherever we want. 
	
	
Step 07 - Deleting Your Kubernetes Cluster

	Once we create a k8s cluster we are charged for it, whether we deploy the application to it or not. 
	
	So it not a good practice to keep our clusters up and running all the time. 
	
	To keep the cost as low as possible we need to keep deleting the clusters that we don't make use of. 
	
	In next steps we will create cluster for stack driver and istio and this current cluster we will not need it for a while, so we will delete it. 
	
	Just go in UI and delete, you will also not see load balancer after this. 
	
	
Section 9: GKE - Integrating Spring Boot Microservices -
Kubernetes with Google Stackdriver


Step 01 - Creating New Cluster to Play With Google Stackdriver

	Distributed Tracing will help in tracing the request accross the microservices. 
	
	We can go to one place to find out what is going on with a specific request. 
	
	In google cloud one of the popular option is to use StackDriver. 
	
	StackDriver provides a number of options, amongst which the important one are looking at the log and also trace the request accross multiple microservices.
	
	How to integrate StackDriver with k8s and with our microservices CES and CCS. 
	

Step 02 - Review Spring Boot Microservices integrated with Google Stackdriver

	Import project 07 and 08 for StackDriver.
	
	Changes made in these projects are - 
	
	- removed dependencies on spring-cloud-starter-kubernetes-all and added below dependencies - 
	
		<dependency> <!-- CHANGE -->
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-gcp-starter-trace</artifactId>
		</dependency>

		<dependency> <!-- CHANGE -->
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring-cloud-gcp-starter-logging</artifactId>
		</dependency>
	
	
	So we want to integrate with stackdriver and we want to see trace and logging inside stackdriver so we added above dependencies in.
	
	Another change is in dockerfile, Earlierr we were using openjdk:8-jdk-alpine now we are using openjdk:8, openjdk:8-jdk-alpine is smaller in size than openjdk:8. 
	But there are some features of openjdk:8 which are not available in openjdk:8-jdk-alpine which stackdriver makes use of and therefore we need to use openjdk:8 as base image in our container.
	
	Other changes are in application.properties =>
	
	spring.sleuth.sampler.probability=1.0 
	
		We would want to trace request and this decides how much percentage of request we would want to trace, typically in production we don't want to trace all the request because tracing puts a lot of load on infrastructure. 
		In production reduce sampling-rate to 0.01
			
	
	spring.cloud.gcp.trace.enabled=false

		We are disabling gcp trace. 
		You might be wondering that we added in spring-cloud-gcp-starter-trace dependency and we are saying enable equal to false.
		This is because, you would want to be able to run your project on local, if you leave your stackdriver trace enabled and run it on your local it would actually try to talk to stackdriver and the project would not run at all. 
		That's why in our local we are disabling the connection with stackdriver trace. 
		And how we are enabling it when we deploy the application? We are enabling it in our deployment.yaml file.
		In deployment.yaml file we are configuring enviornment variable - 
			
			env: #CHANGE
			- name: SPRING_CLOUD_GCP_TRACE_ENABLED
			  value: "true"
	
		So when the application is deployed the trace will be enabled, when application is running on local outside k8s enviornment, you would be able to run it without tracing enabled.
		
	Other change we made is changed the type of service in deployment.yaml to LoadBalancer - 
	
		type: LoadBalancer

	As we have createa new cluster and Ingress will take time, so made this change. 
	
	Other change is we are using release 2.0.0 
	
	Same changes are there in both 07 and 08.
	
	Lastly he build both the projects and pushed images to docker hub.
	

Step 03 - Enabling Stackdriver APIs for Your Google Cloud Account

	One of the important thing with StackDriver API is that you need to enable all StackDriver things on Google Cloud for you specific id.
	So for your account you need to enable all StackDriver apis.
	
	Search APIs and then go to APIs & Services. Then click on Enable APIs button.
	then search stackdriver - select StackDriver API and click on enable button. 
	Then search stackdriver - select StackDriver Monitoring api, enable it. It might be already enabled.
	Then same way enable StackDriver Logging API, StackDriver Trace API, StackDriver Error Reporting API. 
	

Step 04 - Deploy Spring Boot Microservices and Explore Google
Stackdriver
	

	In last step we enabled StackDriver APIs and now we will deploy the application to our new cluster. 
	
	Connect to new cluster first.
	
	Do - 
		kubectl apply -f deployment.yaml
	
	for both project 07, 08 
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\07-currency-exchange-microservice-stackdriver>kubectl get all
	NAME                                      READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-5948c55d6-nhlqp   1/1     Running   0          111s
	pod/currency-exchange-794f6c7c79-kw78k    1/1     Running   0          65s


	NAME                          TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE
	service/currency-conversion   LoadBalancer   10.0.7.201   34.67.33.189   8100:32034/TCP   110s
	service/currency-exchange     LoadBalancer   10.0.9.56    34.66.121.72   8000:32724/TCP   65s
	service/kubernetes            ClusterIP      10.0.0.1     <none>         443/TCP          2d16h


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   1/1     1            1           112s
	deployment.apps/currency-exchange     1/1     1            1           66s

	NAME                                            DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-5948c55d6   1         1         1       113s
	replicaset.apps/currency-exchange-794f6c7c79    1         1         1       67s

	
	The APIs that we have enabled, trace APIs, Logging APIs, etc, there are a lot of limits around those APIs. So don't fire 1000s of requests, if we fire more than 3000 request we will see that our tracing is disabled. So while we are learning we will stay in that limit. We can actually remove these limits when we go into production enviornment. 

	We can check in browser both the application are up and running.
	
	Now we can search StackDriver in Google Cloud Platform and go to StackDriver and we can see the monitoring there. 
	We can see StackDriver provides a lot of features. 
	Resource -> Kubernetes Engine, we can see detailed cluster details, nodes details, pods, container, how much cpu, memory is being utilized, etc. 
	Trace - we can see lot more details, method name, controller class name, how much time it took, flow, etc.

	Great thing about this is that, we didn't had to do a lot of things to get all this, we just had to enable couple of APIs and add few dependencies and thats all.
	
	In this step we got our StackDriver Trace working. 
	

71. Step 05 - Exploring Google Stackdriver Error Reporting and Logging

	
	Wer will look at other features of StackDriver like, Error Reporting, Logging, etc.
	
	In Error Reporting we can see all the errors that were logged in. We can keep monitoring it and also Turn on notification and get a email, etc. 
	
	When we click on a particular error instance we can see entire stack trace of that particular instance. We can click raw and see the stack trace as it was. 
	
	So this provides us a single consolidated view of all the exceptions that occured. 
	
	So it tells us like this is a exception and it occured this many times and it happening frequently, lets get to the root cause of it.
	
	We can change the error to Open, Acknowledged, Resolved or Muted. 
	
	
	In Logging we can see all the logs of all the different application that are present. 
	We can Click on Logging.
	Another way to open Logging to to go in WorkLoads in Kubernetes Engine and then select the WorkLoad and then select Container Logs. This will select few filters which are really helpful. We will be looking at the logs from a specific pod.
	
		resource.type="k8s_container"
		resource.labels.project_id="boreal-gravity-271305"
		resource.labels.location="us-central1-c"
		resource.labels.cluster_name="in28minutes-cluster-stackdriver"
		resource.labels.namespace_name="default"
		labels.k8s-pod/app="currency-conversion"

	If we want to see logs from all the microservices we can update the filter, like remove - labels.k8s-pod/app="currency-conversion"
	
	To trace the request accross mutiple microservices we can copy the id which we see in the log, then we will select option - clear the filter and return to basic mode, then in drop down select Kubernetes Container -> Our Cluster name -> default, here we have all our microservices.
	And once this is selected we can paste the id we copied in Filter By label or text box.
	Now all the logs which are related to that id will be shown to us. 
	
	The id comes because of dependency we added spring-cloud-sleuth, it allows us to trace request across multiple microservices.
	
	[currency-conversion,5e8ab7f6863dcef7ae768c82d39cee92,ae768c82d39cee92,true], here 2nd parameter i.e 5e... is called trace id. It will remain same across all the logs we are seeing. 3rd parameter i.e. ae7... is called span id, when a request goes from one microservices to another this span id will change. 
	
	
Section 10: GKE - Integrating Java Spring Boot Microservices on
Kubernetes with Istio


Step 01 - Creating new Kubernetes Cluster for Playing with Istio


	Istio is something called as Service Mesh.
	That means there will be another container that will be sitting besides your container in the same pod, thus we will need little bit more powerful cluster. 
	So we will create a more powerful cluster.
	We deleted all Earlierr clusters.
	
	We are creating 2 nodes and we changed the Series from N1 to N2(more powerful)
	We will not select istio option of google cloud while creating the cluster, we will create istio on our own later.
	
	We have istio scripts - 11 - to setup istio and we also have a readme.md where we have steps to set up istio. 
	
	To understand Istio we need to understand the concept called Service Mesh.
	Check Kubernetes Architechture diagram.
	
	Until now we just had one container present inside our pod. So we had CES or CCS microservice inside the pod.
	We never took advantage that the second container can be deployed on the pod. 
	
	One thing we might be thinking is why not deploy multiple microservices inside the same pod. What would happen if we had two microservices inside same pod?
	Then they are scaled together, whenver we are increasing the instances of microservice-1 we are also increasing the instances of microservice-2.
	So if we have microservices which have same amount of load, then its fine. 
	Also resources which microservice-1 needs might be different from the one which microservice-2 needs. 
	And that's the reason why we will put the each microservice in there own pods. 
	
	So what can the second container be? 
	Each of thee microservices which are deployed to each of the pods, will have a lot of common functionality. They need Service Discovery, Logging, Distributed Tracing, Release Automation in the sense that you may automate release strategy.
	So there are a lot of common features that are present for all these containers. 
	Why do we want to build that functionality into each of these containers. 
	How about having a separete Service Mesh container along with each these containers, which takes care of all the common features that are needed by all the microservices. 
	And what if all the requests get routed through this specific second container. 
	That's where concept of Service Mesh container comes into picture.
	Istio is one of the most popular Service Mesh implementations. 
	
	We would deploy another container with along with our microservices CES and CCS. And you would see all the traffic would first go to the istio and then it would go to the container which has the microservice.
	
	What is the advantage of this?
	Lets say we want to implement logging, distributed tracing, then we can always implement it at one container.
	If we have that container present in each one of the pods then that container would be taking care of all the common features. 
	
	Service Mesh, Istio, Side Car - 
	The container which we are putting right besides every microservice is also called a Side Car. 
	The concept is called Service Mesh 
	And Istio is one of the most popular implementations of Service Mesh.
	

Step 03 - Installing Istio on Kubernetes Cluster - CRD and Istio
Components

	Connect to the cluster.
	
	Launch up cloud shell to run few scripts. 
	
	Check readme.md => 
	
		gcloud container clusters get-credentials standard-cluster-1 --zone=europe-west1-b
		kubectl create namespace istio-system
		curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.2 sh -
		cd istio-1.2.2
		for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
		helm template install/kubernetes/helm/istio --name istio --set global.mtls.enabled=false --set tracing.enabled=true --set kiali.enabled=true --set grafana.enabled=true --namespace istio-system > istio.yaml
		kubectl apply -f istio.yaml

		kubectl label namespace default istio-injection=enabled
	
	We are now connected to the cluster, now we need to create a namespace called istio-system and then install all the istio related stuff to this specific namespace.
		
		gcloud container clusters get-credentials standard-cluster-1 --zone=europe-west1-b
				
				connect to cluster
		
		kubectl create namespace istio-system
		
		curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.2 sh -
			
			will download istio related stuff for specific version to a folder on google cloud shell. 
		
		cd istio-1.2.2
			
		for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
			
			we are installing all the download istio related stuff
			
		helm template install/kubernetes/helm/istio --name istio --set global.mtls.enabled=false --set tracing.enabled=true --set kiali.enabled=true --set grafana.enabled=true --namespace istio-system > istio.yaml

			with istio we want to actually do tracing, look at analytics, look at different release strategies, etc. And to be able to do this we need to install few templates. We use utility called helm. We will use pre-created helm template which will help to add these features on istio. So we will use template like that, run it and then create a istio.yaml file, which is the output of the above command. And we will run it using below command. 
			istio.yaml will contain all the configuration for us, to be able to add feature that we want on to istio.
			
		kubectl apply -f istio.yaml
		
		kubectl label namespace default istio-injection=enabled
			
			This will enable injection of istio. 
	
	
	At high level we are installing the components of istio and enabling the features that we want - tracing, kiali, grafana, etc.

	We will now execute commands one by one in cloud shell.
	
	
	aniketrajput90@cloudshell:~$ gcloud container clusters get-credentials in28minutes-cluster-istio --zone us-central1-c --project boreal-gravity-271305
	Fetching cluster endpoint and auth data.
	kubeconfig entry generated for in28minutes-cluster-istio.
	
	aniketrajput90@cloudshell:~$ kubectl create namespace istio-system
	namespace/istio-system created
	
	aniketrajput90@cloudshell:~$ curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.2 sh -
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
									 Dload  Upload   Total   Spent    Left  Speed
	  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0
	100  3015  100  3015    0     0   2914      0  0:00:01  0:00:01 --:--:--  2914
	Downloading istio-1.2.2 from https://github.com/istio/istio/releases/download/1.2.2/istio-1.2.2-linux.tar.gz ...  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
									 Dload  Upload   Total   Spent    Left  Speed
	100   614  100   614    0     0    619      0 --:--:-- --:--:-- --:--:--   618
	100 20.2M  100 20.2M    0     0  3958k      0  0:00:05  0:00:05 --:--:-- 6439k
	Istio 1.2.2 Download Complete!
	Istio has been successfully downloaded into the istio-1.2.2 folder on your system.
	Next Steps:
	See https://istio.io/docs/setup/kubernetes/install/ to add Istio to your Kubernetes cluster.
	To configure the istioctl client tool for your workstation,
	add the /home/aniketrajput90/istio-1.2.2/bin directory to your environment path variable with:
			 export PATH="$PATH:/home/aniketrajput90/istio-1.2.2/bin"
	Begin the Istio pre-installation verification check by running:
			 istioctl verify-install
	Need more information? Visit https://istio.io/docs/setup/kubernetes/install/
	
	
	aniketrajput90@cloudshell:~$ ls
	istio-1.2.2  README-cloudshell.txt
	
	Step 1 - Install all the istio Custom Resource Definitions(CRD)
	
	aniketrajput90@cloudshell:~$ ls istio-1.2.2/install/kubernetes/helm/istio-init/files/crd*yaml
	istio-1.2.2/install/kubernetes/helm/istio-init/files/crd-10.yaml  istio-1.2.2/install/kubernetes/helm/istio-init/files/crd-certmanager-10.yaml
	istio-1.2.2/install/kubernetes/helm/istio-init/files/crd-11.yaml  istio-1.2.2/install/kubernetes/helm/istio-init/files/crd-certmanager-11.yaml
	istio-1.2.2/install/kubernetes/helm/istio-init/files/crd-12.yaml
	
	aniketrajput90@cloudshell:~$ cd istio-1.2.2
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
	customresourcedefinition.apiextensions.k8s.io/virtualservices.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/destinationrules.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/serviceentries.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/gateways.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/envoyfilters.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/clusterrbacconfigs.rbac.istio.io created
	customresourcedefinition.apiextensions.k8s.io/policies.authentication.istio.io created
	customresourcedefinition.apiextensions.k8s.io/meshpolicies.authentication.istio.io created
	customresourcedefinition.apiextensions.k8s.io/httpapispecbindings.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/httpapispecs.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/quotaspecbindings.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/quotaspecs.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/rules.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/attributemanifests.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/rbacconfigs.rbac.istio.io created
	customresourcedefinition.apiextensions.k8s.io/serviceroles.rbac.istio.io created
	customresourcedefinition.apiextensions.k8s.io/servicerolebindings.rbac.istio.io created
	customresourcedefinition.apiextensions.k8s.io/adapters.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/instances.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/templates.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/handlers.config.istio.io created
	customresourcedefinition.apiextensions.k8s.io/sidecars.networking.istio.io created
	customresourcedefinition.apiextensions.k8s.io/authorizationpolicies.rbac.istio.io created
	customresourcedefinition.apiextensions.k8s.io/clusterissuers.certmanager.k8s.io created
	customresourcedefinition.apiextensions.k8s.io/issuers.certmanager.k8s.io created
	customresourcedefinition.apiextensions.k8s.io/certificates.certmanager.k8s.io created
	customresourcedefinition.apiextensions.k8s.io/orders.certmanager.k8s.io created
	customresourcedefinition.apiextensions.k8s.io/challenges.certmanager.k8s.io created
	
	Step-2 Install Istio with Components for grafana, prometheus, tracing(jaeger), kiali, as subcharts using Helm Charts.
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ helm template install/kubernetes/helm/istio --name istio --set global.mtls.enabled=false --set tracing.enabled=true --set kiali.enabled=true --set grafana.enabled=true --namespace istio-system > istio.yaml
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ ls
	bin  install  istio.VERSION  istio.yaml  LICENSE  README.md  samples  tools
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ kubectl apply -f istio.yaml
			......
			......
			......
			
	
Step 04 - Review Istio Installation and Deploy Spring Boot App with Side Cars

	
	Step 3 - Verify Installation of Istio
	
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ kubectl get pods  --namespace istio-system
	NAME                                      READY   STATUS      RESTARTS   AGE
	grafana-6fb9f8c5c7-7bp4l                  1/1     Running     0          7m56s
	istio-citadel-68c85b6684-d7p9t            1/1     Running     0          7m52s
	istio-cleanup-secrets-1.2.2-5d2vt         0/1     Completed   0          8m17s
	istio-galley-77d697957f-plkwl             1/1     Running     0          7m57s
	istio-grafana-post-install-1.2.2-4bd8s    0/1     Completed   0          8m21s
	istio-ingressgateway-8b858ff84-knrlw      1/1     Running     0          7m56s
	istio-pilot-5544b58bb6-v2lfs              2/2     Running     0          7m53s
	istio-policy-5f9cf6df57-6pq6t             2/2     Running     2          7m54s
	istio-security-post-install-1.2.2-mz74g   0/1     Completed   0          8m14s
	istio-sidecar-injector-66549495d8-s2tng   1/1     Running     0          7m51s
	istio-telemetry-7749c6d54f-zkt6m          2/2     Running     3          7m54s
	istio-tracing-5d8f57c8ff-l62v2            1/1     Running     0          7m51s
	kiali-7d749f9dcb-bsc2d                    1/1     Running     0          7m55s
	prometheus-776fdf7479-bgchz               1/1     Running     0          7m53s
	
	
	aniketrajput90@cloudshell:~/istio-1.2.2$ kubectl get services  --namespace istio-system
	NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                                                                                                                                      AGE
	grafana                  ClusterIP      10.64.15.147   <none>           3000/TCP                                                                                                                                     9m13s
	istio-citadel            ClusterIP      10.64.2.213    <none>           8060/TCP,15014/TCP                                                                                                                           9m9s
	istio-galley             ClusterIP      10.64.12.109   <none>           443/TCP,15014/TCP,9901/TCP                                                                                                                   9m14s
	istio-ingressgateway     LoadBalancer   10.64.10.111   35.223.166.225   15020:32764/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32235/TCP,15030:31048/TCP,15031:32735/TCP,15032:30092/TCP,15443:32417/TCP   9m14s
	istio-pilot              ClusterIP      10.64.4.5      <none>           15010/TCP,15011/TCP,8080/TCP,15014/TCP                                                                                                       9m10s
	istio-policy             ClusterIP      10.64.15.24    <none>           9091/TCP,15004/TCP,15014/TCP                                                                                                                 9m12s
	istio-sidecar-injector   ClusterIP      10.64.7.47     <none>           443/TCP                                                                                                                                      9m9s
	istio-telemetry          ClusterIP      10.64.4.33     <none>           9091/TCP,15004/TCP,15014/TCP,42422/TCP                                                                                                       9m11s
	jaeger-agent             ClusterIP      None           <none>           5775/UDP,6831/UDP,6832/UDP                                                                                                                   8m57s
	jaeger-collector         ClusterIP      10.64.7.236    <none>           14267/TCP,14268/TCP                                                                                                                          8m58s
	jaeger-query             ClusterIP      10.64.7.43     <none>           16686/TCP                                                                                                                                    8m59s
	kiali                    ClusterIP      10.64.3.144    <none>           20001/TCP                                                                                                                                    9m12s
	prometheus               ClusterIP      10.64.12.197   <none>           9090/TCP                                                                                                                                     9m10s
	tracing                  ClusterIP      10.64.8.203    <none>           80/TCP                                                                                                                                       8m56s
	zipkin                   ClusterIP      10.64.6.238    <none>           9411/TCP                                                                                                                                     8m57s

	
	Connect to cluster from local, cd to 11-istio-scripts-and-configuration
	For now scripts important to use are - 
		01-helloworld-deployment.yaml
		02-creating-http-gateway.yaml
		03-creating-virtualservice-external.yaml
		
	01-helloworld-deployment.yaml is a typical deployment script for 01-hello-world-rest-api application. 
	here we are using service of deault type, which is ClusterIP
	
		spec:
			#type: LoadBalancer
	
	So we don't want this service to be exposed to the outside. 
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\11-istio-scripts-and-configuration>kubectl apply -f 01-helloworld-deployment.yaml
	deployment.apps/hello-world-rest-api created
	service/hello-world-rest-api created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\11-istio-scripts-and-configuration>kubectl get pods
	NAME                                    READY   STATUS    RESTARTS   AGE
	hello-world-rest-api-68586d6ff4-c52bq   1/1     Running   0          94s

	Here READY is 1/1, there is only one container, actually there should be 2 containers, this means istio is not being deployed here. 
	To fix this we need to execute below command - 
	
		Step 4 - Enable adding Istio Side Cars in each Pod
				Enable Istio Injection. 
				
				kubectl label namespace default istio-injection=enabled
				
				So we are saying, in the default namespace, for every pod that is created, inject istio.
				
	Now again do the deployment. 
	
	When we are having 2 containers in a pod and we want to follow logs for a container then we need to give the container name, Earlierr we just use to give pod name. Just giving the pod name will result in error - 
	
		kubectl logs pods-name hello-world-rest-api -f
		
	
	Problem he was facing is his pods were in pending status, he did describe and found out that its failing because there are no free ports. This can be resolved by increaing the number of nodes to 3.
	
	At the end of this step we will not be able to access the hello-world-rest-api from outside, how to do that we will see next.
	

Step 05 - Increasing Kubernetes Cluster Size to 3 Nodes

	Select the cluster -> default-pool -> edit button -> change to 3 -> save
	

Step 06 - Understanding Gateways and Virtual Services for Istio


	Check diagram Istio-Multiple-Services.JPG
	
	From the diagram - 
	
	We have a ClusterIP service which was created for hello-world-rest-api.
	
	k8s Service A can be any microservice like hello-world-rest-api, CES, CCS. This is being created as a ClusterIp service and we want to access it from outsite.
		
	In Earlierr step what we did was we either exposed a service as a LoadBalancer or we created a Ingress before it and Ingress use to route to these two services.
		
	With istio there are couple of more complications that we need to introduce.

	First we will create a Istio Gateway then we will have Istio Virtual Service where we configure the routes to point to each of those services. 
	
	So we will configure Istio Gateway and Istio Virtual Service for the hello-world-rest-api service now.
	
	We will see that Virtual Service helps us in adding lot of indirections. There might be two different releases for this Service A. I might want to send 50% of traffic to one release and 50% of traffic to another release. Or I would want to send 100% of the traffic to one release and 0% to another. All this kind of stuff, the configuration of Istio Virtual Service will enable us to do.
	
	We already have configuration for that available.
	
	02-creating-http-gateway.yaml
	
		apiVersion: networking.istio.io/v1alpha3
		kind: Gateway
		metadata:
		  name: http-gateway
		spec:
		  selector:
			istio: ingressgateway
		  servers:
		  - port:
			  number: 80
			  name: http
			  protocol: HTTP
			hosts:
			- "*"
		
	It is mapped to the ingressgateway, this service is already present. We can see - kubectl get services --namespace istio-system
	
	istio-ingressgateway     LoadBalancer   10.64.10.111   35.223.166.225   15020:32764/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32235/TCP,15030:31048/TCP,15031:32735/TCP,15032:30092/TCP,15443:32417/TCP   152m

	We can see it is a LoadBalancer.
	
	So we are creating a gateway on top of that and we are specifying that we want to handle all the http request to every host which comes on top of it.
	So this what gateway does.
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\09-currency-exchange-microservice-istio\11-istio-scripts-and-configuration>kubectl apply -f 02-creating-http-gateway.yaml
	gateway.networking.istio.io/http-gateway created

	
	Now we will create a Virtual Serive. Virtual Service is very similar to Ingress, however we match it to the gateway - 
	
		apiVersion: networking.istio.io/v1alpha3
		kind: VirtualService
		metadata:
		  name: helloworld-virtual-services
		spec:
		  hosts:
		  - "*"
		  gateways:
		  - http-gateway                      # 1
		  http:
		  - match:
			- uri:
				prefix: /hello-world
			route:
			- destination:
				host: hello-world-rest-api             # 2
				port:
				  number: 8080

	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\09-currency-exchange-microservice-istio\11-istio-scripts-and-configuration>kubectl apply -f 03-creating-virtualservice-external.yaml
	virtualservice.networking.istio.io/helloworld-virtual-services created

	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\09-currency-exchange-microservice-istio\11-istio-scripts-and-configuration>kubectl get svc istio-ingressgateway --namespace istio-system
	NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                                                                                                                                      AGE
	istio-ingressgateway   LoadBalancer   10.64.10.111   35.223.166.225   15020:32764/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32235/TCP,15030:31048/TCP,15031:32735/TCP,15032:30092/TCP,15443:32417/TCP   3h29m

	If we hit the url using that external ip - http://35.223.166.225/hello-world we are actualy getting ht e response. 
	
	I am confused here.. We have only one container, there is no istio deployed on another container, then how is this working?
	
	Another way to get url is to go in LoadBalancer and get the url. 
	
	In this step we had a hello-world-rest-api service, in this step we created gateway and virtual service to access that particular service. 
	

Step 07 - Basic Deployment Strategies - Rolling Updates and Recreate

	In this step we will look at the few release strategies we can use to minimize down times during our deployments. 
	Until now we saw rollingUpdates. RollingUpdate is kind of default strategy which is by default used whenever we create a deployment without a strategy. 
	With RollingUpdate we can say maxSurge and maxUnavailable. Check.
	
		strategy:
			rollingUpdate:
			  maxSurge: 25%
			  maxUnavailable: 25%
			type: RollingUpdate

	This takes long time when running in the production and other problem is for entire duration of deployment two versions are running at the same time. 
	So we have v1 and v2 both running at the sametime in production. 
	
	Lets look at the alternatives and there advantages and disadvatages.
	
	We can have the type as Recreate - 

			strategy:
			type: Recreate

	This is not a recommended strategy. This will terminate everything at one go and restart everything again.
	Due to this we will have a lot of down time. 
	
	So it will terminate everything of v1 and then there is a down time and then v2 will be up and running.

	So these two stratergies comes with default with k8s. 
	
	In next step we will see release strategies which we can use with istio. 
	Even without istio these strategies can be used but it will need lot more configuiration.
	
	
Step 08 - Setting up Multiple Versions of Spring Boot Applications


	We will see alternate release stratergies that are present. 
	We will talk about something called mirroring. 
	
	For any startergy we will need to versions of application live at the same point. 
	
	So we have a script file called - 04-helloworld-multiple-deployments.yaml
	This script file will allow us to create multiple deployment for same application.
	We have one service in it and two deployments.
	
	
Step 09 - Configure Destination Rules and Subsets to implement
Mirroring
	
	As we start playing with multiple versions our images get more complex - check diagram - Istio-Multiple-Services-version.JPG
	
	Earlier we talked about virtual service being route within multiple services, now we will talk about about multiple service versions. This is where we introduce something called as a Destination Rule. 
	
	So when we want virtual service to map to two different versions of same application, those two versions will have same route, however I should be able to switch a certain amount of percentage of traffic to version1 and reaming percentage of traffic to version2. 
	Or I might actually want to do mirroring, mirroring is - suppose v1 is live right now I would want to send the request to v1 and execute the request from v1. But I would also want to send another request to v2 as well, so from the same request two request would be created. One request to v1 where he would get the response back from and one request to v2 which we will use to test v2. If v2 is alive or not, if it is working fine or not. And then we can take v2 live after certain amount of testing.
	So this concept is called mirroring, where we have a single request being processed by two different versions of the same application. We are using it to test the new release and then releasing it.
	
	Obviously there are restrictions as to when we can use mirroring. If there is a db transaction and things like that, then mirroring will not be right scenario to use. 
	
	To be able to do mirroring, we need to update our istio virtual service to use the Destination Rules and we need to create the Destination Rules to map these versions of the service.
	
	Whenever we want to go from the virtual service to Destination Rule, we need to add a subset. So a subset of request go to this version. This concept of subset needs to be added in our virtual service.
	
	We already have a configuration with subset added in - 05-helloworld-mirroring.yaml
	
		route:
			- destination:
				host: hello-world-rest-api				#DestinationRule
				subset: v1
			  weight: 100
		mirror:
			host: hello-world-rest-api					#DestinationRule
			subset: v2

	When we add a subset we need to add a weight, we are saying that this subset of route v1 needs to get weight of 100. 
	And now once we have configured the subset, this gets routed not to the service but to the destination rule. 
	We have also added a mirror to the route. We are saying that in addition the request should also go to this DestinationRule of v2. 
	We have created a DestinationRule in the same file. 
		
	
Step 10 - Executing Blue Green and Canary Deployments with Kubernetes

	In previous step we looked at mirroring, with mirroring we routed some of the traffic which is going to the poduction version to a new version and saaw whether its working fine.
	In mirroring the user will get the response back from a original production release, he will never get the response from the mirrored release. 
	
	However some of the times, you would want two different versions live at the same time. 
	
	In case of Canary Developments, where we actually put both versions live at the same time, we would get the responses back from both the releases. 
	
	If we look at the earlier configuration 05 and 06, the only difference is that there is no mirroring in 06 and we added an additional destination. 
	We said in addition to destination hello-world-rest-api subset v1 which has weight 10% we have another one with 90%.
	So 10% of traffic would go to v1 and 90% of traffic would go to v2.
	
	route:
        - destination:
            host: hello-world-rest-api
            subset: v1
          weight: 10
        - destination:
            host: hello-world-rest-api
            subset: v2
          weight: 90
	
	
	So we can do rollingUpdates, which would take long time. Here also multiple versions will be live at the same time. 
	
	Other thing we talked about was Blue Green Deployment. We would go with this, if we want only one release live at a time. So you should either have v1 live or v2 live. So for this we would use some kind of mirroring concept. We would have all requests being handled by v1 and we would send some of the requests mirrored out to v2. Check if v2 is fine and then we would switch from v1 to v2 completely.
	
	And other option which we looked at was Canary Deployment. Canary Deployment means we start by distributing the percentage of traffic between releases. And then we can slowly make v2 completely live. With Canary Deployment we would have two release live in production at particular point in time. So we send a subset of traffic to new version to check of everything is fine and then we would switch completely to new version.
	
	Each of these deployment strategies has its own advantages and disadvatages and some of it might not be possible for some applications. 
	
	
Step 11 - Review Spring Boot Microservices integration with Istio

	
	In previous steps we were using hello-world-rest-api with istio. 
	We will now use microservices with istio, CCS and CES.
	
	The feature we will see is distributed tracing. Earlier we saw how we can implement distributed tracing with StackDriver, however StackDriver is a cloud specific solution. Its a solution specific to just Google Cloud. 
	So if we want to take our application out of Google Cloud then we need to again do modification according to new cloud. 
	This is why we should prefer cloud neutral solutions. 
	So solution we would see will be more of a cloud neutral thing. 
	
	We have added below dependencies - 
	
		<dependency> <!--CHANGE-->
			<groupId>io.opentracing.contrib</groupId>
			<artifactId>opentracing-spring-cloud-starter</artifactId>
			<version>0.1.7</version>
		</dependency>

		<dependency> <!--CHANGE-->
			<groupId>io.jaegertracing</groupId>
			<artifactId>jaeger-tracerresolver</artifactId>
			<version>0.29.0</version>
		</dependency>
	
	
	jaeger is an implementation of open tracing api. Open tracing API helps us to trace a request across multiple microservices, its an API for that. And jaeger is one of the most popular open tracing apis. 
	So we will use these two dependencies to integrate with istio.
	
	In deployment.yaml we have some configuration added around enviornment 09 project - 
	
		env:
        - name: JAEGER_SERVICE_NAME #CHANGE
          value: currency-exchange						#put value as service name which is there in your kubernetes cluster
        - name: JAEGER_AGENT_HOST
          value: jaeger-agent.istio-system.svc.cluster.local
        - name: JAEGER_REPORTER_LOG_SPANS
          value: "true"
        - name: JAEGER_SAMPLER_TYPE
          value: const
        - name: JAEGER_SAMPLER_PARAM
          value: "1"
        - name: JAEGER_PROPAGATION
          value: b3    					#these b3 headers are used to identify the requests across microservices. 
		  
	
	Same changes will be there in CCS and CES.

	Now in CCS they are using RestTemplate now, instead of feign.

	They did mvn clean install and then docker push for both.
	
	Now we would be deploying these application using istio.
	
	Do kubectl -f apply deployment.yaml for both.
	
	Just deploying the applications won't be sufficient. We will have to configure the istio VirtualService, so that we can access the services. 
	So we have 07-hw-virtualservice-all-services.yaml, here we have configured all the things.
	
	apiVersion: networking.istio.io/v1alpha3
	kind: VirtualService
	metadata:
	  name: helloworld-virtual-services
	spec:
	  hosts:
	  - "*"
	  gateways:
	  - http-gateway                      # 1
	  http:
		- match:
			- uri:
				exact: /hello-world
		  route:
			- destination:
				host: hello-world-rest-api
				port:
				  number: 8080
		- match:
			- uri:
				prefix: /currency-exchange
		  route:
			- destination:
				host: currency-exchange
				port:
				  number: 8000
		- match:
			- uri:
				prefix: /currency-conversion
		  route:
			- destination:
				host: currency-conversion
				port:
				  number: 8100
	

Step 12 - Observing Istio Service Mesh with Kiali

	# Using Kiali

	- http://localhost:20001

	```
	kubectl port-forward \
		$(kubectl get pod -n istio-system -l app=kiali \
		-o jsonpath='{.items[0].metadata.name}') \
		-n istio-system 20001
	```
	Create a secret
	```
	kubectl get secret -n istio-system kiali
	kubectl create secret generic kiali -n istio-system --from-literal=username=admin --from-literal=passphrase=admin
	```
	
	Kiali - Service mesh observability and configuration
	
	Kiali answers the below questions - 
		What microservices are part of my Istio service mesh?
		How are they connected?
		How are they performing?
		
	We already installed kiali when we ran our istio scripts.
	
	Really nice.. we should watch this section again. 
	
	kiali very cool...
	
	
	
	
	
	

	
	

---------------------------------------------------Continuing with Next Section for now, we will follow this section later again---------------------------------------------------








Section 11: GKE - Using Helm to Automate Microservices Deployment on Kubernetes

Step 01 - Understanding Helm Basics and Setting up Kubernetes Cluster

	We create a new cluster here. 
	
	What is helm? And why do we need it?
	
	Helm -  the package manager for k8s
	
	Now for istio we had around 7 scripts. We don't know what was created earlier, what will be created next, etc.
	
	We rather have one package which is used to deploy your application. 
	To manage this package we need something and that where helm comes into picture.
	
	Helm makes it easy for us to create application package. We can say these 7 are the scripts which make up this specific application. 
	
	Helm can do lot more, it can take care of releases, we can upgrade version of a package, etc.
	
	Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.
	Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.
	
	Chart is nothing but a package. A package of scripts which we would want to use to deploy an application. We can also mentain the version of these charts.
	And we can publish these charts out to a repository. So just like we have a jar created and published to a maven repository, we can create a chart and publish it to a helm repository.

	Helm Release, is a installation of a specific version of a chart on a kubernetes cluster. 
	
	So with these charts we would want to create a installation of an application on a kubernetes cluster. And this installation is called a release.
	
	Helm has a Server side component and a Client side component. 
	So we need to install something on our client and also on server, on our kubernetes cluster.
	Helm Client <======> Helm Tiller (Server Side)
	We would install Helm Tiller on our kubernetes cluster.
	And we will install Helm Client on our local machine or whereever or whereever you are trying to access the cluster from.
	
	Simplest way to get access to Helm Client is to use the Cloud Shell. Cloud Shell provides a version of Helm Client. Or we can also install Helm on our local machine.
	
	So we downloded the binary and add the path of that to System Path enviornment variable and Helm Client is installed.
	
	Install Helm Tiller On K8S Cluster - 
		
	Connect to new cluster.
	We need to execute a shell script to install a helm tiller. 
	
	So in 12-helm folder we have the shell script file - helm-tiller.sh and it also contains the instructions.
	
	./helm-tiller.sh
	
	But as we are on windows we won't be able to execute shell script file. So we can drag and drop the sh file on cloud shell. And when we do ls helm-tiller.sh then we can see the file over there. 
	We might want to add execution abilities so do - chmod +x helm-tiller.sh
	Now run it - ./helm-tiller.sh
	This will install helm-tiller on cluster.
	
	aniketrajput90@cloudshell:~$ ls helm-tiller.sh
	helm-tiller.sh
	aniketrajput90@cloudshell:~$ chmod +x helm-tiller.sh
	aniketrajput90@cloudshell:~$ ./helm-tiller.sh
	install helm
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
	100  7150  100  7150    0     0  50444      0 --:--:-- --:--:-- --:--:-- 50709
	Helm v2.16.5 is available. Changing from version v2.14.1.
	Downloading https://get.helm.sh/helm-v2.16.5-linux-amd64.tar.gz
	Preparing to install helm and tiller into /usr/local/bin
	helm installed into /usr/local/bin/helm
	tiller installed into /usr/local/bin/tiller
	Run 'helm init' to configure helm.
	serviceaccount/tiller created
	clusterrolebinding.rbac.authorization.k8s.io/tiller created
	initialize helm
	Creating /home/aniketrajput90/.helm
	Creating /home/aniketrajput90/.helm/repository
	Creating /home/aniketrajput90/.helm/repository/cache
	Creating /home/aniketrajput90/.helm/repository/local
	Creating /home/aniketrajput90/.helm/plugins
	Creating /home/aniketrajput90/.helm/starters
	Creating /home/aniketrajput90/.helm/cache/archive
	Creating /home/aniketrajput90/.helm/repository/repositories.yaml
	Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
	Adding local repo with URL: http://127.0.0.1:8879/charts
	$HELM_HOME has been configured at /home/aniketrajput90/.helm.

	Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

	Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
	To prevent this, run `helm init` with the --tiller-tls-verify flag.
	For more information on securing your installation see: https://v2.helm.sh/docs/securing_installation/
	Hang tight while we grab the latest from your chart repositories...
	...Skip local chart repository
	...Successfully got an update from the "stable" chart repository
	Update Complete.
	verify helm
	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.extensions/tiller-deploy   0/1     1            0           3s

	NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
	service/tiller-deploy   ClusterIP   10.121.4.139   <none>        44134/TCP   3s
	
	
	
	aniketrajput90@cloudshell:~$ kubectl get deploy,svc tiller-deploy -n kube-system
	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.extensions/tiller-deploy   1/1     1            1           4m31s

	NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
	service/tiller-deploy   ClusterIP   10.121.4.139   <none>        44134/TCP   4m31s
	
	
	So in this step helm client is deployed on our local, its connected to cluster and on the cluster we have a tiller installed.


Step 02 - Using Helm Charts to deploy Spring Boot Microservice to Kubernetes

	We will use CES to deploy it on cluster using helm.
	
	To be able to do this we will need to create a helm chart. 
	
	We have helm chart availabile already in 12-helm foler - Chart.yaml, values.yaml and templates folder.
	
	In Chart.yaml we just have kind of meta information. 
	values.yaml contains all the important varaibles and its values.
	These variables are used in templates -> microservice-deployment.yaml
	
	name: {{ .Values.name}} => this is a go template syntax. We are picking up values from values.yaml into microservice-deployment.yaml
	
	So its kind of template which can be used for any deployment of microservices. We can just change values in values.yaml file.
	
	We can deploy this - helm install ./currency-exchange/ --name=currency-services
		
		--name => what name we want to give
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\12-helm>helm install ./currency-exchange/ --name=currency-services
	NAME:   currency-services
	LAST DEPLOYED: Thu Apr  9 11:53:52 2020
	NAMESPACE: default
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/Deployment
	NAME               READY  UP-TO-DATE  AVAILABLE  AGE
	currency-exchange  0/1    1           0          1s

	==> v1/Pod(related)
	NAME                               READY  STATUS             RESTARTS  AGE
	currency-exchange-7f59d4948-xfqdg  0/1    ContainerCreating  0         1s

	==> v1/Service
	NAME               TYPE          CLUSTER-IP    EXTERNAL-IP  PORT(S)         AGE
	currency-exchange  LoadBalancer  10.121.9.105  <pending>    8000:32689/TCP  1s


	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\12-helm>kubectl get all
	NAME                                    READY   STATUS    RESTARTS   AGE
	pod/currency-exchange-7f59d4948-xfqdg   1/1     Running   0          68s


	NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
	service/currency-exchange   LoadBalancer   10.121.9.105   35.223.106.26   8000:32689/TCP   69s
	service/kubernetes          ClusterIP      10.121.0.1     <none>          443/TCP          79m


	NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-exchange   1/1     1            1           69s

	NAME                                          DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-exchange-7f59d4948   1         1         1       70s
	
	
	We are even able to access the url - http://35.223.106.26:8000/currency-exchange/from/USD/to/INR
	
	So whatever we deployed from helm chart is present out there.
	
	Now lets say we want to use the same chart and we want to create a installation for CCS.
	Its very easy. Just make few changes in values.yaml
	You can use folder currency-conversion in there. 
	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\12-helm>helm install ./currency-conversion/ --name=currency-services-1
	NAME:   currency-services-1
	LAST DEPLOYED: Thu Apr  9 12:00:50 2020
	NAMESPACE: default
	STATUS: DEPLOYED

	RESOURCES:
	==> v1/ConfigMap
	NAME                 DATA  AGE
	currency-conversion  2     1s

	==> v1/Deployment
	NAME                 READY  UP-TO-DATE  AVAILABLE  AGE
	currency-conversion  0/2    2           0          1s

	==> v1/Pod(related)
	NAME                                  READY  STATUS             RESTARTS  AGE
	currency-conversion-748c75db4d-jdnx8  0/1    ContainerCreating  0         1s
	currency-conversion-748c75db4d-qrxt8  0/1    ContainerCreating  0         1s

	==> v1/Service
	NAME                 TYPE          CLUSTER-IP   EXTERNAL-IP  PORT(S)         AGE
	currency-conversion  LoadBalancer  10.121.3.49  <pending>    8100:30141/TCP  1s

	
	
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\12-helm>kubectl get all
	NAME                                       READY   STATUS    RESTARTS   AGE
	pod/currency-conversion-748c75db4d-jdnx8   1/1     Running   0          41s
	pod/currency-conversion-748c75db4d-qrxt8   1/1     Running   0          41s
	pod/currency-exchange-7f59d4948-xfqdg      1/1     Running   0          7m39s


	NAME                          TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)          AGE
	service/currency-conversion   LoadBalancer   10.121.3.49    35.223.119.145   8100:30141/TCP   42s
	service/currency-exchange     LoadBalancer   10.121.9.105   35.223.106.26    8000:32689/TCP   7m40s
	service/kubernetes            ClusterIP      10.121.0.1     <none>           443/TCP          85m


	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/currency-conversion   2/2     2            2           43s
	deployment.apps/currency-exchange     1/1     1            1           7m41s

	NAME                                             DESIRED   CURRENT   READY   AGE
	replicaset.apps/currency-conversion-748c75db4d   2         2         2       43s
	replicaset.apps/currency-exchange-7f59d4948      1         1         1       7m41s


	URL - http://35.223.119.145:8100/currency-conversion/from/EUR/to/INR/quantity/10

	
	Exercise - Try "helm create microservice" to create a template folder
	
	
Step 03 - Using Helm Charts to manage Releases to Kubernetes Cluster	

	In the last step we had deployed CES and CCS using helm charts.
	
	If we have any problem in installing helm client then we can zip the its binary and drag and drop it on cloud shell and then unzip it using cmd - unzip zipfile.zip and then we can run helm cmds from cloud shell.
	
	Earlier we used helm install command we can also use it for dry run. It will show the entire templates that are created. 
	It prints out the yaml of what would be deployed - 

		helm install ./currency-conversion/ --name=currency-services-2 --debug --dry-run
		
	We can also see history using command - 
		
		helm history currency-services-1
		
	C:\Users\inarajp\Desktop\temp\Kubernates-Udemy\kubernetes-crash-course-master\12-helm>helm history currency-services-1
	REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION
	1               Thu Apr  9 12:00:50 2020        DEPLOYED        currency-exchange-alone-0.1.0   1.0             Install complete

	If we want to increase the number of instances then we can make change in values.yaml and use below upgrade command - 
	
		helm upgrade currency-service-1 ./currency-conversion/

	Now we might want to roll back to previous revison or previous number of instances/pods. 
	We can check history and know about revisions.
	And then use command - 
	
		helm rollback helm history currency-services-1 1
		
			The last parameter 1 is the revision number we want to rollback to.
			
	
	Other thing which we can do is, suppose we want to add a new template. Right now we have a simple template microservice-deployment.yaml 
	We can have mutiple yaml files present in same directory where microservice-deployment.yaml is present. 
	
	If you check the template folder for currency-conversion, you can see we have 2 yaml files - microservice-deployment.yaml and 00-configmap-currency-conversion.yaml
	
	So if we do upgrade - 
		
		helm upgrade currency-service-1 ./currency-conversion/
		
	we can see that both the yaml files will be evaluted and we can also see that the configmap is created.
	
	

	
	
	



























